doi,url,study,oa_status,first_author,title,abstract,published,journal,publisher,body,nb_tokens_openai_tiktoken,nb_tokens_mistral_sentencepiece
-,https://digitalcommons.unl.edu/libphilprac/5729,Ahmad 2021,True,PhD Pervaiz Ahmad,Towards Intelligent Systems: Modelling Academic E-book Power Users in Transaction Logs,"
Power and non -power use, involving user discrimination on the basis of expertise, is an idea from end -user computing with potential applicability as an interpretive tool for analyzing e-book user behavior . Can academic e -book power users be reliably identi fied in system-generated log data? A case study set of three-year e-book user transaction log data generated by the Ebook Library (EBL) platform was made available by the Edith Cowan University Library (Perth, Australia) to assist with the study. Deep Log Analysis (DLA) was used to explore the data. With statistical meth ods, further investigation yielded insight into whether an equation for identifying academic e -book power users within transaction log data could work at an appropriate confidence level. Identifying and isolating academic power e -book users in transaction logs for study presents some methodological challenges, for DLA targets large datasets requiring new skills and a commitment to learning new methods. This study has met this challenge by modelling academic e-book power users in transaction logs.
",2021,Australian Academic & Research Libraries,,"
Information Systems Committee [JISC] introduces another category of e-book users, power users, whose information behavior (IB) is different from average users (JISC, 2009). This demands further research on the information behavior of another category of apparently intensive or satisfied users, i.e. power users, in e-book domain with a novel measurement technique.
Literature Review
User interface individualization assumes the matching of system features to user groups i.e. the capability of user profiling. While profiling, the notion of power and non-power users is widely used in end user computing. Applying this concept to the e-book phenomenon, what are the attributes of power versus non-power use and how can such users be identified for the purposes of individualization? The topic of exactly what constitutes a power user (PU) or super user (JISC, 2009) of e-books is poorly explored in the academic e-book adoption literature. Using typology of four types of ideal users from Rainie and Jones, Borchert et al. (2009, p. 12) on the basis of simple measurement and speculation describe four categories of academic e-book usersbrowsers (experimenters), learners/lurkers (newcomers), satisfied users (netizens), and efficient users (utilitarians). Academic e-book researchers have also viewed sophisticated e-book users from different perspectives, for example, highest users (Levine-Clark, 2007), heaviest users (Folb, Wessel, & Czechowski, 2011), and most enthusiastic users (Posigha, 2012), with simple measurement. JISC (2009) refers to an e-book 'super user' as ""someone who had looked at five or more e-books within the four weeks leading into CIBER's user surveys"" (p. 24). Out of 8,800 university students who were surveyed, 1,540 (17.5%) were super (power) users who fulfilled the above criterion. The behavioral traits of JISC super users based on 26 e-textbooks on media, engineering, business, and management made available online by JISC to 127 UK universities from 2007 to 2009 via the MyiLibrary platform are as follows (pp. 6 & 24).
• early adopters of e-books, • more mature than most students, typically 22-35,
• more likely to be male, • most likely in business or engineering courses,
• much more likely to get their e-book readings from university library, • extensive readers of wide ranging titles in longer sessions, likely to be more than 20 minutes each session, consuming whole JISC e-books or several chapters, • navigators of e-books proactively via library web pages, • focused, serious, and highly dependent on the valued e-content, • highly satisfied with library provision of print books as well, and • frequent, almost daily users of both formats. JISC (2009) further asserts that since super users are likely to be early adopters of e-books identifying and understanding them is important for inviting their participation in beta testing new offerings and providing candid feedback. Ahmad and Brogan (2012) conceptualize an academic e-book power user (PU) whose pattern of use describes intensity very different from the average or median user. They further characterize a PU as ""...the user who prefers e-books as an information source, manifests exploratory behavior, converts titles browsed to titles read and explores collections independently of embedded links"" (p. 204). The authors claimed validation of this concept of a power user in a quantitative study of intensive EBL platform users using the criterion/formula (mean + 2 standard deviations above the mean) of the total aggregated minutes spent by all users in one year to construct a candidate sample. The study was novel in as much as the broader information systems literature on power use defaults to discussion of downloads and viewings and analysis based on self-reported behavior rather than interpretation of transactions involving information behavior constructs such as navigation, browsing, discovery, knowledge acquisition and engagement.
As the above discussion suggests, power user behavior can also be viewed within domainspecific theories of Information Behavior (IB). For example, Wilson (1999, p. 252) also adopted Ellis' 1987 ideas of search behaviors to form a model of information-seeking behavior in which the act of seeking information to answer a specific query and information searching described searcher interaction with systems used to satisfy searcher information needs. It is in these domains that transaction logs can be informative. For example, the clicking of an embedded courseware link to an e-book is an act of chaining within the meaning of Wilson's model adopted from Ellis. A transaction log might identify the requestor URL providing the basis of insight. Drilling down, the use of a discovery tool or library catalogue to identify e-books involves user interaction with an IR system, an example of search behavior.
Keeping in view Wilson's (2000) work and reiterating the ECT framework a user feels satisfied if the product or outcome meets or exceeds his/her perceived expectation-the phenomenon manifested in the form of read titles for longer hours across different sessions. Dissatisfaction may either lead to leaving or reiterating the search process, for example, an average user may abandon after browsing one or fewer titles but power user behavior may manifest browsing multiple titles and finding a considerable number of unique titles for reading. Wilson's work also elaborates the context of an information need. To understand e-book user behavior such as view and abandonment, skimming and reading, additional evidence is required of factors that shape IB. A researcher must look elsewhere for thinking about taxonomic ranking of behaviors providing a basis for discrimination between 'power' and 'non-power' use.
Clearly, there are problems with a notion of power use that does not account for more advanced information behavior. Titles viewed or time spent in reading can be unreliable indicators of engagement, if all or most activity is generated from chaining via embedded links. A domain appropriate concept of power use, therefore, needs to encompass other attributes of use more closely identified with learning, knowledge acquisition and information literacy. According to Marchionini (2006), exploratory search encompasses activities involving learning and investigation, making it different from lookup, which typically entails fact finding only. Marchionini's idea of exploratory searching describes several higher order cognitive processes or power behaviors evidence of which might be found in e-book transaction logs. Other researchers (e.g. O'Brien & Toms, 2008;Sundar & Marathe, 2010;White, Muresan, & Marchionini, 2006;White & Roth, 2009) provide a further confirmation of power users' advanced behavior generally that needs to be explored in e-book context.
Materials and Methods
The sample data for the study consisted of computer-generated Ebook Library (EBL) transaction log files of e-books used over three years, 2010-2012, at the Edith Cowan University (ECU), Perth, Australia. The ECU Library purchased access to EBL e-book database in 2010. The 2010The , 2011The , and 2012 log files contained 65,190, 70,750, and 97,273 log files contained 65,190, 70,750, and 97,273 records respectively of transaction data, describing the behavior of 8,482, 9,353, and 11,690 year-wise unique ECU e-book users. Features of these logs include the non-normality of data and over-representation of behavior based on embedded links. Table 1 describes log variables and coding of power and non-power users based on the heuristics of 1000 minutes and 10 or more unique titles. The dependent variable is non-power user (NPU) or power user (PU) coded respectively with zero and one (NPU0_PU1). 
The independent variables are Minutes Total (sum of Minutes Browsing and Minutes Reading), Views (sum of Titles Browsed and Titles Read), Minutes Max, Sessions, and Unique Titles Viewed (Unique Titles browsed and/or read).
Data Analysis and Results
The Kolmogorov-Smirnov (K-S) test (p <.001), and other measures (e.g. inspection of skewness, kurtosis, histograms, boxplots) indicated the non-normal distribution of data across all variables based on all e-book users (PUs and NPUs). The heuristic of academic e-book power use adopted for the study yielded 517 PUs overall. In this study, an academic e-book PU is characterized as a person who spent 1,000 or more minutes in browsing and/or reading of 10 or more unique titles in one year. 2 Such a threshold was set to minimize the chance of inclusion of reading behavior concentrated merely around embedded courseware links. When compared with the total ECU population (faculty, students, and staff) (Edith Cowan University, 2013) the e-book PUs are 152/25,943 (0.59%), 233/25,734 (0.91%), and 132/25,404 (0.52%) respectively for 2010, 2011, and 2012.
Based on the 2010 data, in contrast with the JISC ( 2009) study, ECU e-book PUs are most likely to be found in health sciences, business & management, media, engineering, computing, law, and education. Subsequent sections demonstrate significant differences in power user behavior from non-power users, that they can be detected statistically by their patterns of system use, and develop a model that can dynamically determine, a priori, whether a user is a power user or not.
Difference between Power and Non-power Users
A Mann-Whitney U test was used to compare two independent, combined samples (PUs and NPUs) of 2010. This test was selected to see if the values between PUs and NPUs across the variables, Views (transactions), Minutes Total, Minutes in Browsing, Minutes in Reading, Minutes Max spent in browsing/reading a title, Sessions conducted, Titles Browsed, Titles Read, overall Unique Titles viewed (regardless of mode, browsing or reading), and Unique Titles Read are statistically, significantly different. The purposive sample of 152 PUs was compared with a randomly selected sample of 381 NPUs drawn from the 2010 dataset. The NPU population for 2010 was over 8,000. Hence, the NPU sample size was determined from Israel (2012) based on ±5% precision level where confidence level is 95% and P = 0.5 to mitigate type I and II errors.
The Mann-Whitney U test indicated that the values of PUs across all variables were significantly different than those of the NPUs as evidenced in Table 2. Thus PUs spend more minutes in browsing and reading, conduct more sessions, explore more unique titles and browse and read more titles than NPUs and these differences are significant. Hence a picture of the power user behavior begins to emerge where classic behaviors identified with power users of print books are also found to be significant with e-books.
Relationship between Variables (Correlations)
Kendall's tau-b (one-tailed, N = 533) indicated the presence of a strong positive correlation of minutes total with minutes in reading (τ = .92, p < .001), with minutes max (τ = .89, p < .001), with titles read (τ = .72, p < .001), with minutes in browsing (τ = .70, p < .001), with views (τ = .70, p < .001), with unique titles read (τ = .70, p < .001), with sessions (τ = .66, p < .001), with titles browsed (τ = .65, p < .001), and with unique titles viewed (τ = .61, p < .001). Correlation testing results were consistent with the results from Mann-Whitney U testing.
A Model to Predict Power Users of E-books
The researchers recognized that the most useful outcome from DLA analysis of transaction data would come from autonomous, machine-based analysis of user behavior leading to categorization of a user as a power or non-power user and utilization of the result to adjust the user experience of e-books via interface and accessible functionality. Binary Logistic Regression (BLR) was used to see what variables predict a PU and also to confirm a formula that might work with log data to dynamically distinguish a PU from an NPU.
Binary Logistic Regression (BLR)
The 2010 dataset was used as a base to develop a regression equation. BLR is non-sensitive to the conditions of data normality, levels of measurement, linearity and variance (R. B. Burns & R. A. Burns, 2008). As discussed, the PU/NPU subset comprised a purposive sample of 152 PUs and a random sample of 381 NPUs from 2010. Owing to the dichotomous and categorical nature of the dependent variable (PU/NPU), BLR was selected as the most appropriate regression method. The predictor or independent variables derived from the raw transaction logs were Minutes Total, Views, Minutes Max, Minutes in Browsing, Minutes in Reading, Sessions, Titles Browsed, Titles Read, Unique Titles viewed (browsed and/or read), Unique Titles Browsed, and Unique Titles Read. Two variables as a whole, Minutes Total and Unique Titles were not included in the analysis because these were used to derive the response/dependent variable, NPU/PU. However, Minutes Total was bifurcated as Minutes in Browsing and Minutes in Reading in the analysis. One of the bifurcations of Unique Titles was included in the analysis as Unique Titles Read. Hence, Unique Titles Browsed was excluded. Another variable, Views (transactions/accesses), was not included in the analysis as a whole but was bifurcated into Titles Browsed and Titles Read.
The preliminary test showed that two variables, Minutes Max and Titles Read were not significantly contributing to the model hence they were excluded. Using SPSS-21 a BLR re-test of the model was statistically significant, indicating that the remaining five predictors as a set reliably distinguished between PUs and NPUs (chi square = 600.013, p < .001 with df = 5).
The non-significance (p > .05) on the Hosmer-Lemeshow (H-L) goodness-of-fit test, an alternative to chi-square, indicates well-fitting models (R. B. Burns & R. A. Burns, 2008). This desirable outcome of non-significance suggests that the model prediction does not significantly differ from the observed. In our case the H-L statistic (1.000) was not statistically significant, indicating good fitness of the model (Table 3).
Table 3. Hosmer-Lemeshow Test
Step Chi-Square df Sig. 1 0.190 8 1.000
The prediction success rate of the BLR model was 98.7% overall and for NPU and PU as well as shown in Table 4. Nagelkerke's R-squared was 0.969, indicating a strong relationship between the grouping/predictors and the prediction. The Wald criterion demonstrated that the five predictor variables, minutes in browsing (p < .022), minutes in reading (p < .001), sessions (p < .048), titles browsed (p < .038), and unique titles read (p < .042), made a significant contribution to prediction at a = 0.05 level with one degree of freedom as evidenced in Table 5. The logistic coefficients produced the following predictive equation:
Where x = {(0.110 x Minutes in Browsing) + (0.009 x Minutes in Reading) --(0.398 x Sessions) + (0.170 x Titles Browsed) + (0.417 x Unique Titles Read) -14.604}; and e is the base of the natural logarithm (approx. 2.72).
The above equation was applied to two of the randomly selected sample cases, one each from PU and NPU 2011 datasets. Based on one year of data, both cases satisfied at least 1,000 Total Minutes (browsing and reading). The second criterion of unique titles viewed =>10 was satisfied by Case2 only. Therefore, according to criteria, Case2 was a power user and the Case1 a non-power user. The equation classified both cases correctly without knowing the criteria values. As the Table 6 showed the probability of being a power user for Case1 was 7%, and for Case2 as 95%.
Validity and Reliability Testing of the BLR Model
Two tests were conducted to test the validity and reliability of the BLR model. These were the Receiver Operating Characteristic (ROC) test and test of reliability/efficiency.
Receiver Operating Characteristic (ROC) analysis
Receiver Operating Characteristic (ROC) curve is a useful measure of goodness-of-fit to evaluate the performance of classifying binary subjects (IBM, 2013). ROC procedure assesses the predictive accuracy of a comparing model (Gonen, 2006). In this chapter ROC was used to evaluate the fit of the BLR model based on the simultaneous measurement of sensitivity (True positive) and specificity (True negative) for all possible cutoff points using state variables (NPU/PU) and the saved predicted probabilities of the BLR as test variable. The sensitivity and specificity pairs for each possible cutoff point and plot sensitivity were calculated with ROC curve analysis at asymptotic 99% significance level (Table 7). The area under the curve with 99% confidence interval, .999 (.998, 1.000) for PU and .001 (.000, .002) for NPU, is significantly different (p < .001) meaning that the BLR classifies both the groups (NPU/PU) significantly rejecting the null hypothesis of by chance (Table 7 and Figure 1).
Figure 1. ROC validation of the PU model
Based on the data analysis results and validation tests, Figure 2 presents a model of academic e-book power user.
Figure 2. Academic e-book power user model

Discussion
To begin with, the researcher reviewed how the Power User (PU) might be usefully defined. The existing research oriented publication often defines the power user simplistically. For example, ""someone who had looked at five or more e-books within the four weeks"" leading into a user survey (JISC, 2009, p. 24). Other academic e-book researchers simply view such e-book users as, for example, highest users (Levine-Clark, 2007), heaviest users (Folb, Wessel, & Czechowski, 2011;Nicholas et al., 2010;Posigha, 2012), most enthusiastic users (Posigha), satisfied users (netizens), and efficient users (utilitarians) (Borchert et al., 2009, p. 12).
Such a simplistic view fails to account for the LIS literature on information behavior which attributes exploratory search, serendipitous discovery and other attributes to 'advanced behavior' (Marchionini, 2006;O'Brien & Toms, 2008;White & Roth, 2009). Consequently, the research offered an alternative heuristic encompassing conversion of titles browsed to title read and unique titles as well as time spent in browsing and reading. If a model of the power user based on the wider discourse of advanced behaviour were to be adopted, might the data be used to validate such a model? Table 5 includes statistically analyzed variables representing parameters of PU behaviour that were not captured in a concept of preceded literature and Table 7 (Figure 1) shows the results from validation testing of such a model.
Power use is more appropriately considered as encompassing exploratory behavior describing advanced cognitive processes in information behavior (e.g. investigative searching involving multiple iterations and activities such as analysis, synthesis evaluation, and serendipitous browsing with an objective of learning) (Marchionini, 2006;White & Roth, 2009). The researcher explored whether a method could be established and with what variables to categories PUs. The outcome from this research was another discovery-that an equation could reliably predict power use based on three years' worth of EBL transaction log data of e-book usage at ECU. This research has made an original contribution to knowledge by demonstrating that: • concepts of higher level cognitive behaviours in searching and learning can be applied to the understanding of user types described in log data; • it is feasible mathematically to identify a PU on the basis of transaction log records;
• models created in this way can be successfully validated against the data. However, the work done describes the need for calibration involving more datasets; and • models can be used to predict (categorise) users providing in real time the basis of discriminating between users in terms of user customisation and personalisation of e-books.
A set of business rules will also be required that defines the nature of the individualized experience to be offered to users based on machine-based classification outcomes. The goal of user-centric design for e-books should be to deliver individualized views and functionality to users of e-books, based on behavioral profiles. According to Sundar and Marathe (2010), customized offerings can be gratifying especially in the web environment which is known for its issues of information explosion and overload. They further argue that customization may range from simple font or color change to more advanced modifications. This study has also demonstrated the contributing role of e-book customization capability to user satisfaction and continuance intention with large effect sizes.
The patterns of academic power e-book user behavior were explored by (a) using a criterion to distinguish a PU, (b) determining differences between PUs and NPUs by comparisons, and (c) devising and validating a predictive model for the probability of a PU. DLA of EBL e-book transactions yielded a model of power user behavior grounded in evidence contained in the logs.
The model suggests a different approach for identifying and defining an academic e-book power userone consistent with Marchionini's (2006) notion of exploratory searching encompassing activities such as learning and investigation, as well as fact finding. In these terms a power user is one who converts titles browsed to titles read and explores collections independently of embedded courseware links. Further this research demonstrated that a set of potential business rules can be derived that might provide the basis of machine-based user classification. Such classification might be used to deliver individualized views and functionality to users of e-books, based on behavioral profiles.
DLA findings demonstrated that a minority of users accounted for most e-book usage in terms of total views, minutes, and sessions. Thus the findings support the notion of the 'power' or intensive user in e-book utilization, as suggested in previous studies (e.g. Ahmad & Brogan, 2012;Ahmad, Brogan, & Johnstone, 2014;JISC, 2009). Sundar and Marathe (2010) found that ""power users rated content quality higher when it had a customizable interface, whereas non-power users preferred personalized content"" (p. 298).
However, by way of limitation, it is important to acknowledge that the proposed model/equation is based on the data of one case library only, namely ECU. Hence, its power has not been tested on any other dataset, enabling conclusions as to the generalizable character of the model and its usefulness. Taking the current result further in terms of a generalizable solution will necessarily involve calibration using more datasets from other participating libraries. In circumstances where the availability of even anonymized data cannot be assured for reasons of privacy, pushing this research forward with further datasets presents as a challenge to researchers interested in the field.
Conclusion
The paragraphs that follow describe outcomes from this research that revise and/or add to the body of knowledge in relation to building better e-book systems in terms of Expectations and Gratification Theory (EGT) through information behavior profiling.
But what to do with powers users, presuming they can be found and their information behavior documented? User interface design in computing and information systems has evolved significantly, from text-only monochrome displays using keyboard input to touch-sensitive, multitasking tablet applications (apps) that respond to voice commands. Unfortunately, e-book systems have not kept pace with developments in user interface design. If power users of e-book systems have different requirements, then they might benefit from a changed interface and richer functionality. A first step in giving effect to the work done here would be to determine precisely who is a power user dynamically (i.e. as a user interacts with a system) and then to give such users the opportunity to customize and/or adopt a system personalized interface that better supports their needs. This is also part of the narrative of identifying and working with 'power users'.
Given the apparent importance of individualization (i.e. customization and personalization) of e-books to users, the researcher reflected on how power users might be profiled from log data enabling the e-book experience to be customized and/or personalized. Thus evolved the idea of a further study that would attempt to understand how profiling of users might be undertaken dynamically within an e-book delivery system, paving the way for intelligent e-book systems capable of delivering customized and personalized user experiences.
This study demonstrates how power user behavior is different from other user behavior, shows which variables determine such behavior and creates a probabilistic model that can determine a power user based on these variables. The work is rational and significant in as much as profiles might be used to offer customized user interfaces to users-a classic approach to improving user experience with information systems. Such findings reflect the broader discourse on the role of customization and personalization of e-books. Tailoring content on websites is now even more popular and important if companies aim to satisfy all of their users and digital media have made it extremely simple. Customization is more involving and empowering as it offers more active role for the user in ensuring personal relevance and utility of mediated content. Greater customization breeds more positive attitudes toward portals. Greater interactivity engenders more involvement, greater attention, and intimate contact of user with closer scrutiny of content. Self-as-source (agency) may motivate greater engagement with content cognitively and reflect users' identity affectively. Ultimately, this would increase users' attention to content, thus amplifying their experience with it and its effects.",5432,6366
10.1007/s00799-014-0134-y,,Bernard et al 2015,True,Jürgen Bernard,VisInfo: a digital library system for time series research data based on exploratory search—a user-centered design approach,"
To this day, data-driven science is a widely accepted concept in the digital library (DL) context (Hey  et al. in  The fourth paradigm: data-intensive scientific discovery. Microsoft Research, 2009). In the same way, domain knowledge from information visualization, visual analytics, and exploratory search has found its way into the DL workflow. This trend is expected to continue, considering future DL challenges such as content-based access to new document types, visual search, and exploration for information landscapes, or big data in general. To cope with these challenges, DL actors need to collaborate with external specialists from different domains to complement each other and succeed in Electronic supplementary material The online version of this article (doi:10.1007/s00799-014-0134-y) contains supplementary material, which is available to authorized users.
",2014-12-03,International Journal on Digital Libraries,Springer Science and Business Media LLC,"
given tasks such as making research data publicly available. Through these interdisciplinary approaches, the DL ecosystem may contribute to applications focused on data-driven science and digital scholarship. In this work, we present Vis-Info (2014) , a web-based digital library system (DLS) with the goal to provide visual access to time series research data. Based on an exploratory search (ES) concept (White and Roth in Synth Lect Inf Concepts Retr Serv 1(1): 2009), VisInfo at first provides a content-based overview visualization of large amounts of time series research data. Further, the system enables the user to define visual queries by example or by sketch. Finally, VisInfo presents visual-interactive capability for the exploration of search results. The development process of VisInfo was based on the user-centered design principle. Experts from computer science, a scientific digital library, usability engineering, and scientists from the earth, and environmental sciences were involved in an interdisciplinary approach. We report on comprehensive user studies in the requirement analysis phase based on paper prototyping, user interviews, screen casts, and user questionnaires. Heuristic evaluations and two usability testing rounds were applied during the system implementation and the deployment phase and certify measurable improvements for our DLS. Based on the lessons learned in VisInfo, we suggest a generalized project workflow that may be applied in related, prospective approaches.
Introduction
Today, scientific information and knowledge are no longer solely encoded in text format, such as journal articles or book chapters. Increasingly, non-textual formats such as images, audio-visual material, computational models, and other numerical data sets are considered. For scientists, this body of non-textual data and information represents an invaluable source of possibly undiscovered knowledge if meaningful data subsets can be retrieved from typically large and heterogeneous research data repositories. The value of such research data and its (potential) benefit to society are widely accepted [17]. Time series data is an important research data type originating, e.g., in climate research, medical treatment, etc. However, besides the size and the heterogeneity, the time-varying behavior adds another level of complexity to this type of research data [34]. Today, a variety of research data repositories exist. DLs can support the access to and re-use of this valuable type of research data.
Historic scientific discoveries based on experimental, theoretical, and computational science paradigms were always the subject of library service support. In times of dataintensive science (also called the fourth paradigm in scientific discovery [26]), the role of DLs may be more important than ever. Part of the mission of a library of science and technology is to support scientists with methods that allow them to effectively use the available body of knowledge. This includes search and retrieval methods as well as exploration methods that also consider the non-textual data content. Further, it includes providing indexing and citation methods for future reference. To support scientists effectively, these DL functionalities should be adopted to their specific workflows (see Deelman et al. [19] for a characterization of scientific workflows). For example, the definition of similarity for the underlying data content, the calculation of features for retrieval, or the incorporation of aggregation techniques for large data collections have to meet the specific requirements of scientists.
While query-response technologies (as used in classical web search engines) typically act as lookup tools for fact retrieval and known-item search, ES goes beyond that: it aims to support scientific investigation and discovery, e.g., by revealing interesting facets of knowledge that scientists had not seen and considered before. Learning and decisionmaking are aspects that need to be involved in the information seeking process, especially for the content of complex data types such as time series research data. The success of ES tools depends on their visual-interactive capabilities.
Important components include the visual overview of the data content, the visual query definition, and the visual representation of retrieved items [68]. Visual overviews of the content can help scientists to explore large collections of time series research data. Visual-interactive query definitions based on examples or sketches of time series curves can help to make the search process more intuitive. Enhanced visual result representations can serve as an exploration space for applying facets or for gaining new and potentially unexpected knowledge. However, many scientists still perform at least part of their work using general purpose tools-most notably, Excel [29,64]. User-centered design approaches may help to raise the trust in new innovative technologies that address the discussed challenges.
We introduce VisInfo, a web-based, exploratory search system for time series research data. Our contribution is as follows: (1) we present our user-centered design approach, which was executed following a distinct design study methodology. We chose a user-centered approach to support the scientific workflow of our users in the best possible way, and raise the users' trust [29,64]. We included the users from the beginning of the design process that started with a domain and problem characterization, followed by an iterative development phase, and ended with the presentation of the web-based VisInfo system. The process was conducted as a collaborative effort between data collectors, data curators, digital librarians, digital library users, and computer scientists. (2) Furthermore, we present the result of this process, VisInfo DLS, that enables the content-based access to time series research data collected by the Baseline Surface Radiation Network (BSRN). Scientists can explore large data collections with a content-based overview visualization (Visual Catalog). Moreover, they are enabled to define queries of the time series content visual-interactively by example and by sketch. The Result View allows for the exploration of retrieved data subsets in detail. Different perspectives thus allow for an analysis of interesting relations hidden in (a) the time series data content, (b) the geo-location of respective measurements on earth, and (c) measurement period in a calendar-based view. (3) In addition, we present the results of different evaluation strategies iteratively conducted with digital library users. A use case and the results of the conducted evaluations reveal that the final VisInfo prototype is both usable and useful. (4) Finally, we review the lessons learned within the project and contribute a generalized project workflow description, that may help related visualization research efforts within the DL context.
Related work
In the following, we review initiatives in the field of DL to support scientific work with research data. Further-more, we highlight ES approaches for time oriented data. Finally, we report on user-centered design practices from the perspectives of DL, information visualization, and visual analytics.
Scholarly support for research data
In this work, we treat research data as a special document type to be provided by a DL. Examples for data-driven research domains are physics, chemistry, medicine, biology, or earth observation, just to name a few. Facilitating the access to research data leads to both special benefits and challenges for scholarship and DL use. This especially applies for the type of data content considered in this approach (time series data), e.g., arising from measurements in a variety of research domains. Additionally, metadata attached to the research data documents play an important role. Examples of metadata include secondary data about test conditions, or geo-locations of scientific measurements. Many research domains differ in the nature of their research data, their conventions about data use and re-use, and their applied methods [17]. A classification of different types of research data is presented by Kehrer and Hauser [33], with an emphasis on visualization and visual data analysis. The process of passing research data through different phases is often referred to as the data-life-cycle [3,13,17]. The typical phases are data creation, data processing, data analysis, data preservation, data access, and data re-use. The VisInfo approach is primarily targeted towards the phases of data analysis, data access, and data re-use, and aims at tackling respective challenges related to search [13,24,40], and exploration [3,25,68] tasks (see Sect. 2.2 for more details).
Focusing on the user-and task-centered perspective, we highlight the relation of VisInfo to scientific workflows and scientific workflow systems. A (scientific) workflow is a high-level specification of steps (and dependencies in between) to accomplish a specific goal in a datacentered working environment [19,39]. Each step represents the execution of a combinational unit, such as running a program, submitting a query to a database, submitting a job to a compute cloud or grid, or invoking a service over the web to use a remote source [26,29]. One of the most challenging goals in the workflow construction phase is the user-centered development of a data processing pipeline. Typical challenges are based on the complexity of (time series) data [34], the involvement of different stakeholders (including data scientists) [18], and the construction of data-centered workflows with appropriate algorithmic routines in the correct order and with the correct parameter values. Regarding the latter, we refer to the survey of Knowledge Discovery in Databases (KDD) [21], or to basic research in visual analytics [34].
Other challenges related to analysis workflows for research data concern, data management, and data transformation [39], the visualization [23], and advanced search interfaces [25].
As a DLS prototype, VisInfo incorporates research data warehouses (data repositories) as the targeted data source to facilitate data access and data re-use. While not being limited to a particular repository, VisInfo currently accesses thousands of (time series) data sets of the PANGAEA repository [48], a publisher for earth and environmental science data. A variety of research data repositories exists, such as, for example, the Sloan Digital Sky Survey (SDSS) [57] for physics and astronomy research data. Surveys of scientific publications, (open) data publications (e.g., Elsevier) 1 , and scientific data repositories are presented by Marcial et al. [41], and Costas et al. [17]. Finally, we highlight the beneficial means of open data initiatives [63] and metadata standards [14] for this work, increasing the demand for (visual) accessibility functions by scientists working with such data.
Exploratory search in time series data
While a variety of DL approaches for textual access to research data exist [20,47,48,61], the number of DL approaches for non-textual access to research data is comparatively small. However, research fields like information visualization [16], visual analytics [34] and ES [40,68] concentrate on challenges like enhanced exploration and search interfaces for research data. Kehrer and Hauser compare approaches from an information visualization perspective [33]. A review on works for visualizing climate change data is presented in [46]. In exploratory analysis scenarios, clustering approaches are often applied to group large research data. Based on such aggregation techniques, global overviews of the complete data set can be provided, also known as content summaries. Promising examples exist for geo-science [1], earth observation [8,55], energy consumption [66], human motion analysis [11], or cancer research [10]. A metadatabased clustering approach is presented in [65]. In [8,9], and [10], clustering was used to gain insight into interesting relationships between the data content and metadata associated to the research data content. Visual query definition concepts for time series data are reviewed in [5], a comprehensive survey for the visualization of time series is presented in [2]. Research data-based approaches on multivariate data changing over time are presented in [12,62] considering event data or climate data. From this review of the related work, to the best of our knowledge, combined approaches combining research data with content-based visual search in the DL context are scarce.
User-centered design in digital libraries
In the early 1990s, usability concepts and methods made huge progress and also found their way into web-based applications. Shackel's work from 1991 can be seen as one of the cornerstones of this time [59]. Concepts and models such as iterative test and development cycles [43], heuristic evaluations [45], or cognitive-walkthroughs [37] were introduced. A comprehensive overview of these approaches is provided by John [32]. In 2000, Saracevic reviewed usability evaluation efforts in the DL context [54]. He confirmed a lack of evaluation methodologies in DL, and suggested a conceptual framework for DL evaluation. An early formal evaluation during the DL development process was done by Hill in the Alexandria DL Project (ADL) [27]. In the following, usability models and concepts were examined and adapted to the DL context [30,31]. In 2005, Reeves published a comprehensive guide on evaluation of DL [50]. Some of the latest outcomes of usability evaluation for DL include the Greenstone User and Developer Survey [60]. In 2010, Yuan et al. carried out a usability evaluation to examine differences in users' experiences between an information visualization system and a text information retrieval system [69]. Their final results indicate that visualization techniques help to improve the representation and organization of information in retrieval systems. The discussion and adoption of usability evaluation in the information visualization context can be found in [4,49]. Hoerber [28] surveyed user evaluation methods for visual web search interfaces. Their model already includes several development iterations followed by distinct evaluation methods. A generalization of the design and validation of visualization techniques is introduced by Munzner [42]. In her nested model, she clearly defines four nested layers: domain problem and data characterization; operation and data type abstraction; visual encoding and interaction design; and the algorithm design. In our approach, we adapted this method to the DL context. Finally, Sedlmair et al. fit these concepts into a process describing design study methodologies in general [58]. The authors highlight the importance of characterizing the relevant stakeholders and collaborating closely with these during the entire design process.
The VisInfo concept
A wide-spread method, which scientists across many fields employ, is to create visual representations of the data they have generated. Often, it is effective for humans to intuitively assess the essence of a data set that is plotted [16]. Graphical methods of analysis, optimization, problem solving, and design have been used for centuries. Further, there is the notion of designing displays that deliberately show chart-and visual-based representations of numerical values, rather than tables of numbers. That is because for a human, it is easier to intuitively gauge the position of pointers than to actively read digits and have to mentally process them, given the limitations of the short-term working memory [16]. In a similar way, the essence of a curve is often grasped more easily than a series of numbers. Or to quote Henry Hubbard: ""There is a magic in graphs. The profile of a curve reveals in a flash a whole situation-the life history of an epidemic, a panic, or an era of prosperity. The curve informs the mind, awakens the imagination, convinces.""
This notion is at the heart of the VisInfo approach. For this reason, the focus lies on visual analysis of curve shapes. At the Alfred Wegener Institute (AWI) in Bremerhaven, Germany, scientists have developed and employed tools to collect and plot data, enabling them to intuitively process and visually analyze the data set, which shows the importance of graphic representations. For this reason, a related data collection of the Baseline Surface Radiation Network (BSRN) [6] has been chosen for a first prototypical application.
In the following, we recall the basic concepts of our previous work, which formed the basis for the VisInfo prototype.
Focus and adopted similarity notion
We first started to map out how the goal of visual and content-based search in research data could be operationalized. The domain of research data is huge, and, together with our project partners, we decided to restrict ourselves to time series research data. An initial workshop held together with domain experts from the PANGAEA data library [48] operated at AWI revealed that among the many different research interests in earth observation science, time series are a ubiquitous data type, the support of which is expected to benefit a larger user community. As a core DL functionality, we decided to implement content-based visual search support to help scientists explore and retrieve data of interest. Content-based similarity is a multifaceted problem and appropriate similarity notions depend highly on the type of application, including partial similarity, and similarity across different levels of resolution, including agreement of measurement parameters [22,38].
In our initially proposed concept [5], we chose to start with a simple, robust similarity measure. We partitioned all time series of a PANGAEA test repository to segments of one day length each. The resulting curve shapes were named curve patterns in our retrieval system. Finally, the Euclidean distance function was applied, to define the similarity of the time series patterns. 
Visual search and exploration facilities
Our initial concept supported two data access methods: content-based visual querying and visual browsing. Visual search is based on the query-by-sketch principle. By means of an interactive sketch interface, users can define a curve pattern in which they are interested (Fig. 1a illustrates this concept). The system computes a set of top n results of the most similar time series and displays these in a sorted list. Visual browsing allows a bird's-eye perspective on the entire set of time series. The Self-Organizing Map method (SOM) [36] is applied on the set of curve patterns and produces a 2D grid of cluster prototypes, representing distinct time series patterns in the given data. As a specific feature of the SOM, the output can be directly visualized as a content summary solution, which we call 'Visual Catalog' (see Fig. 1b). By the nature of SOMs, the patterns in the Visual Catalog are sorted based on their similarity. This ensures topology preservation of the provided map metaphor. With the Visual Catalog, users can browse and drill down for specific clusters of interest. Bundles of data elements can be visualized with blue opacity bands [56] . It is also possible to select a curve pattern in the Visual Catalog for search (query-by-example). Further basic functionality includes the filtering of metadata items and the highlighting of the correspondence to the clusters or content-based search results.
Analytical facilities
In our work, we focused on the question as to which extent digital data repositories can or should support analytical tasks of the end users. To most scientific users, retrieving time series data of interest is not the end, but the start of further research. In previous work, we therefore also explored which analytical services could be added to the basic search and exploration approach described above. Ideally, such analysis functionality should a) be efficient to implement on top of the basic system and b) be of value to a large number of users. We earlier considered two such analytical services. For one, we devised a schema for combined content-based and metadatabased visualization [8]. This schema allows the user to select a metadata attribute. As an example, consider the selection of the creator attribute. For each instance of this attribute, a Visual Catalog is enriched with a highlighting of the data items provided by the given creator. Glyphs of the catalog for each creator are then shown in a layout, which represents the similarity between each creator by distance in the layout (illustrated in Fig. 1c). Thereby, users can compare the similarity between data level and metadata level, potentially finding interesting cross-relationships between both data aspects. A second analytical service relates to the identification of strong correlations between clusters of time series and frequent metadata items [9]. For example, consider the set of 123 Fig. 2 The VisInfo architecture locations at which the member time series within a given cluster of time series have been measured. Then, the presence of a high number of identical location attributes within a cluster may be of interest, as it relates to a robust cluster of measurements. Technically, we compute the degree of unimodality of the histogram over metadata for a given cluster, and identified the most unimodal metadata elements (see Fig. 1d).
Domain and problem characterization
The conceptual focus of this research effort lies on the similarity notion of research data and the visual search and exploration facilities realizing visual access to this data. As is typical in design studies, the design process started with a domain and problem characterization, with respect to the VisInfo concept presented in the previous section. First, we identified the stakeholders relevant for the application domain, and accordingly, for our user-centered design process (Sect. 4.1). Then, we identified domain challenges in collaboration with the identified stakeholders (Sect. 4.2).
Relevant stakeholders in the design process
Inspired by the stakeholder characterization presented in the DELOS digital library reference model [15] , we identified five different stakeholder types: data collector, data curator, digital librarian, computer scientist, and digital library user. Representatives of these stakeholders were included in the design process. In the following, the stakeholders are characterized, and their input for the design process is presented.
The data collectors are gathering the data, which is later used to create and validate scientific hypotheses. Thereby, the data type is defined. In our approach, the data collectors are earth observation scientists of the Baseline Surface Radiation Network (BSRN). The data consists of time series measurements, e.g., several physical parameters of radiation, temperature, etc. An excerpt of the BSRN data set [6] comprising all measurements of the BSRN is used as the basis of our search and retrieval system. The size of the data set is 25 Gigabytes in ASCII format. For more information about the data set, we refer to our concept paper [5], where a subset is used and described in detail.
The data curators have to ensure that the data provided by the data collectors are persistently stored in a data repository in a standardized format. Moreover, they enable the citability of data sets through digital object identifiers (DOIs) [14]. In our approach, the hosts of the PAN-GAEA portal [48] play the role of the data curators. They provide access to most of the data collected within the BSRN. The data is stored in a specific format-""ISO-8859-1: ISO Western -PANGAEA default"" -that consists of tabseparated text files comprising time series measurements of one month and associated metadata. Each file has an own DOI (Fig. 2).
The digital librarians in our approach are representatives of the German National Library of Science and Technology (TIB). Their main goal is to provide their users a contentbased search and retrieval system for time series research data. Their main focus is on the ease of use of the system, since due to its innovative nature caused by the implied paradigm shift [26], any additional barrier could potentially cause the user turn away from the system before even fully assessing its possibilities. Moreover, the digital librarians requested to work from early on with real data to discuss early demonstrators with real users. To support this, a quick import of new data sets into the system was also requested by the digital librarians.
The computer scientists are the developers of the DLS. To enable the visual access to content-based information, experts from the field of data analysis and information visu- The digital library user is the end user of our DLS. In our approach, earth observation scientists represent this stakeholder. They basically cover two roles in our approach: data collectors and digital library users. Thus, digital library users fulfill the role of the domain expert known from design study methodology [58].
In addition to these implicit requirements on the VisInfo system given through the addressed domain, the stakeholders involved, and the selected data set, we also identified challenges in the domain in collaboration with potential users of our system in a distinct process. From these challenges, explicit requirements on our system may be extracted. In the following section, this process is described in detail.
Relevant challenges in the application domain
To understand and characterize the problems and challenges in our application domain, we worked in close collaboration with our potential users, the earth observation scientists, from the very beginning. The definition of explicit challenges to be tackled by the VisInfo system was supported through a 5-step process (Fig. 4).
As a first step, we developed paper prototypes and showed them to the users to demonstrate our general idea of exploratory content-based search. We then gathered first feedback from the users through informal interviews. We learned that for all users the general idea was inspiring. Still, it was difficult for them to adapt the concept of searching in the data content, since this possibility was not given before. As a second step, a desktop prototype was developed. The core functionality of this system has been described in Sect. 3. Basically, the users could define queries by sketch or example, they could activate filters on the metadata, and finally, they could look up details of the searched patterns in the result list. The prototype was introduced to the users via a screencast. Finally, we evaluated the feedback of 19 earth observation scientists from the BSRN for our interpretation of the domain and their challenges. In the following, we describe the main challenges identified in the described process.
(1) Varying similarity notions: To support content-based search in time series research data, a definition of similarity has to be provided. Depending on the tasks and the users, the interpretation of similarity may vary. In some cases, absolute values of the measurements have to be compared (e.g., to distinguish arctic from desert regions). In some cases, similarity has to be defined via the relative shapes of curves (e.g., to identify similar trends). Summarizing, the similarity of absolute values as well as curve shapes need to be considered.
(2) Different time intervals: For the earth observation scientists within our stakeholder group, the time series to be compared should have the same length (e.g., one day, etc.). However, different lengths may be of importance for their work. The most important time intervals for a curve shape identified were years, days, and hours. Years and days reflect periodicities of natural phenomena. In reconciliation with an expert at the Alfred-Wegener Institute, we decided to focus on daily curve shapes as fixed time interval called patterns in our retrieval system.
(3) Relevant Parameters: For our specific user group, the most relevant physical parameter in the BSRN environment is the shortwave downward radiation (SWD). This measure is relevant for climate research, especially for giving statements about cloud occurrences. Although the parameter temperature is not of the highest relevance for the earth observation scientists, it was considered to be important for demonstration purposes to scientists from other domains.
(4) Visual overview: To provide a visual overview of the entire data set is a very important challenge to be addressed in order to support data exploration. The concept of a Visual Catalog showing the underlying data in one view was presented to the users via the desktop prototype. They reported the lack and the importance of such an instrument.
(5) Different query modalities: Another challenge described by the users was the support of different modalities to formulate a query. Both concepts, query-by-example and query-by-sketch, were considered relevant for their work. Moreover, including the metadata in the query formulation was important for the users.
(6) Flexibility of scientific process: As another outcome of our domain characterization, most of the users emphasized the need for flexibility in their work with time series data. As mentioned before, possible usage scenarios always vary with respect to time interval lengths, physical parameters, similarity measures, etc., based on the task and user at hand.
In summary, the concept presented via the initial desktop prototype, integrating content-based and metadata-based search, addressed many challenges expressed by the earth observation scientists.
The web-based VisInfo prototype
VisInfo [67] is a web-based DLS for the ES of time series research data collections based on visual access. Interested readers are invited to test VisInfo by following this link. 2Now, we first describe the results of the data abstraction phase and highlight the non-visual functionality. Second, we present the visual-interactive capability of VisInfo, as a result of the iterative development phase. In three subsections, we show different visual-interactive views for the content summary, the visual query definition, and the search result analysis. Furthermore, we illustrate a use case of a real-world ES scenario conducted together with domain experts within the development phase. The aim of the use case is to showcase the hypothesis generation and validation process, and thus, to assess the usefulness of VisInfo. Finally, performance aspects of the system are discussed.
Data abstraction and non-visual capabilities
The system architecture of VisInfo is structured in an administration and an application section (see Fig. 2). One datacentered goal of the administrative section is the construction of scientific workflows in a collaborative effort between data collectors, data curators, digital librarians and computer scientists. To cope with the heterogeneity of different time series research data sources, we provide a general internal time series data structure. The varieties of different time series properties are well characterized in the book of Aigner et al. [2], which served as a guideline for the development of the time series data structure used here. An extension of the DataCite metadata kernel [14] was used for the definition and storage of associated metadata. We analyzed the metadata corpus to find the most relevant metadata properties to be applied for faceted search (see Sect. 5.4). On the one hand, data collectors and librarians were asked for relevant metadata properties. Qualified metadata attributes include the contributing scientist, the measurement location on earth, and the climate and surface type (also see Fig. 12). On the other hand, the computer scientists analyzed the relevance of available metadata properties in two research efforts [8,9]. They identified interesting relations between the daily patterns and the season attribute, which was added to the facet list, respectively.. The data-centered workflow of VisInfo is shown in Fig. 7. On the basis of the general time series data structure containing the raw input data, a variety of transformations need to be applied on the time series content. The provided routines help, e.g., to (a) establish consistent data quality, (b) make the time series equidistant, (c) remove outliers, or (d) normalize the value domain. Time series descriptors transform the time series data into the feature space. For the VisInfo prototype, the Piecewise Aggregation Approximation (PAA) [3",6203,6894
10.1108/IJQRM-07-2018-0204,https://digitalcommons.unl.edu/libphilprac/6281,Dadhich et al 2021,True,Dadhich Manich,Determining the Factors Influencing Cloud Computing Implementation in Library Management System (LMS): A High Order PLS-ANN Approach,"
The principal component of this paper is to ascertain the prominent variables of technological, organizational, environmental, and financial constructs that influence library cloud computing (LCC) among the library users and professionals in the selected universities of India. This paper discusses the advantages, opportunities, challenges, and Models of Smart Library in the ICT age library management system. The study also commissioned tools viz. EFA, CFA, and structural equation assess the degree to which selected factors were associated with LCC adoption. Empirical research proposed four hypotheses by selecting the technological, organizational, environmental, and financial constructs and 16 manifests in the specified model. The model was then tested on a sample of 510 respondents of 26 major states, central and private universities of India using SEM-ANN. First, SEM was employed to find out which variables had a meaningful influence on LCC. Secondly, the output of ANN outlined the rank of influencing predictors obtained from SEM. It is evident that technological factors, greater scalability (TF_1), tech-readiness (TF_2), and easier back-up (TF_3), are the most robust antecedents of LCC. Whereas in organizational factors-recognized usefulness (OF_1), are the robust manifest, but in environmental factors-geographical reach (EF_1), administrative support (EF_2), conducive application interface (EF_4), are the significant predictors. Eventually, financial factors-costsaving (FF_1) and better return on investment (FF_2) are the considerable predictors obtained from ANN. The findings further indicate that behavioural intention to adopt the library cloud yielded novel insights that significantly benefit users and stakeholders.
",2019,International Journal of Quality and Reliability Management,,"Introduction
Documented knowledge is now shifting from the desktop to cloud platform through smart userfriendly internet applications, computers, servers, and software (Ali et al., 2018;Amron et al., 2019). There is a phenomenal transformation of notes-based content to web-linked material with the help of revolutionary techniques, i.e., cloud computing. Thus, users access all web material, documents, programs, contents, and records from any computer connected to the particular server and internet (Butler et al., 2021;Changchit & Chuchuen, 2018). The implementation of CC in libraries has turned it from a physical house of books to a virtual place (Yuvaraj, 2016). Similarly, cloud services storage, server, and deployment have dramatically changed the learning scenario and reduced owner costs. CC has also come out with a dear resolution to the current financial calamity to sustain the services provided by organizations (Goel et al., 2020). The study conducted by (Houssein et al., 2021;Phaphoom et al., 2015) elaborated that cloud computing is used at a technical level due to its user-friendliness, cost-effectiveness, and ease of interaction with viewers. Indian libraries, on the other hand, have also been planned to implement these cloud modules (Raut, 2017). Any institution considering the implementation of CC must begin by defining the kind of cloud service components conducive to the ultimate users (Tella et al., 2020). Furthermore, in the word of (Amron et al., 2019;Atiyah et al., 2015), there should be rational preparedness for its adoption to introduce any technology.
According to (Makori, 2016;Shaw & Sarkar, 2019;Wasike, 2015), CCT collects and access data and programs in a private storage hosting space over the network. (Library, 2010) elucidated CC as ""a model for empowering universal, expedient, on-demand web access to a collective pool of configurable computing reserves that can be instantly provisioned and unconfined with minimal management effort or service provider interaction"" (Md. Gulnawaz Azam, 2019). (Manish Dadhich, Hiran, et al., 2021) abridged CC as a framework for delivering on-demand educational content to figuring services through the internet. The data is kept on a server cloud that is aligned through a web, where the users need software packages and appliances to access the contents. However, (González-Gómez et al., 2016;Tong et al., 2020) mentioned that ""cloud computing is the blend of pre-existing knowhows and these skills have mellowed at an unlike pace and were not designed as a whole, they have originated together to produce a knowledge ecosystem for cloud computing."" As a result, it demonstrates that CC is a modern medium to know-how current technology shaping the entire world. 
Cloud Computing and Libraries
The cloud can be a powerful tool for conducting and managing research in the domain of library science (Schneider & Sunyaev, 2016). The evolution of CC is an outstanding task towards nextlevel integrated library systems. For the guidance for academic research, the availability of LCC is a vital source of content, and with the advent of CC, libraries are no longer left in the dark. (Helali & Omri, 2021). Many businesses are embracing the cloud as a digital technology model for ICT infrastructure and services emergencies. (Md. Gulnawaz Azam, 2019). CC allows libraries to evade locally hosting servers that require software installation, upgrade, and compatibility issues. Besides, it can synchronously make workflow greener and enable the libraries to render improved end-user searching and retrieving services (Njenga et al., 2019). Presently, a developed library assists the users to search any content through an extensive network of cloud collaborated librarians. There are many integrated library systems and software viz. Koha, New-Gen-Lib, Emilda, Open-Biblio, and Php My-Library acts as a cloud resource system (Anabel Gutierrez, 2015;Rudansky-kloppers & Bergh, 2019). This software supports CC in identifying, collecting, organizing, and broadcasting digital resources for the users and the library professionals. CC delivers many captivating possibilities for libraries, comprising the potential to diminish technology costs while increasing consistency, capacity, and efficiency for automation actions (Mansouri, S.A., Lee, H. and Aluko, 2015). In the word of (Ali et al., 2018;Qasem et al., 2019), ""CC has made strong incursions into other commercial sectors and is now commencing to find more application in library science"". It is pertinent to discuss that OCLC, a non-profit computer association library service and research group devoted to the community purpose of facilitating the world information on fingertips (Manish Dadhich, 2017). Now, OCLC has acted as a CC vendor providing cataloguing tools over the internet and permits member organizations to get on with their unified data access (Butler et al., 2021;Qasem et al., 2019). These centralized databases distribute catalogue records among many libraries, thereby reducing the time spent cataloguing and indexing contents. Development of CC for integrated LMS and retrieval system is based on global recommendation including Service-Oriented Architecture, Open Library Environment Project, and Integrated Library System for Discovery Interface (Changchit & Chuchuen, 2018;Roux & Evans, 2011).
Opportunities of Library Cloud
The usage of the library cloud renders the following prospects:
• It allows for the use of resources without knowledge of their infrastructure.
• The use of economic scale cloud computing works.
• Regulation and access are also a matter of great concern.
• Seller and service provider asserts costs for a permanent revenue stream by creation.
• The data and service are centrally stored but can be accessed from anywhere. There are a lot of advantages of CC in the present context, viz. save hardware cost, easier maintenance, easy to deploy & replace and upgrade, easy back up for the library, and the essential attribute is single server multiuser (Rudansky-kloppers & Bergh, 2019;Yuvaraj, 2016) Fig. 2
: Model of Smart Library in ICT Age
The architect of a cloud-based library is intended as a modern notion of library structure. The planned model supports cloud platforms, wireless access, and hardware solutions for arraying and managing IAAS, SAAS, PAAS for libraries (Modisane et al., 2021). The CCL framework gives librarians the ability to operate library systems both on and off-locations. It facilitates the libraries from frequent system update management (Shiju & Pramila, 2021). CC has four major types of service models: • Public Cloud -A platform where the service provider makes services accessible through the internet medium. Storage competencies, software, and virtual devices are few cases of resources that differ by service provider. The public cloud enables individuals' atomicity and resource sharing facilities, making it impossible for a single enterprise to achieve it. • Private Cloud -This framework offers one organization committed services and renders a very high control measure on stored data and restricted access. This style is appropriate for corporations that keep confidential or R&D evidence. • Hybrid Cloud -It is an IT architecture that combines public and private cloud at one platform. The system allows for instrumentation, governance, and application portability between users to create a single, scalable, and optimal cloud environment for running a company's computing workloads. • Community cloud -A community cloud is an infrastructure that enables organizations to exchange information through accessing systems and services. One or more voluntary groups, a third party, or a blend of them own, administer, and run it. Thus, equipment is shared among numerous institutions in a distinct group with shared computer needs and objectives. The idea of CC, enabled by virtualization technologies, has arisen as a modern computing model and revolutionizes information and communication technology (Goel et al., 2020;Kathuria et al., 2018). Cloud computing uses the internet as a medium for capturing, storing, and processing data. All TNCs and other small domestic companies use CC to make themselves competent in the present competitive era of the IT revolution over the Internet (Phaphoom et al., 2015). In a nutshell, this study attempts the research questions:
RQ.1: How the internal and external factors influence cloud computing implementation in Library Management System (LMS)? RQ.2: How to establish the model and validate CCA-LMS for HEIs of India? 1.3 Objectives of the Study Scientists and academics have always been drawn to technological developments and its dissemination to the public. Similarly, the purpose of this research is to determine the value of cloud computing applications in academic libraries, with a particular focus on determining the manifests of integrating cloud computing in academic libraries of India. The contribution of the study to explain why cloud hosting is not extensively accepted in Indian university libraries by examining librarians and users' perceptions on CC. The study is carried out with following explicit objectives:
• To determine the vital factors accountable for the CCA by the academic libraries of HEIs.
• To analyze the perception of end users towards cloud adopting model of LMS.
• To establish and validate a model representing core drivers of TOEF for the adoption of CC in academic libraries of India. The paper is meticulously framed to get inside factors influencing cloud adoption in LMS of Indian educational institutions. To fulfil the objectives of the research, the following section is conducive to comprehend. Section 2 portrays the review of literature, research gap, theoretical constructs, and development of hypotheses. Section 3 outlines the research frame and methodology to assess the model. Section 4 illustrates data analysis, SEM, testing of hypotheses, followed by a procedure of ANN modelling, sensitivity analysis and interpretation with the help of statistical tools. The last section considers theoretical implications, limitations, future scope, and conclusions.
Review of Literature
Two previous studies (Raut, 2017;Tella et al., 2020) have enumerated six constructs extricated from TOE and DoI theories to determine the responses of CC among IT executives in the public sector of Malaysian. The statements were a virtual benefit, compatibility, intricacy, trialability, ICT knowledge, security, and innovativeness. The same context has also been tested by (Amron et al., 2019) in their purported model for identifying the tolerability of CC among small and medium companies. (Kumar & Dadhich, 2014;Tong et al., 2020) asserted the maximum contribution of CC was to focus more on LMS, reduce the complexity of IT Applications, lower the managing costs, undermine risks, conduct broader-range distribution, and 24x7 services. (Schneider & Sunyaev, 2016) articulated the ideology of limited to unlimited data sharing concept, which was based on cloud interface. They also discussed philosophical and technical obstacles while implementing cloud networks in libraries and concluded that the digitalization of libraries with the cloud could serve the user at best with no regional boundaries. (Tripathi, 2019) outlined many social functions of a library with two fundamental aspects, i.e., satisfying social reading and deployment of knowledge. As a result, hybrid libraries may replace the physical bookkeeping system, and the future would be based on new-styled automated libraries for researchers and readers. (M. Dadhich et al., 2018;Md. Gulnawaz Azam, 2019) focused that human potential for generating and processing information has vastly outstripped their ability to manage, operate, and use it in the era of ICT. Thus, it is pertinent to put everything on one platform, i.e., the cloud. (Helali & Omri, 2021;Rakesh Kumar Birda & Manish Dadhich, 2019) articulated that people can get a lot of information, but the search costs in time, human resources, and money were extremely high. People no longer want their library to be a one-stop shop for all kinds of information; instead, they want a tailored or personalized cloud service. (Modisane et al., 2021) talked about cloud environment where the users may order customized and corresponding cloud services which can be availed through the internet. (Yuvaraj, 2016) studied that apparent comfort of use, effectiveness, and universal accessibility of the supporting expertise are robust handlers of the embracing of CC in the libraries.
Research Gap and Conceptual Model
Cloud in the context of library science will also transform traditional learning practice to cloud/econtent services rendered in the libraries. With the advent of time, the growth of cloud-based education has begun to see a subsumed change in the higher education system. Numerous determinants have also been discussed above in the context of LMS at various educational institutions across the globe. Over a period, researchers in the information system domain have established several theoretical frames to filter out the major determinants that work as catalysts and act as robust drivers for embracing new technologies. The current work employs one such theory given by (Tornatzky, L.G., Fleischer, 1990), which was again modified/utilized by current studies viz. (Anabel Gutierrez, 2015;Hiran & Henten, 2020) for CCA technology in libraries. Thus, the study intends to fill the following academic gaps:
• The present study seeks to determine factors influencing cloud management systems for Indian libraries using high order SEM-ANN approach. This dual approach in LMS has yet not been observed in the previous research.
• The research on cloud and LMS using a combination of Technological, Organizational and Environmental factors is very limited, further adding one more construct, i.e., financial factors (Bhardwaj et al., 2021;Changchit & Chuchuen, 2018) made the theoretical frame distinct. This frame may delineate the preparedness of Indian academic libraries in CCA. • The model's constructs were evaluated using artefacts found in the existing literature.
• Many emerging educational institutions have not realized the full advantage of implementing cloud management systems. There is still limited multi-factors decisionmaking studies in comparison to single objective investigations on areas like LMS. Further, a literature review of previous works was used to create the measurement constructs. The present study pursues to identify and determine the factors influencing cloud computing implementation in LMS. The CCI was evaluated using 16 items (five-point scale of Likert) referred from published sources. As suggested by past studies, all selected constructs, such as technological, organizational, environmental, and financial, comprise four items each (see table 1). 
Theoretical constructs and development of hypotheses
Physical libraries are needed to be transformed from a book-oriented approach to a user-centred system (Tong et al., 2020). Few studies viz. (Butler et al., 2021;Yu, 2021) also discussed that code the readable classification system has created incredible contribution contributions managing books in libraries, but implementing cloud cloud-based has revolutionized LMS. Thus, R&D in the field of library science has been evoked in the recent past. Eventually, CC will lead the librarianship into a new age of the techno-cloud library system that is transforming the face of the library. The past research work viz. (Qasem et al., 2019;Roux & Evans, 2011) also emphasized that in the future, the users can access the library as a service, a friend, a portal, a social leveller, a memory, an experience, and a network, among other items.
Table 1: Cloud Computing and Execution Measures

Constructs
Sources for CCA in Library Finding and Executive Measures Technological Factors (Ali et al., 2018;Ibrahim et al., 2021;Modisane et al., 2021;Shiju & Pramila, 2021) Cloud provides greater availability and scalability, equips the library techreadiness, gives competitive advantages over the traditional system, renders easier back up for users.
Organizational Factors (Amron et al., 2019;Kumar & Dadhich, 2014;Makori, 2016;Shaw & Sarkar, 2019;Wasike, 2015) Very cost saving of organizations, recognized usefulness during the covid scenario, recognized usability, security issues, unlimited storage capabilities Environmental Factors (Chauhan et al., 2021;Kathuria et al., 2018;Njenga et al., 2019;Rana et al., 2018;Tripathi, 2019) Furnish geographical reach to endusers; administrative support is pivotal, vendor supports in pre-requisites, understandable application interface. Financial Factors (Changchit & Chuchuen, 2018;Mansouri, S.A., Lee, H. and Aluko, 2015;Yuvaraj, 2016) Financial freedom, cost-saving, better return on investment, subsidies Hypotheses for the Research H1: There is a connotation between the technological factors and CCA in libraries. H2: There is subsume association between organizational factors and CCA in libraries. H3: Cloud computing implementation in libraries is influenced by environmental factors. H4: Financial factors have a strong association with CC of libraries.
Research Layout
Primary and secondary data were designed in a systematic frame to simplify the procedure of research. The surveys helped to gather the raw data from ultimate sources and the collection of secondary data outlined from unswerving sources (Manish Dadhich, Purohit, et al., 2021; Dubey, R., Gunasekaran, A., Childe, Stephen J., Papadopoulos, T., Luo, Z.,Wamba, S.F., 2019). The first phase entailed EFA and CFA trailed by SEM in the subsequent level. With the assistance of SPSS-21 and AMO-22, the collected data was processed, scrutinized, and construed in the system framework. Having measured 510 respondents from India's state, central and private universities, the research framework implied a lucid pattern. Besides, the questionnaire is divided into two segments. The first segment consists of seven questions about the demographic feature of the panellists and librarians. The second section contains four constructs made up of 16 variables. The research was performed from January to April 2021.  Anderson, n.d.;Langevin et al., 2020). In the words of (Lepore & Spigarelli, 2020), a sample size of 510 is likewise sufficient for a model with 25 or more manifest. The number of elements and levels of commonality of every gauge in a model also influence the sample size.
Common Method Variance (CMV) evokes spurious support for the tested ideas. In research of behavior and social science, the effect of CMV is a vital validity issue (Hair Jr, Joseph F., G. Tomas M. Hult, Christian Ringle, 2016). The researchers applied statistical tests to reduce CMB by putting the dependent and independent factors in different portions of the questionnaire. In addition, the researchers ensured that the assertions in the items are clear, simple, and unambiguous. The analysis also demonstrates that most manifests have a correlation of less than 0.5, indicating that the data sets are not affected by multicollinearity. It signifies that the study's constructs were not redundant in nature.
Data Analysis and Explanation
The component of the demographic indication pertaining to the respondents of the CC users in the library is delineated in table 2. All items spotted mean values of more than 3.40 and less than 5.00. Most of the values of σ were less than 1.0, suggesting not a wide dispersion from the mean. Besides, the Kaiser-Mayo-Olkin score was used to measure the adequacy of the sample size. The measured score of 0.864 proposed that the samples were sufficient to accomplish the factor analysis. Determinants of CCA were analyzed, and table 4 articulated the G'F'I (0.926), A'G'F'I (0.802) along with N'F'I (0.856), and R'F'I (0.823) whereas C'F'I (0.910), Tucker-Lewis delineates 0.908 and the value of RMSEA was 0.025 that specifies that the anticipated model is in a good fit (Manish Dadhich, Purohit, et al., 2021;Gupta & Nagpal, 2020).  The factor correlation among latent items has to be less than the SQRT of the average variance explained of each factor (Anil & K.P., 2016). With all figures, it has been inferred that the model meets the criteria of reliability, the validity of substance, convergent validity, and discriminant validity. In this way, it was succeeded by the testing of the structural equation model. The discriminant validity infers the degree to which dormant items are diverse from other dormant variables in the selected frame. Besides, the construct correlation among dormant items must be less than the square root of AVE of every factor (Mashelkar, 2018). With these all standards, it has been confirmed that the model estimates reliability criteria, the rationality aligns with convergent validity. In this way, it was accomplished by the perusing of the SEM.  The rationalized SEM model exhibits theorized connotation among the latent variables. The assessment of standardized regression loads was applied to fetch an appreciation concerning the proposed disposition, as signified by (Agrawal, 2019;Rakesh Kumar Birda & Manish Dadhich, 2019;Singhal, 2020). As mentioned in the above table that βeta, S. Er, Cri. ratios were positive, and null hypotheses can be rejected because the computed p-values of all four projected hypotheses were technological factors (0.040), organizational factors (0.038), environmental factors (0.032), and financial factors (0.026) were significant as p<0.000. Hence, expressed hypotheses were supported and accepted. The established model makes a substantial contribution to the literature in this field. Similarly, few studies concentrate on adopting CC from the perspective of users, which has entirely transformed the LMS in Indian universities.
LMS Analysis of ANN
By combining SEM with artificial intelligence methodologies, this work signifies a multianalytical strategy. SEM and Multiple Regression are linear statistical techniques that can only find linear relationships, potentially simplifying complex decision-making processes (Omar et al., 2015). To overcome this challenge, an ANN model that can recognize non-linear correlations is proposed. An advantage of this practice is that the NNM may learn multifaceted linear and non-linear correlations between predictors and adoption choice (Manish Dadhich, Hiran, et al., 2021). Further, neural networks do not test hypotheses and analyze causal relationships due to their 'black-box' nature. As a result, a two-stage strategy is adopted in this work, similar to (Sadasivam & Lakshme, 2016). In the first order, a structural equation is employed to evaluate all the hypotheses and extract pivotal predictors in the model that are further subsumed as inputs to the ANN in the second order to measure the status of each manifest. ANN acquires knowledge during the learning process and stores it in synaptic weights, which are interneuron connection strengths like those found in the human brain. Based on the research for this study and previous work on technology adoption, a concept for calculating the number of latent neurons in one hidden stage and one output is proposed (Naresh Kumar, 2016).
Simulation experiments demonstrate that increasing the sum of neurons in the hidden layer improves estimation accuracy to some extent (R. D. Raut et al., 2018); nevertheless, taking too many of them dramatically increases the computational weight. Assume that the number of concealed neurons is excessive. In that circumstance, the network may memorize all training instances and not generalize or produce adequate output with data not used during training. Trialand-error is widely used because there is no heuristic technique for calculating the number of hidden neurons.
Results of ANN
Python & SPSS was used to evaluate the NN approach. At this step, the statistically imperative elements from the SEM were included in the model. Four hypotheses have been identified as critical for future investigation based on the structural equation's findings. As a result, these elements were represented as input variables in the input layers; in this case, the output layer's dependent variable was library cloud management adoption. A cross-validation technique was also employed to solve the model's over-fitting problem. Both the hidden and output layers use the sigmoid function to activate neurons (Foo et al., 2018). All inputs and outputs were standardized to improve training efficacy and give shorter training times and higher performance [0,1]. To avoid overfitting, ten-fold cross-validation was executed, with 90% of the data being used for network training and 10% for testing and determining the accuracy of the trained network (Dwivedi et al., 2021). The RMSE of both the training and testing data and the mean and standard deviation for both data sets are computed for determining the model's prediction accuracy. The ANN model comprises four constructs as input neurons in terms of variables, followed by one output-LMS.   
Theoretical Implications
At the outset, the integration of financial factors in this research model has provided a novel theoretical contribution to adopting library cloud management systems. With incorporating typical features of the panellists/users viz. internet bandwidth, usages of RFID technology, and user strength of library, scholars may better understand their effect on LCC. This theoretical outcome may be conducive to fetch groundwork for imminent researchers in the field of technology adoption. The research may shed light on several new manifests by adopting TOE theory in LMS at the university level. Thus, the cloud being at a budding stage has been exposed to scholarly argument and relevance in the field of LCC. These outcomes help CSP to understand how cloud content can entice user willingness and adoption at one end. Similarly, data on the cloud can be beneficial for users, but assuming privacy and security is vital for service providers. Further, few studies have also demonstrated that CCA is positively associated with the institutions' performance. Our findings also validate that the factors TOEF influence CCA in LMS in education institutions. Cloud provides greater availability, scalability and equips the library for tech-readiness. It also elasticities competitive advantages over the traditional system and renders easier back up for users. The study strongly endorses CC as possibly communicating and extending the opportune time for the shifting from the brick-and-mortar to the system of the cloud-based LMS.
Limitation and Future Scope
There is nothing ideal in this universe because shortcomings and weaknesses are part of a system. This research is no exception. The use of past-reported instruments for data collection and the small sample size are two notable drawbacks. As a result, instead of the restricted, potential researchers should consider expanding the reach of the analysis to include more librarians from other universities. The study only looked at specific factors for LCC, but several other constructs may also be explored further. As a result, budding researchers should glance at other variables to see how they influence CCA and indulge in comparative studies using snowball sampling. The findings of this research can be a firsthand option as a reference point for further research. Further, these investigations are declarative; they are prone to human error. Thus, certain interesting future research topics would perform a longitudinal study to identify changes in LCC adoption among educational institutions with specific needs. Besides, other constructs viz. cloud service provider, vicinity, users' attitude, and compliances may be vital in this subject matter but were not merged in this study.
Conclusion
Cloud computing servers are available in several different forms. The study discusses how the variables of technological, organizational, environmental, and financial factors exhibited the adoption of library cloud networks in the present pandemic scenario. The study portrays that all four dimensions have a notable impact on the CCA of the Indian universities and how CC expertise leads to change in delivering content for HEIs. This attempt replies to the current hums for empirical research in library cloud computing and its implications for future advancement in the education industry. The paper employed tools viz. EFA, CFA, and Structural Equation casting assess the degree to which selected factors to LCC espousal and their relative significance. An empirical study planned four hypotheses by selecting the constructs viz. technological, organizational, environmental, and financial aspects and 16 variables in the model specified. The model was then tested on a sample of 510 respondents of 26 major states, central and private universities of India using SEM-ANN. The consequences emphasized the relative importance of selected constructs for enhancing the LCC's better access cloud contents. The calculated p-value of all four projected hypotheses was TF (0.04), OF (0.03), EF (0.03), and FF (0.02) were momentous as p<0.000. These constructs are again used as inputs to the ANN to govern the relative position of each predictor in a second stage. The results of the ANN outline the importance of every input variable to predict how much the value is predicted by the neural network with the unlike figure of independent items. It is evident that TF_1 (99%), TF_2 (96.1%), TF_3 (63%), TF_4 (77%) is the highest explanatory factor in LMS. Organizational construct is explained by ORF_1 (71%); environmental factors consist of EF_1 (75%), EF_2 (100%), EF_4 (68%) that were vital enough for the adoption of LMS cloud. Eventually, FF_1 (49%) and FF_2 (63%) items scored highest to explain cloud adoption for higher education institutions in India. This study furnishes a clear indication of the pluses of identifying the vital elements of LCC and efficient cardinal technology adoption. These technologies provide a platform for sustainable learning through a resource management system. These results may offer valuable insights to technocrats toward digitalization and the knowledge deployment in the campus and off the campus. However, previous studies (Anabel Gutierrez, 2015;Goel et al., 2020;Kathuria et al., 2018;Phaphoom et al., 2015;Tong et al., 2020) acknowledged the constructs and were in line with few principal components viz. LCC, adoption, and knowledge performance in HE. The findings of the study reveal that CC is an incredible notion for academicians, librarians, and other users on various fronts and delineated an extreme level of acquaintance with the cloud concept.",6373,7323
10.3145/epi.2012.ene.02,https://digitalcommons.cwu.edu/libraryfac/15/,Fu 2014,True,Ping Fu,Supporting the Next-Generation ILS: The Changing Roles of Systems Librarians,"
This paper compares current responsibilities of systems librarians supporting the traditional ILS with anticipated responsibilities associated with supporting the nextgeneration ILS and examines how the roles of systems librarians will change in migrating to the next generation ILS from the traditional ILS. The method used for this study is content analysis. The content sources are online job banks for keeping an archive of past listings over the past five years. The analysis results demonstrate a shift is happening where the primary roles and responsibilities of systems librarians supporting the next-generation ILS are becoming more human/organizations related, while those positions supporting the traditional ILS show that top roles are concentrated on information technology. Overall, this suggests that systems librarians are expected to manage much less in terms of tasks directly related to information technology. Consequently, systems librarians should re-engineer themselves accordingly so that they will be able to support more critical issues in the library.
",2012,Library Journal,,"
Systems librarians play a critical role in academic libraries (Iglesias, 2010). They are the experts who not only understand libraries and information technologies, but also enable these two fields to work seamlessly together as a whole (Iglesias, 2010). The evolution of the Integrated Library System (ILS) has dramatically changed not only the responsibilities and duties of the positions, but also the basic knowledge, skills and abilities expected by employers.
The origin of the ILS in libraries dates back to the late 1960s and early 1970s, when computer technology was used to automate the processing of print materials and as an electronic version of the card catalog (Epstein, 1983). In the 1970s and 1980s, due to the innovation of computer technology and telecommunications, the first generation ILS was invented as a character-based mainframe application, which included staff modules for cataloging, acquisitions, circulation, serials, administration, and the character-based Online Public Access Catalog (OPAC) interface (Saffady, 1994). Systems librarians' positions were created to manage different aspects of these independent automation systems such as maintaining mainframe hardware, operating systems, and graphic terminals. The evolution of the Internet throughout the 1990s and into the 2000s resulted in the appearance of the second generation ILS (Hart, 2001), commonly known as the traditional ILS. The traditional ILS is built on the client-server computing model (Ross & Marmion, 2000) and delivers modular functionality (Majumdar & Singh, 2004). The traditional ILS improved greatly during the 2000s. Today, most libraries are using a well-established proprietary or open source traditional ILS. While the traditional ILS handles library print materials very well, it has limitations concerning the management of details for electronic resources such as licensing information of databases and ejournals.
Because modern libraries are more heavily involved with electronic content, they now have to purchase and manage multiple systems to handle their multiple collections. In order to manage electronic materials, for instance, vendors have developed the Electronic Resource Management Systems (ERMS) and other tools that exist either as a standalone system or as a built-in module in an ILS. Accordingly, systems librarians' responsibilities and roles have been greatly expanded (Rhyno, 2013) to manage the ILS, ERMS, link resolvers, and other add-ons. The traditional ILS requires systems librarians to invest significant time in the maintenance and upgrades of hardware, software, databases, and applications. Because systems librarians are largely on their own in integrating these systems they are required to have certain advanced knowledge and skills in library information technology.
In order to provide a single, unified management system for libraries to manage their print, electronic, and digital materials, vendors started to reintegrate or reinvent their traditional ILSs. ILS vendors and open source ILS developers expect to replace their traditional ILS, ERMS, link resolvers, and other add-ons with a fully integrated unified system (Breeding, 2012b). Service-orientation architecture (SOA) principles, cloud computing technology, Application Programming Interface (API), and other modern information technologies are adopted by these vendors and open source developers in the development of their next generation ILS (Breeding, 2012b). Since the next generation ILS will be deployed in a cloud-computing environment, libraries do not need to purchase or install anything locally. This allows libraries to ""subscribe"" to the nextgeneration ILS. In accordance with the subscription, the maintenance and updates of infrastructure, software, and applications will be provided by vendors (Breeding, 2011). This innovation will impact the responsibilities and roles of systems librarians and the knowledge, skills, and ability requirements for systems librarians supporting the nextgeneration ILS.
In order to identify and measure the changes mentioned above, this study will analyze the contents of online job postings for systems librarians over the last five years, and an examination of vendor staffing proposals will provide a look into the future for positions supporting the next generation ILS. This analysis will examine the changes in fundamental roles placed on systems librarians who must manage the next-generation ILS. As a response to these role changes, this study will discuss threats, opportunities, and challenges being faced by systems librarians today.
Literature Review
Many studies regarding systems librarianship have been conducted and presented in the library literature. In light of these studies, Liu and Cai (2013) found that cloud computing has a significant impact on systems librarianship. However, through needs assessment and impact analysis, Liu and Cai (2013) found there was not enough evidence to prove that cloud computing might negatively impact a systems librarian's career. While cloud computing specifically might not be a threat, Liu and Cai (2013) suggested systems librarians improve their knowledge and skills to meet the new challenges in the field. Fu and Fitzgerald (2013) conducted a comparative analysis on how the software architecture and the workflows/functionality of the traditional ILS and the nextgeneration ILS may impact system and technical services staffing models at academic libraries. They suggested that ""redefining staff job descriptions and reorganizing library organizational structures might be necessary in order to better adapt to the changes brought about by the next-generation ILS"" (p.57). Breeding (2012a) claimed that the next-generation ILS would eliminate many hardware and maintenance investments for libraries. Breeding (2012a) advocated that the nextgeneration ILS utilizes Web-scale technology deployed via cloud environment, so vendors can centrally manage the majority of systems tasks that had been performed by local systems staff in a traditional ILS environment. Sutton (2011) examined the relationship between the knowledge, experience, and skills expected of systems librarians and the curriculum and support offered to library students at ALA-accredited programs. Sutton (2011) gathered and analyzed the contents of online job postings for systems librarians in the previous five years to determine what employers were looking for when hiring systems librarians. Sutton (2011) also examined the websites of ALA-accredited institutions to determine what the schools' curricula, course offerings, and career assistance offered students interested in systems librarianship. Sutton found six of the top 10 most frequently required knowledge and skills were human/organization related. The most frequently mentioned requirement was for communication skills, required in about 70% of jobs.
Iglesias (2010) presented a series of case studies in his book on how the roles of systems librarians have been impacted by the fact that libraries now purchase or subscribe to more online databases hosted by vendors. The book explored how Web technologies and shifts in technology management impact the profession. The chapters provide insight and information on the shift in systems librarians' roles towards acquiring more expertise and experience in dealing with external library service providers. Goetsch (2008) showed academic libraries are creating new job roles and/or reinventing traditional positions to meet new and emerging user needs brought about by technology, globalization, and financial crisis. Through a content analysis of selected job vacancy announcements in the last decade, Goetsch (2008) examined the impacts, benefits, and tensions that this change in users has brought on academic libraries. Ingersoll and Culshaw (2004) showed that the primary responsibilities and roles ascribed to systems librarians lies in the areas of planning, staffing, communication, development, service and support, training, and daily operations. The primary work tasks of most systems librarians, however, include ILS administration, server management, workstations maintenance, software and applications maintenance and upgrades, configuration, patch management, data backup, printing issues, security, and inventory. Ingersoll and Culshaw (2004) also emphasized systems librarians should be proactive in facing constant change and keep abreast of emerging library technologies. Rhyno (2003) argued that mainstream Web technologies, such as XML, have been widely used in library systems and applications, and thereby expanded the role of systems librarians. Guinea (2003) examined the role of systems librarians in the administration of a university ILS. Guinea (2003) found that systems librarians served as a bridge between library and other university units in the development of library-initiated projects and the promotion of information technology-based applications. Xu and Chen (1999, 2000, 2001) in a series of studies from 1999 to 2001 examined 133 systems librarian job advertisements from January 1996 to December 1997 and compared the results with a survey of employers and newly hired systems librarians through content analysis. Xu and Chen (1999) found that a strong background in information technology is required for the job of systems librarians.
Methodology
For the purpose of this analysis, a systems librarian job is defined as a librarian whose primary responsibilities are directly or indirectly related to the management of an integrated library system where a degree from an ALA-accredited institution or equivalent is required or desired. This definition likely excludes some comparable positions for which a MLS/MLIS degree or equivalent is not required. Different job titles, such as ""Systems Librarian"", ""Systems Coordinator"", ""Automation Librarian"", ""Head of Systems"", ""Technology Librarian"", will be regarded as equivalent job titles as long as they meet the definitional criteria mentioned above. Two types of positions were gathered and categorized. The first category supports the traditional ILS such as Millennium, Voyager, and Aleph, while the second category supports the next generation ILS such as Alma, WMS, and Sierra.
The analysis was conducted using free online job postings from the last five years. While Sutton's (2011) and Xu's (2001) studies focus on gathering the knowledge, skills, and abilities of systems librarians and analyze how current education meets the industry's needs, this study intends to create a list of typical responsibilities of systems librarians and then determine how the roles of systems librarians are expected to change as a result of adopting the next generation ILS. The sources compiled are from online job banks that keep an archive of past job listings, including code4lib jobs, ALA JobLIST, and various university job listing sites. The content from all sources were gathered together in a single spreadsheet in order to facilitate its organization and manipulation. Duplicates and re-posts were removed. The responsibilities and duties described in each job description were examined for similarities in order to determine a typical list. Specific responsibilities such as administering an ILS were listed individually, while more general responsibilities, definitions of which vary from one posting to another, were grouped together under an appropriate heading. All postings were examined a second time once all categories had been determined to ensure complete coverage. Due to the fact that only a few positions for managing the next-generation ILS are available in the job market, in order to support an in-depth analysis, vendors' claims were gathered and grouped from their documents, webinars, product demonstrations, and RFP responses.
Analysis and Results
A total of 52 job postings were gathered over the past five years (see Tables 1 and2). Among these jobs, 7 advertisements (see Table 3) indicated that their libraries were one of the next-generation ILS early adopters or intended to migrate to the next generation ILS.  The analysis result shows (see Table 4) 65% of positions use titles such as ""Information Systems Librarian"", ""Integrated Library Systems Librarian"", ""Systems Librarian"", and ""Senior Systems Librarian"", which indicate that the incumbents play a role as Systems Librarian only, while 35% of the job titles explicitly require that the incumbents must play an additional role to support another area other than systems. The job titles appear as a combination of the two areas such as ""Systems and Acquisitions Librarian"", ""Systems & Web Development Librarian"", ""Systems & Serials Librarian"", ""Systems and Technical Services Librarian"", ""Systems and Electronic Resources Librarian"", etc. Regarding the responsibilities and roles of systems librarians supporting the traditional ILS, as shown in Table 5, five of the top seven roles are concentrated on information technology, which implies that systems librarians must have certain advanced knowledge and skills in information technology. Approximately 80% of positions indicate that systems librarians are a systems administrator who administers, supports, and enhances a variety of library systems, including but not limited to the integrated library system, link resolver, proxy server, federated search system, archives management system, and interlibrary loan management system. A systems librarian must also be a system implementer who performs activities including software installations, integrations, configurations, upgrades, patches or other fixes, and enhancement, in addition to the day-to-day management, maintenance, troubleshooting, and user support.
Seventy-seven percent of the jobs require systems librarians to act as an operator to perform data loading and validation. Sixty-seven percent of the positions demand that systems librarians be technology leaders to keep abreast of developments in library technologies and maintain current awareness of information tools. Fifty-two percent of the positions request that systems librarians be report generation experts who participate in the design and coordination of statistical and managerial reports. Fortyfour percent of the positions require systems librarians to be webmasters who provide vision and leadership in designing, developing, and supporting library websites by integrating them with the larger library Web presence including discovery tools, digital collections, electronic resources, and other Web services. Some positions require systems librarians to identify, develop, and implement new Web applications and tools, particularly for mobile environments. Forty percent of the positions designate that the systems librarian serve as an expert resource within the library regarding core production systems and applications, and also provide advice and consultations to library staff to maximize effective use of technology.
The rest of the roles listed in Table 5 are related to the human/organization facet of the profession. This result is consistent with Sutton's (2011) finding that six of the top ten most frequently required knowledge and skills were human/organization related. Sixtyfive percent of positions require that systems librarians serve as liaisons to IT units on campus to coordinate hardware network maintenance, upgrades, and integration with other enterprise systems. Systems librarians also serve as primary representatives and contacts to the designated library system vendors to coordinate systems, databases, and applications maintenance and upgrades. Fifty-eight percent of positions ask systems librarians to be project managers who lead and manage library projects that include the implementation of new applications to meet the needs of students, faculty, staff, and community users of the university. Forty percent of positions require systems librarians to supervise library IT staff and set priorities for the systems department on a regular basis. Twenty-eight percent of positions assign systems librarians as a faculty liaison for selected disciplines to participate in collection development, engage teaching faculty in selection and acquisition activities, and develop and maintain disciplinespecific Web research guides. Nineteen percent of positions require systems librarians to advise library directors in regards to library technology issues, Web trends, cataloging and collection development, and to serve as members of the library's management team. Nineteen percent of positions expect systems librarians to actively participate in the formation and implementation of library policies and procedures and long-term strategic planning.
In addition, sixty-one percent of positions want systems librarians to perform general library duties such as reference service, orientations, library workshops, circulation, collection development, program planning, and evaluation. Some positions may require systems librarians to work at multiple locations within the institutions and may involve day, evening, and weekend assignments. The general duties also include curriculum development, participation in the governance process through engagement in scholarly pursuit and other professional activities, committee work, and student activities. Sometimes systems librarians are required to serve as library directors in the absence of directors as requested.
Regarding the seven positions (see Table 3) anticipated supporting the next-generation ILS, the roles of systems librarians are listed in Table 6. Because the seven positions above are tasked with maintenance of a traditional ILS while anticipating the transition to a next-generation ILS in the future, more information regarding future staff roles was needed. To supplement the low number of job postings dedicated to supporting a next generation ILS, this study also gathered ILS vendor's claims from documents, product demonstrations, and RFP responses regarding staffing requirements. Many different claims from vendors were found. For example, OCLC generally estimates that WMS can save 90% of local systems workload, while Ex Libris seems to have a more reasonable and descriptive explanation. The table below lists the expected level of local systems staffing required for managing the next-generation ILS. The content is adopted from the Ex Libris's Response to Request for Proposal of the Orbis Cascade Alliance's Shared Library Management System, February 29, 2012. As shown in Table 7, the essential tasks of supporting the next-generation ILS have dropped to six from the 19 associated with supporting the traditional ILS. It seems that the systems administration role still appears at the top of the roles in Table 6. However, one reasonable explanation is, as shown in Table 7, that systems administration responsibility is potentially limited to managing the following: service calls tracking, configuration and customization, discovery interface, firewall, local LAMP (Linux operating system, Apache HTTP Server, MySQL database software, and PHP, Perl or Python), and the external cloud environment. Therefore, the actual systems administration responsibility might drop greatly. This result offers evidence to support Breeding's (2012a) claims that the next-generation ILS will eliminate many hardware and maintenance investments for libraries.
As shown in both Table 6 andTable 7, the liaison role stands at the top of the list, which is more human/organization related, as compared to the sixty-five percent that supports the traditional ILS. The analysis result suggests that greater communication is crucial, whether it is between systems librarians and IT for issues of network firewall management, or with vendors to solve issues such as logging and tracking service calls and upgrades coordination. The weight on the roles of serving as a technology leader and an expert source are expected to be greatly increased as well. The next-generation ILS will bring many new technologies and challenges to library staff. The analysis results support Fu and Fitzgerald's analysis (2013) that the more integrated workflows and functionality of the next-generation ILS ""allow library staff to work with more modules, play multiple roles, and back up each other, which will bring changes to traditional staffing models"" (p.57). Thus, systems librarians need to develop more human/organization skills than ever before. Training and communications skills take on a larger role as systems librarians are asked to provide training for staff to help them adapt to the change, and assist staff to better understand technological possibilities including changes to workflows and functionality offered by the next-generation ILS. Since the next-generation ILS utilizes a discovery layer interface, systems librarians should expect to take care of the discovery interface's maintenance, configuration, and customization.
Additionally, the weight on the role of serving as a project manager is expected to be increased, particularly during the transition and migration from the traditional ILS to the next-generation ILS. Systems librarians must show leadership and project management skills in ILS migrations during the phases of planning, preparation, data extraction and loading, configuration, project management, and coordination.
Conclusion
The analysis demonstrates a shift in the primary roles and responsibilities of systems librarians brought about by the adoption of the next-generation ILS. Positions for systems librarians supporting a next-generation ILS are becoming more human/organization related, while those positions supporting the traditional ILS show that top roles are concentrated on information technology. As the next-generation ILS becomes the norm, systems librarians will be expected to manage much less in terms of tasks directly related to information technology. Going forward it seems the maintenance and upgrades of computers, servers, operating systems, databases, and client applications will be centrally managed by vendors or cloud hosting services. Systems librarians no longer need advanced knowledge and skills to handle hardware and software; however, they do need some knowledge and skills to manage firewalls and customize cloud resources. Meanwhile, systems librarians are expected to increase their responsibilities and roles greatly in more human/organization related tasks. These tasks include communication with vendors and coordination with university IT, strong familiarity with workflows and functionality, staff training, and discovery interface configuration and customization. Additionally, systems librarians are expected to show strong leadership capabilities and excellent project management skills for the transition from the traditional ILS to the next-generation ILS. The role change of systems librarians in the direction of human/organization skills requires systems librarians to reengineer their knowledge and skills. This knowledge and skill will facilitate the process of becoming an expert source for staff regarding new workflows and functionality of the next-generation ILS.
Ideally this analysis result can provide some useful information and insight for potential systems librarians, library schools, and employers. Finally, future studies should be considered to reexamine the conclusion of this study when more positions associated with supporting the next-generation ILS are available.",4249,4885
-,https://digitalcommons.cwu.edu/libraryfac/30/,Fu and Carmen 2015,True,Ping Fu,Migration to Alma/Primo: A Case Study of Central Washington University,"
integrated library system to Alma/Primo, Ex Libris' next-generation library management solution and discovery and delivery solution. A chronological review method was used for this case study to provide an overall picture of key migration events, tasks, and implementation efforts, including pre-migration cleanup, migration forms, integration with external systems, testing, cutover, postmigration cleanup, and reporting and fixing outstanding issues. A three-phase migration model was studied, and a questionnaire was designed to collect data from functional leads to determine staff time spent on the migration tasks. Staff time spent on each phase was analyzed and quantitated, with some top essential elements for the success of the migration identified through the case review and data analysis. An analysis of the Ex Libris' Salesforce cases created during the migration and post-migration was conducted to be used for identifying roles of key librarians and staff functional leads during the migration.
",2012,El Profesional de la Información,,"I. Introduction
Today, many academic libraries are moving from traditional integrated library systems (ILS) to the next-generation ILS (Breeding, 2012). For a medium-sized academic library system like Central Washington University Libraries (CWUL), a member of the Orbis Cascade Alliance (OCA), the prospect of migrating to a new system was determined by the consortium. The OCA is a nonprofit consortium of 37 colleges and universities in Oregon, Washington, and Idaho. One of the Alliance's visions is to enhance collaborative technical services and cooperative collection development among 37 member institutions (Cornish, Jost, & Arch, 2013). In July 2012, the Alliance decided to select Ex Libris' Alma library management system and Primo discovery service for all Alliance libraries (Cornish, Jost, & Arch, 2013). Before the migration, all Alliance libraries, including CWUL, were using the Millennium system from Innovative Interfaces Inc., except for one library using an open-source system called Evergreen and another one using the Voyager system from Ex Libris. The migration of the 37 libraries was split into four cohorts (Drake & Cornish, 2014). CWUL was in the fourth cohort, the migration of which started in July 2014. The information about this process can be found at the Orbis Cascade Alliance website https://www.orbiscascade.org.
CWUL is a medium-sized academic library with forty-one librarians and staff. At the beginning of the migration, the Millennium ILS contained: The electronic resources were managed by ProQuest's Serials Solutions 360. There were about 130 databases and 30,000 e-journal titles. More information about CWUL can be found at the libraries' website http://www.lib.cwu.edu/. This paper describes how CWUL interacted and collaborated with the OCA SILS Implementation Team and Ex Libris to process systems and data migration from Millennium to Alma/Primo. A chorological review method was used in this case study to provide an overall picture of key migration events, tasks, and implementation efforts. The key events and tasks such as pre-migration cleanup, migration forms, integration with external systems, testing, cutover, post-migration cleanup, and reporting and fixing outstanding issues are reviewed one by one. A questionnaire was designed to collect data from functional leads to determine staff time spent on the migration tasks. An analysis of the Ex Libris' Salesforce cases created during the migration and post-migration was conducted to identify roles of key librarians and staff functional leads during the migration. And a three-phase migration model was applied. Staff time spent on each phase was analyzed and quantitated. Some top essential elements for the success of the migration are also identified through the case review and data analysis.
II. Literature Review
Many studies have been published on ILS migration. However, only a few of them focus on the next-generation ILS migration.
In light of these studies, Julich, Hirst, and Thompson (2003) described a traditional ILS migration at the University of Iowa. Their study focused on a number of areas of the Aleph 500 ILS migration, including system selection, implementation project structure, project management and tracking, hardware, vendor relations, public relations, data conversion, systems administration, functional testing, functional problems, training, local programming, staff client, switch to production, batch/reports, news releases/software changes, and postscripts. Compared to the next-generation ILS migration, the traditional ILS migration is a more complex and timeconsuming process. They also analyze and quantitated staff efforts during the selection process and implementation.
Cervone (2007) discussed some issues in ILS migrations. First of all, a migration is a timeconsuming and generally thankless task. Many libraries are moving to the next-generation ILS ass it not only replaces the library management back-end system but also improves greatly the front-end user interface. Besides, the selection of an ILS vendor is very important in the early stages of the migration. His study elaborated three major phases of a migration, i.e., systems selection, implementation, and production stabilization. He emphasized that establishing functional working groups early in the project is critical for the success of the project overall. Cornish, Jost, and Arch (2013) described the factors that lead OCA to move to the shared library management system of Alma/Primo, describing the steps of the new system selection, including the work of several research and planning groups and a formal Request for Information process. The Alliance Council's decision to move to a shared ILS is to support the Alliance's vision ""for the shared library management system, including collaborative technical services and cooperative collection development"" (p16). Fu and Fitzgerald (2013) focused their study on the impact of the next-generation ILS on staffing at medium-sized academic libraries. Moving to the next-generation ILS may impact the staffing model due to the new architecture and functionality of the next-generation ILS. The nextgeneration ILS allows library staff to work with more modules, play multiple roles, and back up one another. In particular, paraprofessional staff could play more critical roles in library technical services and daily operations. In other words, the next-generation ILS might bring changes to traditional staffing models. For instance, systems staff will spend less time in hardware and software maintenance and backup. Instead, they will spend more time in local applications development and communication with vendors. It is important that libraries provide staff more opportunities and training to help them better adapt to the changes brought about by the next-generation ILS. Fu (2014) studied the changing roles of systems librarians and found that they served as a primary leader and a project manager in the ILS migration. Compared with a traditional ILS, systems librarians in the next-generation ILS have less technical operation to manage but concentrate more on the collaborating with library departments and other units on campus, maintaining a good working relationship with ILS vendors on software upgrades, fixing and reporting issues, troubleshooting, and providing staff training. Singh (2014) showed that the case study method has become widely used in many disciplines, including library science. The case study method is particularly useful in describing the process of an ILS migration, as it focuses on individual incidents in the real world and concentrates on a single unit of an organization. Vaughan and Costello (2011) observed that shared systems managed by a more formalized, official consortium have become a trend for libraries. Their study focused on how a consortium shared an ILS and managed their system regarding cost sharing, support, and rights and responsibilities. In addition, they stressed the importance to formalize the coordination role of systems librarians in their capacity as the chief manager of ILS.
III. Scope and Methodology
The case study method is used in this study to make a detailed analysis of the ILS migration at CWUL with a view to fully understanding the migration and deriving more general theoretical statements from regularities observed. The instruments used to gather data include direct observation, participant observation, emails, Alliance website, Library website, Library wiki, Library shared drives, surveys, meeting minutes, internal documents, and Ex Libris Salesforce cases.
The participants' observation was primarily chosen for data gathering in this case study because one of the observers fully led the project while another observer participated in the system migration. A chronological review of the key events and tasks was chosen for analyzing the data generated during the migration. Through this chronological review and analysis, a three-phase migration model was identified as follows:
 Phase One, defined as the pre-migration cleanup and preparation phase, started on July 1, 2013 and ended on June 30, 2014.
 Phase Two, defined as the migration, testing, and training phase, started on July 1, 2014 and ended on November 18, 2014.
 Phase Three, defined as the cutover, post-migration cleanup, and reporting and fixing outstanding issues phase, started on Dec 19, 2014 and ended on May 30, 2015.
Since CWUL started its pre-migration cleanup and preparation one year in advance, the entire migration process lasted almost two years.
In addition to the chronological review, a survey was designed to collect information and data from functional leads regarding staff time spent on each phase during the ILS migration. Six specific questions were specifically designed for this survey.
Furthermore, 185 Ex Libris Salesforce cases created by CWUL during the migration and post migration were analyzed. The results of the analysis were used for measuring outstanding issues which occurred in each functional area and for assessing the roles of the project manager and functional leads.
The selection of the vendor and ILS was usually a part of an ILS migration (Wang, 2009). However, since the selection decision was made at the consortial level, it was excluded in this case study.
IV. Migration Phases

Phase One: Pre-migration Cleanup and Preparation
The meeting minutes of the CWUL Shared Integrated Library System's (SILS) Implementation Team show that Phase One started on July 1, 2013 and ended on July 30, 2014. During Phase One, several key tasks and events were reviewed. The first major task was to build a wellorganized team. The team was officially named as the CWUL SILS Implementation Team on July 1, 2013 by the Dean of the Libraries. As the project manager of the ILS migration, the systems librarian convened the first team meeting. The members of the team consisted of representatives from each department, including both department heads and functional leads. The University Information Technology (IT) also had its representative on the team. The charge of the team, the roles of the project manager, and the roles of team members were widely discussed and briefly defined. The communication mechanisms were discussed and a documentation center was created by the project manager after the first meeting. The Library wiki was chosen to collect information and store resources from OCA SILS Implementation Team, Alliance working groups, peer institutions, vendors, and institutions outside the Alliance. The internal documents created by the CWUL SILS Implementation Team were stored in the institution's shared drive. Sub-teams and taskforces were formed at the first meeting. The CWUL SILS Implementation Team decided to meet monthly during Phase One and weekly during both Phase Two and Phase Three.
The pre-migration cleanup was identified as the first priority task for the Team. CWUL started using Millennium ILS in the late 90s. Over time, thousands of brief bibliographic records were created through various cataloging projects. Since all libraries at OCA would move to one shared platform, data cleanup to remove those brief bib records was mandatory for all libraries. The Alliance SILS Cataloging Working Group developed a set of rules and detailed step-by-step cleanup task guidelines to help libraries in the bibliographic records cleanup. Some other Alliance working groups such as SILS Acquisitions Working Group, SILS Fulfillment Working Group, and SILS Serials/ERM Working Group also had recommended cleanup lists and guidelines for the consortial libraries. The CWUL SILS Implementation Team decided to start data cleanup in July 2013 and planned to complete the cleanup within one year. The functional leads were assigned to lead the data cleanup in their functional areas.
Collecting information and resources was the second major task for the team. This job was primarily done by the systems librarian. In addition, the CWUL SILS Implementation Team organized a one-day trip to the nearest peer institution, Eastern Washington University (EWU), for direct training. All department heads and functional leads participated. EWU was in the second migration cohort and their system was already live at that time. EWU librarians and functional leads shared their migration process, documents, experiences, and lessons. By observing a live Alma/Primo system in person, CWUL librarians and staff built more confidence in the ILS migration process.
Other tasks were considered by the CWUL SILS Implementation Among tasks and events listed above, completing Alma forms or assigned processes was a critical path to other tasks of Phase Two. Since CWUL functional leads were already familiar with the form requirements in Phase One, with the help of Ex Libris, they completed the Alma Migration Form, Alma Field Mapping Form, Alma Configuration Form, and Link Resolver Form on time and delivered them to Ex Libris. Upon received those forms, Ex Libris began to set up a test database for Cohort 4 libraries. Those tasks required project management, team work, and collaboration.
Staff training was another critical path to the rest tasks of Phase Two. In early September of 2014, Ex Libris provided the Cohort 4 libraries with an onsite Alma certification training. The systems librarian and another staff from Systems, the Cataloging Functional Lead, and the Circulation Functional Lead received the training. The Cataloging Functional Lead was primarily responsible for the administration of Resource Management (Ex Libris' terminology for cataloging). The Circulation Functional Lead was primarily responsible for the administration of Fulfillment and Resource Sharing (Ex Libris' terminology for circulation and consortial borrowing and lending). The systems librarian and staff were responsible for Acquisitions and Alma general administration. All Functional Leads also attended the OCA Alma Functional Workshop from September 9 to 12, 2014.
Another key task was the Alma integration with external systems. In order to facilitate the data exchange between Alma and external systems, a Secure File Transfer Protocol (SFTP) server was setup by the University IT. The task included setting up integration profiles in Alma and developing local applications for the following services and data exchange:
The testing officially started on September 15, 2014 after functional leads received the Alma certificate training and functional workshops. A three-month timeframe was given for the testing. Functional leads and their sub-teams were responsible for their areas. Ex Libris had testing guides. However, the CWUL SILS Implementation Team also adopted additional testing documents and forms created by other institutions. During the testing, Ex Libris' Salesforce cases were created for tracking and troubleshooting outstanding issues. Ex Libris also had a golive readiness checklist for the CWUL SILS Implementation Team to fill out after the testing.
Phase Three: Cutover, Post Cleanup, and Reporting and Fixing Outstanding Issues
The cutover, a process of switching to Alma/Primo production from Millennium, was split into ten major tasks, as shown in Table 3, and took about one month. The cutover started with sample data extraction and the delivery of files. One thing worth mentioning is that all configuration data were retained from the Alma test environment. Once Ex Libris received the final forms and full data, they required a few days to do clean up in Alma.
Then Ex Libris loaded full data into Alma. The records loaded into Alma were shown in Table 4. Almost all these records were successfully loaded into Alma. Some records were rejected but were reloaded into Alma after issues were fixed. As shown in Table 4, the Cataloging Department cleaned up 62,362 bibliographic records and 93 item records. The Circulation Department cleaned up 356 patron records. Since the Acquisitions Department did regular cleanup for orders in Millennium, the number of cleanups were not able to be reflected from Table 4. The course reserve records were manually created in Alma by the Circulation staff. Migrated from link resolver form into Alma Data source: CWUL SILS Implementation Team's shared drive CWUL went live as scheduled on Dec 18, 2014. The major tasks after go-live were post migration cleanup and reporting and fixing outstanding issues. Table 5 shows that as of May 30, 2015, a total of 185 Salesforce cases were created for post migration cleanup and reporting and fixing outstanding issues. As shown in Table 5, the systems librarian created a total of 87 cases, covering general, Alma, Primo, Electronic Resources, and Acquisitions issues, accounting for 47% of all cases. The electronic resource management librarian created about 15% issues. Interestingly, the number of cases created by functional leads from the Cataloging, Circulation and Acquisitions was very close. The results show that the systems librarian played a critical role in the migration. The results also show that functional leads who are paraprofessionals can also play a leading role in Technical Services. The results support Fu's findings (2014) that the systems librarian serves as the project manager and the main contact to the vendors and that the responsibilities and roles of the systems librarian are shifting from concentrating on systems administration when managing a traditional ILS to focusing on collaboration and project management when managing a nextgeneration ILS. The results also support Fu and Fitzgerald's findings (2013) on staffing models that paraprofessionals can play more important roles in library technical services and daily operations when libraries use the next-generation ILS, particularly in an environment of small and medium-sized libraries.
The top outstanding issue of the migration was electronic resources. These cases created by both the systems librarian and the electronic resource management librarian accounted for approximately 25% of all cases. The second outstanding issue was from the Primo account, which represented 17.30% of all cases. The third top issue concerned Acquisitions, with cases created by both the systems librarian and the acquisition functional lead, showing a 15.67% of all cases. The top forth issue was for Cataloging and Circulation, accounting for 12.45% of all cases respectively.
V. Staff Efforts
In order to determine staff time spent on migration phases, a questionnaire was designed by the authors and sent out to the functional leads after the migration. Eight functional leads responded. Table 6 shows that in the pre-migration preparation phase from July 1 2013 to June 30, 2014, the functional leads spent approximately an average of 4 hours per week on pre-migration cleanup and preparation, accounting for 10% of their work time. In the testing phase from July 1 2014 to November 30, 2014, the functional leads spent approximately an average of 4 hours per week on testing and an average of 16 hours per week on training, accounting for 10% and 40% of their work time respectively. In the cutover process in Phase Three, from December 1, 2014 to December 18, 2015, staff spent 25% of their time per week. Since CWUL started its migration preparation one year in advance, key staff spent only four hours per week on data cleanup so that pre-migration preparation tasks did not have significant impact on daily operations. In the testing phase, staff spent significant time on testing and training, approximately 50% of their time per week. This finding supports the findings of Cervone ( 2007) that ILS migration is a time-consuming process and generally involves three phases.
Finding 2. A pattern of staff time/effort for each phase was identified, analyzed, and quantitated. Phase One, with a one year timeframe, showed 10% staff time/effort was needed. Correspondingly, if the timeframe is half a year, then 20% staff time will be needed. If the timeframe is three months, 40% staff time will be needed. Phase Two, with a five-month timeframe, showed 50% staff time/effort was needed. Lastly, for cutover with a one month timeframe, 25% staff time effort was needed.
Finding 3. About 60% of the issues were created by librarians, particularly by the systems librarian and the electronic resource management librarian. About 40% of all issues were created by functional leads who were paraprofessionals.
Finding 4. The top 5 outstanding issues are identified as related to Electronic Resources, Primo, Acquisitions, Cataloging, and Circulation.
Finding 5. Most events and tasks were completed by team work across departments, including university IT and other units on campus. The success of this project shows that the combined guidance, assistance, collaboration, team work, and project management from both the Alliance and Ex Libris were essential to the completion of these tasks.
VII. Conclusion
The key events and tasks in the Central Washington University Libraries' ILS migration were reviewed and analyzed. There were three phases in the migration. A pattern of staff time spent on each phase was identified with a survey. This pattern shows that staff spent 50% of their work load in testing and training. Ex Libris Salesforce cases created during the migration and post migration were analyzed. It is found that the systems librarian and the electronic resource librarian played a leading role in reporting and fixing outstanding issues. Meanwhile, functional leads, who are paraprofessionals, also played a leading role in Technical Services. In addition, project management, communication, team work, and collaboration were top essential elements contributed to the success of the ILS migration.
Hopefully, this case study will provide some useful information and insights for the mediumsized academic libraries planning to migrate to Alma/Primo.",4304,4899
-,https://urn.kb.se/resolve?urn=urn:nbn:se:lnu:diva-76971/,Grammenis and Mourikis 2020,True,Efstratios Grammenis,,"
The present master thesis is an exploratory qualitative study in academic libraries regarding the transition from the integrated library systems to the next generation integrated library systems or library services platforms and the potential implications in their internal workflows. Nowadays, libraries all over the world are facing up with a number of challenges in terms of acquiring, describing and making available to the public all the resources, both printed and electronic, they manage. In particular, the academic libraries have more reasons to wish to fulfill their users' needs since the majority of them use the library sources more and more for scientific research and educational purposes.In this study we attempt to explore the phenomenon in the globe using the available literature and to identify the implications in libraries' workflows and the possible future developments. Moreover, through observation and semi-structured interviews we try to identify the current developments in the Greek context regarding the adoption of next ILS and possible implications in their workflows. Finally, we attempt a comparison between the Greek situation and the international one.
",2007,Library Hi Tech,,"List of Figures
. The inductive approach adopted from lecture 5 (Jokela, 2016) ............... Figure 8. Data analysis in Qualitative Research adopted from Creswell (2009) ..... Figure 9. Alma dashboard for a circulation supervisor adopted from Yang (2013) Figure 10. Sierra dashboard statistics adopted from Antonios' desktop at work ...  Table 1: Abbreviations List 
List of Tables

Abbreviations List
Abbreviations in the below table presented by the row they were presented in the text.
Introduction and Research Setting
An integrated library system (ILS), also known as a library management system (LMS), is actually an enterprise resource planning (ERP) that integrates all library modules such as acquisitions, cataloguing, circulation, serials control, information and reference services into one package for effective management of library processes (Breeding, 2013;Lantovics, 2016, Omeluzor andOyovwe-Tinuoye, 2016).
In the decades of 80's and 90's the Library Information Systems (LIS) rapidly developed and their presence in the libraries of the world was ubiquitous (Wang and Dawes, 2012). After the developments in technology and especially in Information and Communication Technology (ICT) had led the libraries in other more challenging paths, they forced them to adopt more state-of-the art technologies in order to serve their users (Madhusudhan and Singh, 2016).
Before the Web and the ATMs, public keyboards were put in libraries attached to dumb terminals. These terminals were connected to mainframes, and libraries workflows were supported, either relied on data supplied from a central hub, or created stand-alone systems for local inventory control. Those inventory systems, built upon ordering, acquisition, and circulation of physical materials evolved into the integrated library systems (ILS) with which most libraries are now familiar (Pace, 2009).
In our days libraries face pressures in terms of inadequate funding and increasing demands for their services, technology is a critical success factor for them. Libraries have to deploy the most appropriate technology platforms for resource management and discovery search. University libraries but also research and national ones, experience complexity in managing collections of large scale and diverse formats. They also need systems optimized as much for lending e-books or other digital material in addition to their longstanding print offerings (Breeding, 2016).
Apart from that there are other more practical reasons that libraries have to change their old ILS. One reason is that an old software and hardware system needs to be replaced because it comes to its end and there is no more maintenance (Kelley et.al, 2013). Another reason is the ongoing changes and mergers between the leading vendors in the library systems field that drives to the creation of new products offered by them.
Moreover, the libraries got access in several databases through subscriptions, acquiring new electronic resources paying a great deal of money (Fu and Fitzgerald, 2013;Breeding, 2015). Subsequently, libraries created their own repositories in an attempt to gather the scientific, research and educational work produced by the universities and at the same time, through collaborations and synergies they are creating consortia in order to reduce the cost of subscribing in grand publishers and vendors (Fu and Fitzgerald, 2013).
Finally, apart from these many libraries have access to other less known databases or web sites which are nevertheless necessary for them. In the first decade of the current century the traditional ILS were well established, but the libraries needed new tools for the management of those resources (Breeding, 2015).
The technology developments along with the increasing users' needs and the need of libraries to offer not only traditional services but also new and innovative ones have led the discourse of the libraries' evolution and their role to the emerge of the term library3.0 which represents the fact the libraries' role has been evolved and has become more challenging and complicated (Chan, 2015). The following graphical representation (figure 1), we believe that depicts the current situation not only for academic but also for all types of libraries: These issues have become an object of study among the professional librarians and the system analysts and there is so far a number of studies and case studies that deals with these concerns. Wang and Dawes (2012) mention these issues and the tremendous changes in both resources and services that libraries are provided with. They also mention the fact that the electronic material is surpassing the printed and is becoming a dominant library resource. Furthermore, Breeding (2009) mentions that there is dissatisfaction towards the current ILS products because they have failed to manage the electronic content and their user interfaces do not meet the contemporary expectations.
As the clients of a library are also keen on searching the web using the state of the art search engines such as Google and Yahoo, the inability of Web OPACs to fetch and deliver the required information to clients, increased their dissatisfaction towards the services provided by the libraries (Green, 2014). That made the transition from a traditional LIS to a next generation system a need that had to be implemented in the near future. This happened after ProQuest introduced Summon, a service that implemented a web scale discovery service (ibid).
Green (2014) argues that such ""discovery layers"" are making use of the Open URL standardized format, allows the clients of the library to extend their search to the library's full range of acquisitions no matter if they are printed or digitized, thus minimizing the time they spent from research to actual possession of the requested information.
It is worth mentioning that in the next-generation ILS there is no Online Public Access Catalog interface (demise of the local catalog Breeding, 2014) but the vendors offer additional discovery products as the discovery-layer interfaces for their nextgeneration ILSs (Fu and Fitzgerald, 2013;Omeluzor and Oyovwe-Tinuoye, 2016).
Therefore Green (2014) argues that a next generation ILS may adopt the Software as a Service (SaaS) model that allows a single instance of information to be provided in more than one clients following a subscription-based process. This subscription model also helps libraries to reduce their running costs, as they will be able to achieve better prices from the vendors in comparison to buying the books but also there will be a cost reduction due to the lack of hardware maintenance as this is now vendor's responsibility.
Most academic libraries still rely on integrated library systems and the transition to the next generation ILS is in a relatively early phase despite the fact that procurements of new systems result in the selection of a library services platform, with Ex Libris Alma currently seeing strong popularity (Breeding, 2016). The next generation ILS is still under development (Kelley et.al., 2013) but if the trend continues the number of traditional ILS deployments will decrease over time among large academic libraries (Breeding, 2016, p. 3).
In the beginning of a new cycle of transition, academic libraries are expected to replace their legacy systems with the new platforms during the next decade (Breeding, 2014) and this shift signifies the way that libraries will manage their resources and deliver their services.
""I interpret the progress seen in these recent years as the establishment of a new generation of technologies for libraries. But it's just a set point in an ongoing series of continuous cycles. This new generation follows several that have come before and others that will unfold in future years. It is important that these technologies continue to advance and be reinvented in ways that break away from the limitations of those previously established"" (Breeding, 2013 p. 16) Marshall Breeding (2013) also mentions that the most successful library automation products are those who manages to retain the classic ILS model and the same time managed to converge and develop fast responding search services to satisfy the arising needs of their clients.
Purpose Statement and Research Question
Although the integrated library systems offer great opportunities for the libraries worldwide in order to organize and spread their resources to the users and to automate their procedures, the current developments on this field with new systems called ""library services platform"" (Breeding, 2012, p. 24) or ""the next generation integrated library system"" (Wang and Dawes, 2012, p. 1) or ""web-scale management solution"" (Burke, 2012) and ""library management service"" (Dula et al., 2012) along with the increasing publications and the dramatic transformation in libraries' collections have created new standards that university libraries need to follow (Breeding, 2014). The aim of this research is to describe what has been done so far and to identify the implications for academic libraries establishing these new systems in its internal workflows.
As already mentioned the libraries' collections have gradually moved the recent years from printed to electronic resources and nowadays the academic libraries worldwide maintain the biggest part of their collections in e-format and annually subscriptions in e-platforms while their printed material is declined (Yang, 2013& Breeding, 2014). This shift along with the fact that institutional repositories were created in order to gather the knowledge that is produced in the universities, resulted in a situation where a university library has to manage and organize a heterogeneous material in multiple formats with a number of different communication protocols following them. Pace (2015) and Romaine & Wang (2017) mentions that the next generation library systems are actually electronic resource management systems (ERM) which were developed as separate systems in order to facilitate libraries in organizing and providing their electronic resources because the traditional ILS failed in this point as they were developed only for managing printed material. This resulted in a situation where the libraries separated their workflows and the staff from traditional workflows.
In the framework of the present thesis, we also examine in brief the current situation in Greek academic libraries comparing with the international one attempting to identify for the Greek context possible implications on their workflows as well.
As there is no study so far concerning the Greek academic libraries overall, but partial ones, we believe that our contribution to the Greek context will be useful and it may provoke further studies on this field.
The research question is:
""What are the implications for the academic libraries regarding the adoption of the next generation ILS in their internal workflows?""
The research was conducted by studying the current literature regarding the ILS and making a comparison with the past and the present situation and looking at the future developments. Also, the qualitative approach was followed along with observation and interpretation and semi -structured interviews. This approach we believe that is suitable for our study because from the one hand we explore the phenomenon in the globe and on the other hand we try to identify possible implications on the libraries' workflows both internationally and nationally. Moreover, the fact that we were able to conduct the observation and the interviews with fellow colleagues and partners that we have been collaborating with for several years was an added motive.
Topic Justification
It is widely accepted that the increasing users' needs for instant and accurate information created serious matters for librarians and the librarian community has been striving to find new roads and trustworthy solutions. Moreover, the role of libraries has changed from the traditional services to the proactive or even interactive or integrated library services offering advanced support to their patrons (Kamar & Clair, 2015).
In this framework, libraries and, speaking about research, the academic libraries espoused several procedures in order to deal with these matters. First, as it has already been mentioned, the academic libraries implemented new information systems which actually automated the procedures that were already in use, such as cataloguing, circulation, acquisitions, etc. (Breeding, 2016).
Moreover, in many cases academic libraries are facing funding issues thus, in order to overcome monetary boundaries and keep serving their patrons new consortia have been emerged, like the one of Greek Academic libraries, so to deploy most appropriate platforms and services in the most cost-effective way. Such changes are followed by alternations in libraries' everyday workflows.
Scope and Limitations
The scope of this research is twofold: from the one hand to describe and explore the previous and the future situation regarding the integrated library systems (ILS) and its off springs the library services platforms and the implications on their workflows and on the other hand to describe in brief the aforementioned issues for the university libraries in Greece.
The possible implications for academic libraries by changing their internal workflows covers broader matters such as advanced skills both from librarians and IT professionals, poor technical support from vendors' side, interoperability issues and staff training seminars. We believe that our study will contribute both the librarians and the IT specialist to identify the most important aspects of the migration from the old system to the new one by focusing on their workflows and how these are changed or amended.
There are several reasons that led us to deal with this subject in our master thesis which we believe that are quite important for academic libraries worldwide. First of all, despite the fact that a big number of university libraries have migrated from ILS to next generation ILS an important percentage of them they have not migrated yet or are planning to migrate in the near future.
So, there is an ongoing public discourse among the librarians for this subject either on public forums and conferences or through publications mainly in scientific journals. Secondly, the vendors offer new applications and search engines to the libraries providing new and advanced services but at the same time limitations are emerged. Third, new libraries synergies and consortia are being created by the academic libraries worldwide in order to cope with the decreasing budget and to negotiate from a better position with the vendors.
Last but not least personal interest for the current developments in Greek context regarding the adoption and implementation of these next generation ILS is an added factor for our decision.
As it is mentioned semi structured interviews were conducted with five academic libraries (three in the region of Attica, one in Patra and one from USA). We decided to explore the current situation in libraries recently moved to new systems adopting next Generation ILS (proprietary and open access) and to one library that has not migrated yet but is planning to do it.
Moreover, we conducted an interview with a former associate Dean in university libraries from a university in USA (St. Louis) in order to have the American perspective. Of course, the sample is small, but we believe that exactly because these libraries have recently migrated to the new systems and are the biggest in Greece and one of the biggest in USA with a big number of employees and different disciplines the findings will be useful for our study Furthermore, we conducted a semi structured interview with a vendor representative here in Greece for ILS and next generation ILS in order to have the vendor's perspective on this field and to identify future possible developments regarding the implementation of new applications and tools.
Both researchers are familiar with the information systems and particularly with the LIS as Stratos holds a degree in computer science from the department of informatics from TEI of Thessaloniki and works as a vocational high school teacher on this field and Antonis has a bachelor from TEI of Athens from the department of Librarianship and Information Systems and has been working for over twenty years as a university librarian.
Thesis Organization
The thesis consisted of six (6) chapters.
Chapter 1 is the introduction where the research setting, the research question and the topic justification are presented.
Chapter 2 provides the literature review on the selected topic along with the findings on the academic libraries workflows and how the implementation of the new systems affected the entire libraries procedures.
Chapter 3 presents the methodology and the methodological tradition. The hermeneutic approach along with the interpretive research was used in order to interpret the phenomenon and the exploratory qualitative approach was followed in order to identify the possible impacts on academic libraries' workflows and to answer to the research question. Two techniques were carried our observation and semistructured interviews.
Chapter 4 provides the findings from the empirical data along with their analysis and Chapter 5 is a discussion on the findings both from the literature and the empirical data.
Finally, chapter 6 provides the conclusion of the study, our contribution to the research and proposes future research settings.  Chapter 2 constitutes a review of the literature that is used in this exploratory qualitative study. First, a general overview is presented, where is mentioned the gradually transition of the academic libraries from the traditional LIS to the next generation ILS. After that, the implications of the implementation of the new systems on the libraries' workflows are presented and finally, a summary of the literature is provided where the findings are displayed.
General Overview
In the international literature there are several published articles in scientific journals concerning exactly what libraries and especially academics have to face up with. Wang and Dawes (2012, p.79) focus on the Service Oriented Architecture (SOA) ""for building business applications as a set of loosely coupled distributed components linked together to deliver a well-defined level of service"". These services will be able to communicate each other in terms of data exchange and service collaboration. A very good example of a service in libraries is that of check-in or check-out service.
Furthermore, Peter Green in his suggestion (2014 IATUL conference) and referring to the decision of Curtin University Library in Australia to implement a next generation library system, demonstrates some major concerns for the librarian community. In specific he: a. investigates why vendors have invested so much in the development of new systems and why libraries are taking them up, b. examines the pros and cons of moving from a locally hosted service to a cloud based one, c. considers the impact of a rapid development methodology and d. reflects on the expected outcome after the end of this long scale procedure. It is understandable that libraries' attempts to fulfil users' needs in gathering all the available sources under one umbrella should be a very well-organized procedure in order to have the intended outcomes.
In his paper George Machovec (2014), examines the aforementioned issues in the frame of libraries consortia. The author focuses on issues facing this type of libraries collaboration, including e-resource licensing, ebooks, next generation integrated library systems, shared print archiving, shared digital repositories, governance etc. In this research Machovec attempts a throwback to the LIS and mentions that after a period where libraries developed separated software environments a number of reasons (financial, technology developments, etc) urged them to proceed to synergies in order not only to share their sources but also to reduce the costs having better deals with vendors.
In Gareth Wyn Owen's paper (2016), a case study of the Wales Higher Education Libraries Forum (WHELF) project is presented. That project aimed to design and implement a library management system (LMS) that would be common and shared amongst all Universities of Wales, the National Library of Wales and the whole of National Health System Libraries in Wales. In this paper are also presented the methodology approach, the limitations and practical implications as well as the benefits that occurred and an estimation of further future benefits for the Wales library consortia (ibid).
Moreover, Fu and Fitzgerald (2013) in their paper analyze how the traditional integrated library system (ILS) and the next-generation ILS may impact system and technical services staffing models at academic libraries. Through their study they compare two traditional ILS and three next generation ones, by focusing on software architecture and functionality. The result of their analysis was that indeed the next generation library systems could have essential impacts on the existed ones and also to the staff models if the role of the librarians was going to be redesigned and meet the challenges and the opportunities of the new era.
Marshal Breeding (2016), Carla Wale (2011), Sharon Yang and Melissa Hofmann (2010) are also make references to open source ILSs that are implemented worldwide in comparison to proprietary ones. There are many reasons for a library to turn her interest in open source ILS amongst others are the reduction of running costs that comes from proprietary licensing fees and maintenance fees, the ease of customization and the innovation that follows the support that relies on large communities rather one vendor (Wale, 2011) and the centralization of technical infrastructure of multiple libraries within a campus or under one institution (Breeding, 2014).
The last matches to Sierra case in Greek university libraries where 26 academic libraries are under one umbrella and the server of the system is administered by one leading institution, known as ILSaS (Papadatou et al., 2017). The Greek academic libraries have so far developed synergies and consortia such as HEALink established in 1998 which concerns the management of their electronic resources and the negotiation with vendors and the creation of a new consortium which concerns the common cataloguing and interlibrary loan policies under the umbrella of Sierra from Innovative in 2015. The benefits of these procedures are obvious but need to be further investigated.
Implications on Academic Libraries' Workflows
These new systems, which actually are electronic resources management systems (ERM) were developed as separate systems, because the traditional ILS were not able to manage the electronic collections (Yang, 2013& Pace, 2015). This resulted in a situation where the libraries changed their workflows and a number of their personnel had to be designated for managing the new system (Pace, 2015).
The typical workflow and functionality are built on a modular structure which include Systems Administration, Cataloging, Acquisitions, Serials, Circulation, and Statistics and Reports and they are called ""client modules"" (Fu and Fitzgerald, 2013).
It is almost expected (if not for sure) that when libraries change their systems then they will restructure their internal workflows, reorganize their units and reengineer their staff because of the implementation of the new system (Kelley et.al., 2013;Fu and Fitzgerald, 2013;Breeding, 2015).
It is also expected that some workflows mainly regarding collections and technical services will considerably change before the migration from the old system as an extensive and intensive preparation need to be done in advance in terms of data cleanup and training as well as maintaining core duties (Johnson & Ireland, 2017).
Not only each migration from an old system to a new one but each phase of change brings new operational tasks that benefit from technology (Breeding, 2015). It is a common that large academic libraries have one e-resources librarian (systems librarian or electronic resource librarian), another one as a reference librarian and one or two in the cataloguing and acquisitions department (Pace, 2015& Stachokas, 2018).
This significant change happened when academic libraries started to establish the next ILS the so called ""electronic resource librarian"" begun as a public service generalist and it evolved as a technical service specialist or as an expert to the electronic resources management (Stachokas, 2018).
Large academic libraries with an important proportion of their budget spending on acquiring electronic resources not only have an ERL's position but also, they are led to hire new staff by creating two clusters: one for licensing, acquisitions and collection analysis and one for metadata, discovery, management of knowledge bases, and addressing technical problems (Stachokas, 2018). Fu and Fitzgerald (2013) in their analysis argued that the implementation of the next generation ILS would have a huge impact on libraries' staffing and organizational structures as there will be no requirement for local staff to execute traditional works such as server and storage administration, backup and recovery administration, and server-side network administration. Instead this staff could be used for other functions such as learning how to use APIs so that they will be able to support the customization of their institutions' discovery interfaces and the integration of the ILS with other local enterprise systems, such as financial management systems, learning management systems, and other local applications.
Academic libraries had accepted the fact that traditional ILS covered their basic needs and functions but new emerged e-resources such as web-based content, licensed resources, digital material and the creation of digital repositories changed dramatically their collections and the associated workflows (Pace, 2009).
Furthermore, in subsequent phases, new products and tools are offered by vendors that subsume much of the functionality of these multiple applications, resulting in more streamlined and integrated platforms (Breeding 2015).
Academic libraries and vendors agreed that new tools and systems had to be designed for better management of libraries' resources and services and for efficiently workflows (Pace, 2009). Nevertheless, it seems that finally libraries adopted its internal workflows to the limitations of the offered systems (Breeding, 2007).
While the percentage for electronic materials is crossing the 50% of the whole budget (Burke, 2012& Yang, 2013), much fewer than half the staff were devoted to these separate systems and workflows (Pace, 2015). According to Pace (2015) and Ohler (2013) the major flaw of ERMs is that they were created for the same purpose that an ILS was; just for handling exclusively physical materials, ERMs were similarly suited only to electronic materials and when the libraries had to manage both printed and electronic material they had to have multiple systems in play. Medeiros (2013, p. 4) in his viewpoint is thrilled with the implementation of the next generation ILS as he believes that these systems ""promote collaborations, leveraging records and applications built by others in order to facilitate efficiency"". He also urges that better workflows management functions are delivered. Medeiros (2013) and Wilson (2012) agree that the integration of multiple management points (ILS, ERMS, knowledge base, digital repositories, etc) will help academic libraries to handle a complex data management but at the same time are wondering ""how are these systems working in real situations?""
Andrew Pace (2015) challenges up to a point these new systems, as having worked as a librarian decided to become a systems librarian in order to build new and advanced systems for managing, discovering and providing better services for library resources.
In his paper argues that of course these new systems allow for advanced internal workflows for librarians in terms of ordering and purchasing material (both printed and electronic) but he stresses that libraries need better management workflows in order to handle complex orders, packages and title lists available on multiple platforms with multiple mechanisms. These separate systems have created obstacles for efficient workflows (Romaine & Wang, 2017) Here it seems that both differentiate from Breeding's expectations (2012) who argued that these new systems will offer to libraries flexibility of designing their workflows most suitable to their needs. Breeding argued that in libraries' modules such as acquisitions (both printed and electronic), cataloguing and serials management the new systems enable libraries to organize their work themselves rather than impose their own rigid workflows (Breeding, 2012). From the other hand Pace (2015) mentions that workflow efficiency remains elusive having multiple systems in play. Kelley et.al. (2013), in their study about 77 institutions in USA that migrated or are planning to migrate in a new system mention that their expectations apart from changing the internal workflows and structures are for better account management for both library personnel and users, including integration with institution-wide accounts. Moreover, they hope that the new system will have a built-in digital preservation code and the appropriate support.
It is quite demanding to picture the library's workflows as the current essential products are enough and one integrated system cannot include all of them (Pace, 2009). Andrews (2007) illustrates the most common:
•Open URL Link Resolver
• Federated search tool
• Digital archive, institutional repository, and portfolio products
• Electronic Resource Management (ERM)
• Compact and robotic storage systems for archived print materials
• Next-generation portal and discovery tools (for all the above)
• A management interface (for all the above) to determine usage and user satisfaction and allow for ad hoc reporting and statistical analysis According to Mackinder (2014) workflows in many cases are considered as processes, yet they are totally different as workflows are the generic set of directives that allow librarians to accomplish their work procedures such as investigating, ordering, licensing, as well as other chunks of processes that exists between them.
Workflows are depending on policy decisions and they depict the big picture of what has to be done in order a normal flux to be established for librarians, while a procedure is more detailed. Meaning, that workflows provide the general guidelines that demarcates the procedures, as procedures are the steps librarians and other library staff have to follow in order to carry out a workflow (Mackinder, 2014).
The Library Services Platforms support roles that are defined by the system administrator and in its turn the system identifies the librarian's role by his login to a dashboard where all the tasks associated with the role are displayed (Yang, 2013). By this way for instance a librarian in acquisition module who occasionally does cataloguing, or circulation is able to see list of buttons or links for acquisitions and cataloguing tasks (figures 3, 4 & 5). Ohler ( 2013) makes an interesting reference comparing the workflows that academic libraries developed with the traditional LIS to the workflows that are developed with the new systems. More specifically, in the development of the traditional ILS libraries focused on their local workflows and how they can be developed or amended without searching for commonalities with other libraries and to share experience. This resulted in a situation where in terms of acquisitions and serials modules were different from library to library.
Moreover, the proprietary nature of the traditional LIS hampered the interoperability of these systems something that libraries were needed when migrating from the old system to the new one. The same mistake repeated in the issue of the workflows with the next generations systems where either the system was too specific to match with the local practice (Collins & Grogg, 2011) or it was too general to support local workflows (Wang & Dawes, 2012).
This resulted in a situation where academic libraries had to either reinvent their workflows to match the design of the systems or to find out workarounds to facilitate the gaps in the system and in its turn resulted in duplicating the work between the traditional acquisiti",6138,6972
10.6017/ital.v42i1.15599,,Guo and Xu 2023,True,Jin Xiu Guo,"Decision-Making in the Selection, Procurement, and Implementation of Alma/Primo","
This case study examines the decision-making process of library leaders and administrators in the selection, procurement, and implementation of Ex Libris Alma/Primo as their library services platform (LSP). The authors conducted a survey of libraries and library consortia in Canada and the United States who have implemented or plan to implement Alma. The results show that most libraries use both request for information (RFI) and request for proposal (RFP) in their system selection process, but the vendor-offered training is insufficient for effective operation. One-third of the libraries surveyed are considering switching to open-source options for their next automation system. These insights can benefit libraries and library consortia in improving their technological readiness and decision-making processes. INFORMATION TECHNOLOGY AND LIBRARIES MARCH 2023 DECISION-MAKING IN THE SELECTION, PROCUREMENT, AND IMPLEMENTATION OF ALMA/PRIMO 2 GUO AND XUBudgeting for a migration project needs to be secured before the project takes place. The one-time migration cost has a huge impact on a library's decision on a new system. Lengthy procurement processes mean that it can take a year to communicate requirements, solicit bids, and make a final decision. Libraries also wonder if they should acquire such a new system through a consortial deal or on their own. A successful implementation of a new system starts with making a sound choice. The system migration project encompasses various technological and management decisions made by project managers, team leaders, and library administrators. Decisions about data cleanup, migration mapping, system configuration, communication, and training can have a tremendous impact on project outcomes, staffing, existing workflows, and job functions and responsibilities. In the meantime, the project itself also provides libraries a great opportunity to improve the existing operational and staffing model and to adjust their strategy to manage technological and organizational change.There are few studies on decision-making of the Alma/Primo selection, procurement, and migration from the user's perspective. Alma is a cloud-based library management system that helps libraries manage, deliver, and discover digital and physical resources. It offers functionalities such as resource discovery, resource management, resource sharing, and analytics. Primo VE is a next-generation library discovery platform that provides users with access to a central index of the library's collections. It offers a personalized and intuitive search experience, with features such as faceted searching, saved searches, and item recommendations. Both Alma and Primo VE are Ex Libris products. This case study fills the gap and provides a better understanding of how American and Canadian library leaders and administrators make decisions for their libraries and consortia. The pairing of Ex Libris's Alma and Primo products has become a widely accepted next-generation system due to its cloud-based model for managing both electronic and print resources. The findings of this study offer insights and lessons learned to help library leaders and administrators to make better decisions on their future technological change.
LITERATURE REVIEWThe growing user demand for electronic resources over the last decade has led libraries to make a rapid digital transformation to manage and deliver online library services. Consequently, system providers are hungry to develop the next-generation library systems. Organizations have started to adopt cloud computing as their infrastructure. A benefit of cloud computing is that local IT staff no longer need to handle hardware failures and software installation. Cloud computing streamlines processes and saves time and money. Additionally, cloud computing not only enables libraries to deliver resources and services in a network and a library community but also frees libraries from managing technology to focus on collection building, service improvement, and innovation. Therefore, libraries have started to migrate their client-based Integrated Library Systems (ILS) to cloud-based next-generation systems, often referred as LSPs. These LSPs can be connected with other web applications, increase collection visibility and accessibility, streamline workflows, reduce duplication of staffing and collections, and create a greener ecosystem for organizations. 4 Library consortia have been playing vital roles in resource sharing, cooperative purchasing, discovery, user experience, and technical support. Many libraries migrate to a shared nextgeneration ILS or LSP by joining a consortium. Besides sharing common needs, participating Respondents
",2023-03-20,Information Technology and Libraries,American Library Association,"INTRODUCTION
With the exponential growth of digital information, libraries have been seeking innovative systems to manage electronic resources and provide collection services. The next-generation integrated library system (ILS) should address both current challenges and future demands. With that in mind, new cloud-based commercial products have come into the market in recent years. Ex Libris Alma, OCLC Worldshare, and Innovative Sierra are often referred to as library service platforms (LSPs) compared to a client-based ILS. Among these new products, selecting and implementing a new system is no small task. Studies show that libraries might overlook the capacity of an ILS to accommodate many functions and make a tough choice between sticking with the current vendor or switching to another before investing time and resources to migrate to a completely new system. 1 Libraries do not make these kinds of decisions in a rational manner, which involves clearly defining the problem, identifying and evaluating potential options, weighing the pros and cons of each option, considering an organization's values, goals, and preferences, making a choice based on a systematic analysis, and continuously reassessing and adjusting the decision as new information becomes available. As a result, a selected system might not be the best fit for a library's actual needs. 2 Library consortia also face a similar challenge, but in a more complex context. For example, sharing cost, level of collaboration, and integration with other library applications can be quite different from a small library to a large research library. Additionally, the requirement for security and scalability can vary among consortial members. Ninety-four percent of academic libraries migrated their systems to Alma in 2018 by joining a consortium. 3 At a consortial level, managing a system migration project adds a significant challenge because of the competing, often conflicting desires of constituent institutions.
DECISION-MAKING IN THE SELECTION, PROCUREMENT, AND IMPLEMENTATION OF ALMA/PRIMO 3 GUO AND XU libraries are quite different with respect to their sizes, the kinds and numbers of resources they provide, services, priorities, and staffing. Although this could pose some challenges like cost sharing for participating libraries, workflow design, policy, and a collaboration model for libraries, libraries still benefit greatly from the shared catalog and enhanced metadata as well as cooperation on a global level through the product community such as ELUNA and IGeLU. 5 The selection of a new system is not a small decision. Calvert and Read pointed out that some libraries turned to ""sheep syndrome"" of selecting what other libraries have bought due to the lack of software knowledge. 6 Their study suggested that a request for proposal (RFP) could be a part of the LSP selection process by providing a consistent set of vendor responses with a narrow scope, a formal statement of requirements for benchmarking, and a mechanism for vendors to compete. Gallagher advised considering existing contracts, financial resources, and RFPs before beginning a system assessment. He indicated that the expiration date of the current ILS and opt-out clauses of the existing contract could be the indicators of a go-live date. A price quote including a one-time implementation fee and a cost-benefit analysis of the current ecosystem compared to the vendor offer could provide a helpful document that envisions future library services. 7 In addition to an RFP, Yang and Venable also considered the library automation marketplace and needs of their own library when migrating from SirsiDynix Symphony to Alma/Primo. 8 Gallaway and Hines embraced competitive usability techniques to test a set of standard tasks across multiple systems by using focus groups at Loyola University New Orleans to select a nextgeneration system. 9 They also collected anecdotal information and feedback on the system performance of the current library online catalog through a survey of library staff. This evidencebased decision-making process makes system selection in a rational manner. Manifold, on the other hand, proposed a principled approach to selecting a new LSP. He believed that system selection was a part of the continuing process of organizational change and needed to involve library staff and users throughout the process. Today's LSP systems can connect almost the entire range of library operations, from resource management and acquisitions to user request fulfillment and the integration of subject guides on research, teaching, and learning A system migration is much more than just a move to a new system; instead, it is a transfer to a new culture. He suggested the acquisitions process must start with educating participants on the features of various systems, methods of vendor assessment, the rules of contract negotiation, communication, and stress management. The success in system selection and implementation should be measured over the life span of the system to guide new decisions along the way. 10 In addition to commercial products, some libraries are acquiring open-source software (OSS) that enables them to have a greater control over customization. The potential benefits of OSS include cost effectiveness, interoperability, user friendliness, reliability, stability, auditability, and customization. Koha, Evergreen, FOLIO, ABCD, WinISIS, NewGenLib, Emilda, PMB (PhpMyBibli) and WEBLIS are examples of OSS ILS/LSP products on the market. 11 When selecting and implementing an OSS solution, small libraries such as the Paine College Colins-Callaway Library, with a limited budget and small staff, chose a hosted open-source ILS (Koha) to obtain specific expertise and services at a reasonable price. 12 Once a system is selected, the implementation process itself can be critical to the perception of overall system success. Lovins expressed concern about choosing a project management approach that is schedule-driven over results-driven. He also recommended organizing implementation activities around the incoming system functionality. For a consortium-wide system migration, a ""train-the-trainer"" strategy was adopted in the training program, which mostly offers demonstrations instead of instruction to future trainers. 13 The program hardly met libraries' expectation for training.
Active staff participation in a system migration is key to a project success. Banerjee and Middleton reported that when library staff owned the migration process, fewer mistakes and greater satisfaction with the new system, as well as quicker troubleshooting of problems that did arise as a result of the migration, were observed. 14 Avery shared that the God's Bible College Libraries did an informal pre-and post-assessment of library users and staff to gather feedbacks on both legacy and target ILS. He recommended conducting a formalized pre-and post-evaluation of user satisfaction with the ILS. 15 Stewart and Morrison observed that acquisitions workflows in a shared Alma environment must balance required consortial needs with local policies and procedures. The unmet training needs and the lack of an electronic resources management (ERM) module in Alma presented challenges for library staff to develop and manage Alma workflows. They argued that a two-year project cycle was super ambitious especially if the consortium size and variety of individual libraries involved were large and wide. 16 When migrating from Horizon to Symphony (both are SirsiDynix products), King Fahd University of Petroleum and Minerals based in Dhahran Saudi Arabia experienced a delayed implementation. Some unmet needs, such as a dramatic shift of workflows, user interface customization, and training support by a system provider or its parent company not matched by a local vendor, became hurdles for this project. 17 Although a new LSP including Alma/Primo and OSS empowers libraries to create unified workflows across functional modules, this feature requires a system user to have cross-functional roles to conduct these activities. 18 When migrating from non-Ex Libris product lines to Alma/Primo, libraries may need to make tough implementation decisions. For example, the University of South Carolina migrated library data to Alma/Primo from Innovative's Millennium and EBSCO's Full Text Finder. When the legacy and target products are from different vendors, the system migration can be more complicated in communication, data mapping, data quality, and expected results of data migration. For the USC Library, the preexisting duplicate records for electronic resources should have been cleaned up before the migration. 19 Libraries should address their concerns about key activities during the implementation to get the best possible result. The Joint Bank Fund Library had a three-day onsite training in workflows in the middle of the project. It would be much more effective if the library had communicated with the vendor to reschedule the training at a later stage of the migration because library staff were not yet familiar with the LSP by the expected time. 20 The University of North Carolina at Charlotte migrated from OCLC's Worldshare Management Services (WMS) to Alma/Primo after migrating from Millennium to WMS four and a half years previously. The Atkins Library went through the second system migration because WMS modules did not meet their library's needs. Going through two system migrations in the span of five years was particularly costly and frustrated Technical Services staff spent more than half of their work time on data cleanup. Additional time for data cleaning, workflow design, and training was also needed after the migration to Alma. 21 Fu and Fitzgerald studied the effect of LSP staffing models for library systems and technical services by analyzing the software architecture, workflows, and functionality of Voyager and DECISION-MAKING IN THE SELECTION, PROCUREMENT, AND IMPLEMENTATION OF ALMA/PRIMO 5 GUO AND XU Millennium against those realigned in Alma, WMS (Worldshare Management Systems), and Innovative Sierra. They discovered that the workload of systems staff could be reduced by around 40 percent, so library systems staff could have additional time to focus on local applications development, the discovery interface, and system integration. In the meanwhile, the functionality of the next generation ILS provides a centralized-data services platform to manage all types of library assets with unified workflows. Consequently, libraries could streamline and automate workflows for both physical and electronic resources through systems integration and enhanced functionality. This change requires libraries to reconsider their staffing models, redefine job descriptions, and even reorganize the library structure to leverage the benefits of a new LSP. 22 Western Michigan University (WMU) decided to reorganize its Technical Services department after the Alma migration was completed in 2015. After the Alma implementation, it was observed that staff spent 38 percent less time working with physical materials. The Systems Department also shifted its focus from back-end system support to front-end user and other new technologies. WMU consolidated fourteen departments into six and renamed Technical Services to Resource Management, composed of Cataloging and Metadata, Collections and Stacks, and Electronic Resources. The LSP administration was shared by four certified Alma administrators and one discovery administrator residing in the Resource Management department. 23 Although researchers and library practitioners have studied ILS selection and implementation processes and the impact of migration on library operation and staffing, only the studies on the RFP and usability testing have focused on decision-making on the ILS selection. Today, library administrators and leaders face technological change more often while making a transformation to a digital business model. They should understand how decisions are made at different organizational levels when managing change. This study is to fill this gap and help library administrators and leaders to better prepare for future change through the following research questions:
• What is the decision-making process and what do libraries consider?
• How do libraries evaluate the migration project?
• What are the impacts of the system migration on library staffing and operation?
• What lessons have libraries learned from the system migration? • What will libraries do differently for the future system migration?
METHODS
Researchers have adopted both qualitative and quantitative methods for studies about system migration. The literature indicates that both interviews and surveys have been employed to collect data for these studies. 24 A usability testing through a set of tasks across systems has also been utilized in a system selection. 25 A comparative analysis of vendor documents, RFP responses, and webinars has been applied in studying the impact of system migration on staffing models. 26 In this research, the authors used a qualitative method through a survey to understand decisionmaking on system selection, procurement, and implementation.
Data Collection
The population for this study is those libraries that implemented or are planning to implement Alma. Through the ELUNA membership management site (https://eluna40.wildapricot.org/), the The majority of the respondents in this survey were deans, directors of the library or university librarians, and system librarians (see table 1). Also, there were a wide variety of other position titles across cataloging, acquisitions, technical support, and reference, who participated in the survey (see table 2).
Participating Libraries Geographic Location
The participating libraries were located in the United States and Canada, and the majority of them were American libraries (see table 3). The American libraries were distributed in 36 states, while the Canadian libraries came from 4 provinces.   
Library Size
The libraries served a wide variety of student sizes, ranging from less than 1,000 to over 50,000 students (see table 4). The smallest library had only 199 students while the largest library system or consortium had 482,000. The number of employees in those institutions ranged from less than 1,000 employees to over 20,000 faculty and staff (see table 5). The smallest institution may only have 10 employees, while there were three larger institutions with over 50,000 faculty and staff. 
Library Type
The majority of the libraries were single campus libraries; some were part of a multicampus library system or consortium libraries (see table 6). The other types of libraries may include single campus libraries serving more than one institution or location, central offices of a consortium, part of a statewide system, or independent libraries involved in consortium purchase and implementation of Alma. The majority of previous ILSs used by the participating libraries were Voyager, Aleph, Millennium, and Sierra (see table 7), and their vendors were Ex Libris, Innovative Interfaces, Inc., and SirsiDynix (see table 8). Thirty-seven percent of libraries reported that they had used their previous ILS over 20 years before they planned to migrate or migrated to Alma (see table 9). Also, one-fifth of libraries indicated that prior to Alma, it was their first time to adopt an ILS. Therefore, this was their only experience in system migration (see table 10). All libraries used Cataloging, Circulation, and OPAC modules in their previous ILSs, and they also used other modules (see tables 11 and12).   The majority of libraries reported that they will implement or have implemented the following Alma modules: Fulfillment, Primo/Primo VE, Resource Management, and Acquisitions (see table 13). Some libraries mentioned that they also used Summon to replace Primo/Primo VE as they had used it before the system migration. When asked if an RFI (request for information) was involved, more than half of the libraries responded with a confirmative answer (see fig. 1). About half of the libraries reported that they did not conduct a system functionality survey to collect information from library users and colleagues (see fig. 2). More than half of the libraries indicated that the RFP (request for proposal) process is required for the system migration (see fig. 3). There were a variety of reasons why for those libraries who did not conduct the RFP process (see fig. 4), such as an RFP may not be necessary when migrating systems to the same vendor, there was no increase in expenditure, or the expenditure did not reach a budget threshold (e.g., less than $100,000), or the previous contract stipulated it if upgrading to a new product with the same vendor. Another reason was that libraries might have an existing relationship with vendors and would like to continue using DECISION-MAKING IN THE SELECTION, PROCUREMENT, AND IMPLEMENTATION OF ALMA/PRIMO 12 GUO AND XU their products. Some libraries were given authority by the university administration and library directors to handle the negotiation, or they thought an RFI offered sufficient information to make this decision. Other libraries had no choice in conducting an RFI or RFP process for reasons such as their system was outdated and they had to migrate, the decision was made by consortium, or Alma was their sole source procurement.     
Data Migrated
The most common types of data migrated to Alma were bibliographic records, holdings and items, patrons, and circulation data (see fig. 6). Some libraries reported that they also migrated other types of data including vendor lists, e-resource data, all available data types, etc.
Discovery Service
The survey asked if there were any libraries that migrated to Alma and did not choose Primo/Primo VE for their discovery service. Nine libraries reported they were in this case. Four of them used Summon, four chose EBSCO Discovery Service, and one adopted their locally developed product. When asking the reason for their choices, the nine libraries indicated that they would like to stay with the existing discovery service. Additionally, two of the libraries stated that a budget limitation was a part of their reasons, and one library thought the better discovery service for users was the rationale.
Part III: Feedback on Alma Migration System Migration Evaluation
The majority of libraries reported that they did not conduct a formal post-migration evaluation. Half of the libraries thought the migration achieved their project goals, or met the needs of library operations (acquisitions, cataloging, fulfilment, discovery, etc.) (see fig. 7).
Figure 7. Whether a formal post migration evaluation was conducted.
Some libraries also provided their own migration evaluation, including RFP mandatory requirements signoff, availability study, focus groups with library staff, usability testing with students and faculty, feedback and cross-checking with consortium, debrief of library staff, etc. Some only did an informal evaluation, which turned out to be not handled well or not very satisfactory. For example, one consortium did a survey on the migration and provided the feedback to Ex Libris for improvement. Other libraries reported that they had not done the evaluation as they did not start the migration process, were still in the migration stage, that an evaluation was not a part of the decision-making process, or that Alma was offered as a free product because of their consortial partnerships.
Valuable Lessons Learned
The authors asked what were the most valuable lessons the libraries had learned from the migration project, and how they would implement the migration differently if they had a chance to do it again. The most valuable lessons concentrated on training, communication, engagement, implementation process, and data cleanup/preparation (see fig. 8). These lessons are shared in greater detail in the discussion section. 
Prospective Migration
When asking if libraries would consider working with Ex Libris again if they migrated to a new system in the future, 70 percent of libraries gave an affirmative answer, but some libraries indicated that they would seek other alternatives (see fig. 9). When asked how likely libraries would be to consider implementing an open-source ILS, the majority of libraries conveyed that they would not consider open source; only 7 percent of libraries would consider it (see fig. 10).  
DISCUSSION
The authors examine the above findings further through the lens of the research questions raised in the literature review section.
The Decision-Making Process and Factors Considered
The survey indicates that both RFI and RFP are important for a selection process. Fifty-two percent of the libraries conducted an RFI and 57 percent required the RFP process for the system migration. Interestingly, even with a variety of sound reasons such as no increase in expenditure, within the budget threshold, existing relationships with vendors, sole source procurement, consortium decision, riders, etc., some libraries still did not roll out the RFP process. Besides RFI and RFP, 43 percent of libraries went through a system functionality survey to collect information from library users and colleagues.
For most libraries, the library dean or director, Alma local implementation team, or Alma project working group of a consortium were involved in the decision-making process. In some cases, university executives such as provost, VP finance, CIO, CFO, campus IT, and associate dean or associate university librarian for library technology made a collective decision. In a rare case, the dean of Arts, Languages & Learning Services made the call for the system selection.
When considering system migration, many factors can be important. This survey shows that libraries mainly consider budget reality; ERM, bibliographic, and authority control; discovery layers; and cloud-hosted systems. It is interesting that most libraries would like to move to a cloud-based system that has better functionality for discovery and electronic resources management. The survey also reveals that library administration needs to find a way to offset the cost increase of the system migration. The lack of comparable system or service offerings in the market also contributes to the decision on system selection.
Project Evaluation
Project evaluation provides important feedback from both system users and system providers and a great opportunity for libraries to learn. The findings indicate that many libraries do not have a formal assessment process. Some consortia have conducted surveys and provided feedback to Ex Libris, but no response reported to the feedback from Ex Libris. Both libraries and system vendors have lost the opportunity to learn and improve project management. For example, welldocumented complaints on dissatisfaction with Ex Libris training have not been effectively addressed. Some libraries believe a demonstration-focused training model does not provide the same experience as onsite training offers. Many libraries have had trouble with acquisitions workflows. The EOCR (electronic order confirmation record) and EDI (electronic data interchange) processes are standard practices in libraries today to generate order records and create invoices automatically and should be a part of implementation contract to ensure that libraries can operate appropriately after a new system goes live.
It is time for both libraries and system providers to consider a formal project assessment as a part of system migration down the road. Libraries will not do better if they do not improve today. Libraries cannot improve if they do not know where previous projects have gone wrong. A better way to learn from mistakes is project assessment. 
Impacts on Library Staffing and Library Operation
Some libraries reported that insufficient staffing over the system migration has created additional problems and hardships. Some library departments have been stretched very thin in order to work on the migration project in addition to their regular operational duties. However, about onethird of survey-participating libraries have reported that meeting the needs of library operation including acquisitions, cataloging, fulfilment, and discovery is a criterion of project evaluation. The lack of dedicated LSP project migration staff creates a challenge for system migration. Most importantly, additional staffing time and technical capacity are important factors that decide if libraries could fully take advantage of the functionalities of a new system. Libraries might manage the system migration better by hiring additional technical staff on a project basis to handle technical aspects if staff cannot be released from library operation to focus on the migration project.
The system integration and unified automated workflows of a modern LSP can enable libraries to run their operations more efficiently. Particularly in a shared environment or network, libraries could share bibliographic records for general collections wider and deeper, which could dramatically reduce the need for both original and copy cataloging. System staff no longer need to install or upgrade proprietary software and maintain servers in house. These changes might cause job insecurity for some library staff. It is critical for library leaders to make adjustments to some job responsibilities or develop new skills to meet new demands. This requires library administration to create a culture of embracing change, learning, and collaboration. Staff can take the advantage of a new system by being curious and reassessing previous workflows. Library administration could create a flexible structure to encourage learning and collaboration across departments.
Lessons Learned
Many libraries shared valuable lessons they learned from the migration projects. Those lessons concentrate on training, communication and engagement, implementation process, and data cleanup and preparation.
Training
Many libraries expressed dissatisfaction with the training provided by their vendor. For example, libraries moving to Alma reported that Ex Libris could have focused more on in-person, postmigration training. As it was, staff felt undertrained because they had access only to online training before the libraries had access to their own data in Alma/Primo. Additionally, Ex Libris did not have regular trainers for a particular library, so there was less continuity across training sessions than there could have been.
Some suggest that Ex Libris do a concentrated several-day initial training for migration so that libraries have a solid overview of the entire system before data exports for testing loads, and then delve into a detailed weekly training that includes more library staff. It seems a good idea to schedule more training sessions after implementation because libraries may not know how the system functions during the implementation period.
In an ideal world, libraries would put more contractual obligations on Ex Libris to train staff more thoroughly. After all, libraries need to hold Ex Libris more accountable for project outcome. trainers does not work well in large migration projects. Ex Libris needs to train the library staff rather than focusing on training the consortium support staff and expecting them to do most of the staff training. Ex Libris indeed carries a variety of training webinars that are free; however, for bespoke training or intimate training sessions, they charge their customers. A barrier for many libraries is that they just cannot afford to pay more on these bespoke training sessions so they depend on other in-house training and best practices (e.g., work groups, training committees, inhouse power users, etc.) to train/manage the training needs of their library personnel.
Communication and Engagement
Many libraries express that communication is extremely important and buy-in from stakeholders at all levels is critical to the migration project's success. Investing the initial time to have all stakeholders onboard will pay off. Blocking off time for weekly meetings with involved staff and Ex Libris is key. Some suggested asking more questions and seeking to understand the functionality of the new system more deeply. For consortial libraries, librarians can become much closer to each other and learn to seek out and receive help from one another in the ways that they might never do before. The networking can be an invaluable source for mutual support going forward.
Some libraries reported that due to the lack of communication, an overly sudden decision for the implementation timeline was made at the legislative level. Information regarding requirements and expenses was not fully clarified before the process began and came as a surprise during the migration. The whole process felt very rushed by the vendor with insufficient trainings, which turned out to be very dissatisfying.
Implementation Process
A system migration is complex and requires a great deal of time, institutional resources, and staff. Some key processes needed to be better prepared in advance, such as staff trainings, project plans and major milestones, system analysis, customer inputs for implementation and configuration, data cleanup, physical to electronic processing (P2E), source data extraction, validation and delivery, workflow analysis, fulfillment network, authentication, third-party integrations, data review and testing, go-live readiness checklist, etc. In practice, the migration was often more timeand resource-intensive than expected, meaning that libraries found it difficult to complete their part of the process in the contractually-specified time. Libraries should clear the decks of core staff to focus on migration, and make sure there are no other major projects occurring at the same time. If staff have insufficient time during the migration window, libraries need to hire temporary experienced staff for the project. This investment will benefit library operation in the long run. The implementation team members should have more dedicated time to be trained so that the library staff are well prepared and knowledgeable in the areas in which they work. It is wise to clean up data as much as possible prior to migration. It would be ideal if the existing workflows were fully documented with diagrams so that it would be easier to determine what parts of the workflows need change. Some libraries reported their migration happened during the pandemic with state-issued stay-athome orders in force. It was extremely stressful juggling all of the changes for the library while keeping up with system migration. Ideally, it would be better to avoid doing the migration during a pandemic and postpone the migration. But if libraries have no other choices, one benefit is to take advantage of closures for cutover days. The stress of the implementation and trying to get things done may cause frustrations to boil over. It is advised to manage these situations by adding additional support where needed and by always ensuring that communication is a top priority so that any confusion is kept to a minimum.
For consortial libraries, it is important for individual institution members to have their own project managers. The consortial libraries would have tried to standardize more configurations across the consortia, like user groups, circulation settings, item types, etc. Some libraries felt the whole migration process was rushed by the vendor, which turned out to be not very successful. Libraries should not let the vendor talk them into a compressed, severalmonth migration timeline; instead, they should spend more time in the preparation and implementation process.
Data Cleanup and Preparation
Although it is tedious and time consuming, many libraries suggested cleaning up data as much as possible prior to migration. More pre-migration data cleanup would avoid the post-migration mess. Some libraries recommended more stringent cleanup of catalog records, acquisitions data, circulation data, patron records, weeding, etc. It is important to make sure the cataloging structure matches the structure of the new system. Had they taken the data review stage more seriously and fully modeled the processes and workflows that would be needed, they would have had fewer data cleanup problems to address after the migration was complete. Some libraries cautioned that Alma's P2E (physical to electronic) migration process was more complex than anticipated. They stated that the P2E conversion did not work as it should have, and Ex Libris should do a better job in the",6033,6719
10.1016/j.lisr.2022.101191,,,False,Shahnaz Khademizadeh,Analysis of book circulation data and a book recommendation system in academic libraries using data mining techniques,"
The use of data mining modern technology in library management systems and information centers is of great importance. With the increasing availability of a large quantity of information, traditional tools and practices without wasting time and cost cannot respond to users accurately and quickly. The present study aims to analyze book circulation transactions and discover the user's book loan patterns to develop a recommender system. The data included 109,639 transactions and information from 8636 user records. Microsoft SQL Server and Matlab software were applied to analyze the data. Item-based collaborative filtering algorithms and decision tree methods were also applied. The results led to the extraction of rules for suggesting books to users. Analysis of the circulation data could be applied to address many issues like evaluation, collection acquisition policies, allocating funding for materials, and suggesting approaches to deselecting and allocating physical space for materials.
",2022-10-07,Library & Information Science Research,Elsevier BV,"Introduction
Data mining has become the most active research database technique, and many scientists have studied its applications and theories, and the results of their investigations have provided a proper theoretical framework for applying data mining in academic libraries (Huang, Li, & Xiao, 2018;Liu, 2018;Ochilbek, 2019;Philip, Haugen, Lener, Pannabecker, & Brittle, 2017;Rattan, 2019;). Applying data mining techniques in academic libraries leads to discovering hidden patterns of users' information behaviors and ultimately leads to a collection development based on users' real needs for information (Krishnamurthy & Balasubramani, 2014;Siguenza-Guzman, Saquicela, Avila-Ordóñez, Vandewalle, & Cattrysse, 2015). In order to analyze users' information searching behaviors and be aware of their information needs, academic libraries must use new methods and technologies; data mining techniques are among the most important of these approaches. Therefore, the application and development of data mining techniques are essential for the databases of academic libraries.
Academic libraries use various information technologies to manage big data, such as loan data, and can only succeed in satisfying users' needs if they can anticipate the potential information needs of users and provide suitable solutions for meeting them. For this purpose, data related to users' previous activities should be investigated for necessary analyses and predictions. Therefore, it seems essential to predict the usage rate of users and the amount of their need for available resources (number of loans). In this regard, the loan circulation of information resources provides researchers with valuable information for data mining and improvement of the circulation system of academic libraries (Yi, Chen, & Cong, 2018). Resource circulation analysis is one of the conventional approaches for evaluating the academic library collection (Philip et al., 2017). The results of this analysis can improve the quality of services, provide latent knowledge of data, discover meaningful relationships and patterns between book-loan data, users' data, and library resources, and identify and analyze frequently used resources (Yu, 2011). The results can also manage financial resources, budgeting, and strategic decisions (Rattan, 2019). To make better decisions, interact effectively with users, and suggest related information resources, the circulation department needs to know users' usage rates, search patterns, library resources, and academic fields. (Jomsri, 2014;Liu, 2018;Silwattananusarn & Kulkanjanapiban, 2020;Uppal & Chindwani, 2013).
Problem statement
Academic libraries encounter a large and growing volume of data related to resource circulation data. Circulation data are often used to analyze the characteristics of a library's collection or to identify the current status of books' use. A library has a vast collection of materials, and at the same time, users produce vast amounts of circulation data daily using library materials. Library circulation data are big and structured data that can be analyzed immediately. Because of these points, it has become increasingly important to identify the book circulation data and utilize them efficiently for library collection development and management. Collection evaluation through the analysis of circulation statistics is also essential for accurately grasping the current status of collections in the library and predicting users' collection usage patterns to keep the collections up to date and improve the quality. It is the basis for solving space problems and increasing the usability of collections by weeding out low-use collections. Similarly, circulation statistics are an invaluable source of information for figuring out users' preferences for a specific subject in a collection in that books are still an important communication channel between library users in all subject areas (Rose-Wiles, 2013;White, 2017).
Users spend much time searching for and retrieving their needed resources; therefore, selecting them is significantly challenging. Users search for necessary information resources through Online Public Access Catalogs (OPACs). In many cases, users' searches are not followed by a successful retrieval, and they cannot fully meet their information needs. The diversity and a significant number of information resources, and the increase in the number of users, have made reference librarians unable to make suitable suggestions to all users for selecting appropriate resources. Analyzing circulation history, discovering loan patterns, and suggesting resources to users are practical and effective solutions to the raised challenge. In this way, an OPAC can suggest similar resources based on the discovered patterns employed by other similar users.
Presently, recommender systems are extensively used to suggest users' most appropriate information resources. In addition, academic libraries have an essential role in educating the students; hence, the effectiveness of recommender systems is an important issue that can improve students' performance. The effectiveness and efficiency of recommender systems have been discussed in theory, but no research has been conducted on the discovery of the rules in practice.
The main issue of the present study is to analyze book circulation data and discover the usage patterns of the books in the circulation section of the central library of the Shahid Chamran University (SCU) of Ahvaz to manage resources, discover the users' interests and develop a recommender system in this library. In order to study and ultimately solve the various dimensions of the research problem, the primary purpose of the present study is to mine and analyze the information resource circulation and discover the user's loan data pattern to develop a recommender system. The research questions are:
RQ1. What is the frequency distribution of student loan periods regarding time, faculty, and field of study?
RQ2. What is the effect of academic degree and field of study on overdue items?
RQ3. Which of the field of study, academic degree, and faculty is the most influential factor on the loan period and frequency of renewal?
RQ4. What are the most frequently used books suggested for ordering in the collection development department?
RQ5. What are the circulation rules and patterns of user's book loans?
Literature review
In a report presented by Philip et al. (2017) from the academic libraries at Virginia Tech, it was claimed that academic libraries need to use data mining and text mining techniques to identify and investigate students' research and study tendencies. In this regard, Rattan (2019) believed that acquisition, borrowing, and users' research interests, budget predictions and human resource management, service and resource marketing, strategic planning, and discovering user information searching behavior are the sectors in which it was possible to perform a data mining process to create stronger and more useful library services for users.
Most of the data mining studies like (Liu, 2018) conducted in LIS discovered the access patterns and library readers' loan data by applying the association rules. Association rules seek to find the relationship between the characteristics of two components or data sets, and is a method to discover relationships between items in large databases, and is one of the most important forms of discovering and extracting patterns in learning systems. This data mining seeks to extract rules from the characteristics of the data and is used in a variety of large databases to analyze and predict user behavior. Association rules in library loaning transactions seek to discover the potential rules of book loaning (De, Dey, Bhatia, & Bhattacharyya, 2022).
Other studies such as (Bussaban & Kularbphettong, 2014;Krishnamurthy & Balasubramani, 2014;Li & Chen, 2008;Xia & Liu, 2018) who applied the collaborative filtering algorithm. Collaborative filtering is an algorithm for use in recommender systems. Collaborative filtering is a combination of similarity calculation, prediction, and recommendation. Recommendation systems are divided into three categories; Collaborative Filtering is one of the three categories. Content-based recommending and hybrid methods are two other categories of recommendation system (Gemmis, Lops, Musto, Narducci, & Semeraro, 2015;Jain, Kumar, & Sharma, 2020;Najafabadi, Mahrin, Chuprat, & Sarkan, 2017).
The researchers conclude that the association rules are the most appropriate method to suggest resources based on the circulation history of resource borrowing in libraries. According to the results, data mining can significantly affect the book selection policies, book circulation in the loan section, and analysis of users' information needs.
In another category of studies on data mining in libraries, researchers have uncovered some facts regarding the user's book loan pattern by analyzing libraries' loan data to understand the trend of resource circulation and usage rate of books. Appendix A reviews related literature thematically and methodologically.
The results of most studies indicate the usefulness of utilization of data mining techniques in analyzing circulation data, user information behavior, library use patterns, usage rate, improvement of providing services and the performance of libraries, creating recommender systems in the libraries and information centers, discovering rules and users' interests, as well as mining readers circulation rules and management affairs.
Most studies on the discovery of loan data rules have applied the association rules method and data mining techniques such as MFP-Miner, Apriori, ABC, FP-Growth, and K-means clustering. The difference between the present study and the reviewed articles is that this study uses a different data mining technique, such as a collaborative filtering algorithm. In most studies, only the extracted rules are discussed; in others, the analysis of loan transactions without considering the rules of dependency and recommender systems are reported. Therefore, no research has independently studied an academic library in Iran using the present study's method. However, in addition to the development of recommender systems in libraries, the analysis of library's circulation data to improve acquisition performance, purchasing the most used copies, identification of frequently used subject areas, and influential factors on the overdue, loan, and renewal are also discussed in the present study.
Climate related conditions are other factors that affect information behavior. The SCU library is located in an entirely tropical region, which has given this university a unique feature. For instance, the early closure of the university, the holding of final exams earlier than other universities, and the low presence of faculty members and students in the summer have a direct impact on the users' information searching behaviors and the operation of the academic library.
On the one hand, the way of people's lives, jobs, and industrial position of Khuzestan province, Iran, including the existence of organizations such as Khouzestan Steel Company (KSC), National Iranian Drilling Company (NIDC), Khuzestan OIL and Gas Refinery, National Iranian South Oilfields Company (NISOC), and the agricultural hubs have made the relationship between industry and universities more prosperous in this region. On the other hand, academics are considered advisors alongside the industry and agriculture. Naturally, all these cases have made the professors' and students' information searching behavior different from other universities in the country. Therefore, the selected target population and its specific conditions are other differences observed compared to other studies conducted in this topic.
Methodology
The present study is applied research conducted using the descriptive survey method and data mining techniques.
Data collection
Data was collected over three years (2017)(2018)(2019) at the library of Shahid Chamran University of Ahvaz (SCU), Iran. The library was established in 1955 under the scientific support of the Library and Information Science Department and it has provided services to academic users for more than 65 years. According to the data extracted on 2021-08-22 from the reference site of Iranian universities (https://www. uniref.ir/), the SCU library is the most extensive academic library in southern Iran and is an excellent sample of Iranian academic libraries for conducting the present study.
The steps of the data collection phase include data collection, data cleaning, data extraction, and data integration. Cleaning, extraction and integration of data eliminate irrelevant attributes whose existence will reduce the accuracy of data mining. The data were collected from the Azarsa OPAC. Information about resources, book circulation, and users' appropriate queries were extracted from Azarsa OPAC and stored in the profiles. After this step, it was necessary to perform the data cleaning step. Among the reasons for data cleaning were the non-uniformity of data, including fields of study, book titles, faculty titles, and dates. For example, the study field of ""history and philosophy of education with the orientation of Islamic education"" existed in four forms in the output file, all of which were made more uniform.
Before normalizing the data, there were 1361 academic fields in the output file, and after making the data more uniform, this number was reduced to 156 academic fields. Also, irrelevant data and data that had miss-values were identified and removed from the output file. Next, Hijri dates were converted to Gregorian dates, and data coding was done. After data refining, eliminating inefficient and distorted data, adding computational and information fields, and encoding the data, the Excel 2016 files were imported to the Microsoft SQLserver 2012.
Seventy-four thousand two hundred twenty-six records were analyzed in the loan file, including bibliographic information (title, author, accession number, call number), user information (student number, academic degree, field of study, faculty), dates of loan and return, and allowable loan period. Moreover, 8636 records were also investigated in the file related to users' profiles, which included all students who were members of the SCU library from 2017 to 2019. Finally, 35,413 records were analyzed in the file related to renewals, including the users' information, frequency of renewals, and dates.
Analysis methods
Microsoft SQL Server and MATLAB were applied to model and conduct data mining steps. Different methods can generate a recommendation system for books. Many of these methods rely on association rule algorithms, meaning that the relationship between different books is calculated based on historical data (Wang, Xu, Feng, Peng, & Ma, 2021). To analyze the data, the collaborative filtering algorithm based on items was implemented to discover the pattern of users' book loans and develop a recommender system in the library OPAC. Liu (2018) indicated that the success rate of book prediction and suggestion based on collaborative filtering was higher than other traditional algorithms. The item-based algorithm provides results better than those provided by the user-based algorithm for all similarity measures (Fkih, 2021), and the similarity between two items provides better accuracy than that provided by the similarity between two users (Ning, Desrosiers, & Karypis, 2015).
In the next step, the collaborative filtering algorithm was implemented in MATLAB (version 2015a). The Excel 2016 file containing the preprocessed data was imported as an input into the implemented code. In book-based collaborative filtering, the similarity between books a and b is defined as m, which is the frequency that two books are borrowed together (Liu, 2018). With increasing the number of m, the two items are more similar. The books with the highest scores are considered to be suggested to others.
In order to create a book recommendation list, first, a book loan model would be developed. The collaborative filtering input data is expressed as an R matrix, which is drawn in such a way that for each book i and j that one user has simultaneously borrowed, a number is added to the value of R (I, j); therefore, the book scores are calculated by considering the frequency of different users who have simultaneously borrowed them. The higher the R matrix score of the two books, the more similar they are. The book recommendation system presented in this study protected users' identities and records of past book borrowing.
The steps to make a book recommendation list are provided in the next step. According to the high volume of data when implementing the collaborative filtering algorithm, the transactions related to one and two frequencies of simultaneous book borrowing were eliminated. The transactions of three or more frequencies were entered into MATLAB 2015a, and 7416 books were entered and analyzed through this procedure. At this step, a book recommendation list is created based on the book's scores. The following formula was designed and used to predict the probability of book recommendations.
The authors assume that i, j are the two books' titles in the library. The matrix R is obtained in such a way that every time a user borrows a book (i, j), a number is added to the value of R (i, j), and x (i, j) is the number of times that two books are borrowed together. According to the above formula, the probability of recommending a book was calculated to be equal to the score of each book based on the R matrix divided by the frequency that two books were borrowed simultaneously. The results of implementing the item-based collaborative filtering algorithm for each book are presented as a recommended list. The decision trees (DTs) were also applied to classify users based on factors affecting book borrowing and renewals. Among all the methods that have been introduced for extracting and deducing knowledge from large volumes of data, the DT approach, with its high accuracy and interpretability capabilities, is significantly prevalent and practical in the process of decision and classification (Weinberg & Last, 2019).
A DT is a tree data structure that consists of an arbitrary number of nodes and branches at each node. A node without any edges is called an internal node, and other nodes are called leaves. The instance used for regression or classification is split into two or more groups by an internal node concerning a specific function. The values of the input variable(s) consider a particular function in the training stage (Rondović, Kašćelan, Lazović, & Đuričković, 2019). The association rules and DT methods are also among the most popular data mining techniques. Some data mining research conducted in library and information science, using association rules, discovered access patterns and loan transactions of library users (Bussaban & Kularbphettong, 2014; Krishnamurthy & Balasubramani, 2014;Li & Chen, 2008;Liu, 2018;Ochilbek, 2019;Xia & Liu, 2018;). To draw the decision tree, after clearing the data, including eliminating inefficient and distorted data and making fields of study and academic degrees more uniform, titles of fields of study were matched using the text mining algorithm so that the words with a similarity higher than 90% were converted.
An operation was performed on the data to reduce the volume of data and draw DT. Jaccard similarity coefficient, calculated by dividing the number of observations in both sets by the number of observations in either set, was used to compare the degree of similarity between academic fields. Finally, the fields of study were clustered with the K-Nearest Neighbors algorithm (K-NN) after calculating the evaluation error of the optimal value of k, considering the number K = 3, which led to the development of nine clusters in this step. In addition, the decision tree was drawn using the Python tool of the scikit-learn class, and its training-set accuracy score was calculated for the loan duration of 82.5 and the frequency of renewals of 92%, which shows the validity of the drawn diagram.
The accuracy, effectiveness, and predictability of books based on a book-based collaborative filtering algorithm are much higher than that of the user-based algorithm. Therefore, the book recommendation system in academic libraries can be set up based on the same algorithm (Liu, 2018). Sirikayon, Thusaranon, and Pongtawevirat (2018) proposed the resource recommendation system using the collaborative filtering algorithm at the Dhurakij Pundit Academic library; they confirmed this method's accuracy by analyzing the recommended books. Another reason for selecting this method in the present study is that the collaborative filtering algorithm is appropriate for developing a model of a book recommendation system for the digital library and will lead to the development of users' facilities in search of the required book (Jomsri, 2014). Association-rule technologies have been widely applied in libraries. The use of association-rule technology can effectively capture the readers' book rules and predict the readers' reading needs (Tsuji et al., 2012;Wang et al., 2021).
Results

Frequency distribution of student's loan period in terms of time, faculty, and field of study.
The clustering algorithm classified the library members into three levels based on the average loan period. This algorithm, in the form of an unguided method, classified the members based on features and similar properties into level 1 with less than 36 days interval between borrowing each book, level 2 with the interval between 36 and 68 days, and level 3 with more than 68 days interval between borrowing each book. Table 1 presents the frequency of library members at the three levels separately.
The members were categorized into three levels based on the loan period of each book; 33.8%, 33.03%, and 33.13% of the members returned the borrowed books in less than 36 days, between 36 and 68 days, and more than 68 days, respectively (Table 1).
The faculty of Engineering of SCU is first among all faculties, with the distribution of book borrowing equal to 29,111 books. The faculty of Economics and Social Sciences, the faculty of Education and Psychology, and the faculty of Literature and Humanities have respectively obtained the second to fourth ranks with the frequency distribution of 13,392, 5977, and 5820 books. The lowest book borrowing rates are related to the faculty of Arts and faculty of Veterinary, with frequency distributions of 32 and 22, respectively (Fig. 1).
The results are indicated in Table 2 regarding the maximum and minimum subject areas borrowed in the SCU library. This table presents the data related to five topics with the highest and lowest frequencies.
The highest book borrowings are related to mechanical, electrical, and civil engineering, with frequencies of 9678, 8367, and 4526. The lowest frequency of loan period in the SCU library was related to the painting and motor behavior field of study. See Table 2.
Effect of academic degree on the overdue period
The average number of loan days by bachelor's, master's, and Ph.D. students can be observed in Fig. 2. The bachelor's, master's, and Ph.D. students return books to the library after 70 days, 80 days, and 99 days, respectively (Fig. 2). It is noteworthy that academic libraries have different rules for the loan period, and the final analyses are performed based on these rules.
In Appendix 2, the usage rate and return date were considered in the same criterion of loan duration. The investigation of the DT indicates that factors affecting the book's loan period are the academic degree and field of study in respective order.
Appendix 3 presents the DT of the effect of academic degree and field of study on the frequency of book renewals. As indicated in Appendix 3, the academic degree and the field of study affect the number of renewals in the SCU library
Recommended books to the acquisition section for ordering and purchasing
Among the borrowed books from 2017 to 2019, five books with the highest loan period in the SCU library are demonstrated in Table 3. ""Differential and Integral Calculus and Analytical Geometry,"" with 548 loan period frequency, was the most used book compared to other books. ""Fluid Mechanics,"" ""Strength of Materials,"" ""Rites of Life (Applied Ethics),"" and ""Basic Circuit Theory and Networks"" had the highest loan frequency distribution in the SCU library from 2017 to 2019.
Circulation rules and user's book loans pattern in SCU library
As explained in the methodology section, in the present study, the collaborative filtering algorithm has been applied to analyze the SCU library transactions and present the book recommendation model in the Azarsa OPAC. First, an example of an R matrix for six books is presented in Table 4. As indicated in Table 4, for example, the ""Information Age: Economy, Society, and Culture"" and the ""Discourse on Colonialism"" have been borrowed eight times simultaneously. Moreover, a sample recommendation list is presented in Table 5. The higher the R matrix score of the two books, the more similar they are.
It is noteworthy that due to the high volume of findings obtained from data analysis in the present study, only a few examples are presented in this article, and a complete list of book recommendations with R matrix scores and the probability percentage of relevancy of the recommended book were presented to SCU library as the result of the present research.
According to the implementation of the collaborative filtering algorithm on the library database, if the rules extracted from this research are implemented in the Azarsa OPAC when the user searches for the book ""dynamics,"" the OPAC can recommend the mentioned books in Table 5 to the user according to the recommendation list. Based on the R matrix score, these books are the most similar to the requested book. The loan percentage and the R matrix score of the best relationships extracted from association rules of books in the SCU library are presented in Table 6.
The data in named ""Kinematics and Dynamics of Machines"" in the SCU library OPAC, it is 95% probable that they would also select the book ""Fluid Mechanics"" for reading. It should also be noted that with a probability of 98%, the books ""Public International Law"" and ""Textbook of Rules of Civil Jurisprudence"" would be borrowed together. The OPAC can recommend the book ""Principles of Accounting, Volume 2"" to a user who has searched the book ""Theory of Microeconomics.""
Discussion
Book circulation is an essential aspect of library value and positively correlates with academic success (Soria, Fransen, & Nackerud, 2017). In the present study, by applying data mining methods, book borrowing and circulation data were analyzed to design the recommender system and discover patterns and hidden data relationships in the SCU library.
The evaluation of the influential factors on loan and overdue periods, identification of the subject areas and frequently used copies in the library, and provision of a suitable model of users' access to books are other issues investigated in the present study. In this regard, to prevent the increase of information resources that are overdue, it seems necessary to consider the effective and relevant factors and manage them   properly. Moreover, to determine the influential factors in loan and overdue periods, after classifying users based on the loan period, the diagram of the average loan period based on the academic degree and field of the study was drawn, and the loan period, as well as the overdue period, were calculated.
To meet users' needs reasonably, it is necessary to impose restrictions on borrowing materials, which requires the development of special regulations. The circulation rules regarding the loan duration and the number of borrowed materials usually depend on the user's situation and may vary for different libraries. For instance, according to the present regulations in the SCU library, the allowed loan duration for each book is 30 days for master's and Ph.D. students and 15 days for bachelor's students. Since it is not possible to consider a precise formula for this issue and each academic library, depending on its needs and facilities, can make a proper decision regarding the loan duration and the number of materials to be loaned to its users, data mining of loan data of this library was performed to obtain some information about the circulation of the books and also make management decisions.
According to the analysis of loan data, most students, who are members of the SCU library, return their books to the library late; this result could lead to changes in the policies and procedures of libraries to change the loan period to prevent books from returning late. As mentioned in the results section, the average number of loan days for undergraduate, master's, and Ph.D. students is far from the allowed loan days. For example, undergraduate students are obliged to return books to the library after 15 days; however, the average loan period is 70 days for these students. Therefore, it is necessary to take measures such as  increasing the loan duration and modifying library policies. According to the present research findings, Ph.D. students have more overdue than master's students, but the loan period for both academic degrees is the same. Accordingly, Ph.D. students, compared to undergraduate and master's students, need further resources and more time to conduct research. Renaud, Britton, Wang, and Ogihara (2015) and Arshad, Ameen, and Jabeen (2021) also found that undergraduate students checked out the most books, followed by graduates and faculty. Hence, it is necessary to change the library's policies in this context. This study's finding was in line with Yang (2020) who concluded that loan duration is a significant factor in judging how actively books are utilized. When librarians want to determine loan duration policies, they must pay attention to the student's status, e.g., undergraduate student, graduate student, faculty, and staff. Therefore the finding showed that making a formal planned evaluation of the existing collection was a seriously overlooked aspect of the collection management process in university libraries, which is in line with the study conducted by Ameen (2010). Furthermore, the findings revealed that students in the fields of study of ""Hydrology and Water Resources,"" ""Motor Behavior,"" and ""Art Research"" had the longest overdue period. Therefore, different fields of study have different loan durations, which may be due to differences in the nature of fields of study and students' information needs. The DT was drawn to investigate the influential factors on the loan duration, and it was shown that the first and second factors were the field of study and academic degrees, respectively. Therefore, the field of study is essential in formulating policies and circulation rules in the SCU library. Renaud et al. (2015) also pointed out the impact of the field of study on circulation data.
The rules regarding the number of books that can be checked out at a time and the duration of renewal in the academic library are also important, and it seems necessary to establish such rules according to the documentation of data mining research. The results indicated that academic degree had the highest impact on the frequency of renewals, and secondly, the field of study factor affects the renewals. According to the results, the policies related to renewals can be altered so that it would be possible for higher academic degrees to renew books for longer durations. In addition, according to information regarding the field of study, different policies can be considered for book renewals.
The library's circulation data mining is the method to determine the most frequently used subject areas. The SCU faculties recorded a higher level of borrowing and indicated which were more popular titles. These data could be used to specify the library's acquisition rules for each subject area, faculty, and copy.
Analysis of circulation data can effectively help the mining of strong associations between books and utilize the mining results for recommending relevant books to readers, thus offering them personalized service. The information mined can also serve as references for university professors in their teaching. The latter can incorporate associated knowledge in their classes to help the students better understand course material",6413,7069
10.1080/10572317.2020.1840001,,,False,Dimitrios Kouis,Library Service Platforms (LSPs) Characteristics Classification and Importance Ranking through DELPHI Method Application,"
The purpose of this research is to identify the innovative features of the LSPs that differentiate them from the LMS, as well as to evaluate their importance, based on the opinions of the Greek information scientists. The method used is the Delphi 2-round questionnaire. The results show that the experts most value an all-in-one system that incorporates all modules within it, instead of different collaborating software. Interoperability between systems, the adoption of new metadata standards, the SaaS architecture and the multi-tenant model are also held in high regard. In contrast, the use of the mobile library applications feature has achieved low ranking.
",2020-10-30,International Information & Library Review,Informa UK Limited,"Introduction
From the early 2000s, library collections began to change and shift from the printed form toward digitized and e-born material, leading to the creation of a hybrid environment, where a large variety of printed and digital content types co-exist. At the same time, the scientific community's requests for accessing content (a context inextricably linked to the libraries) were constantly increasing. In more detail, the continuous need for easily discoverable and immediately accessible digital scientific content forced libraries to exploit new options for their collection development and enrichment (e.g., digitization, journals subscriptions, institutional repositories, etc.). Consequently, the libraries' information systems also had to adapt to keep pace with contemporary needs and the latest technical requirements. The existing Library Management Systems (LMSs) were expanded or redesigned to meet the new needs that had arisen, such as the management of electronic subscriptions and the collaboration and exchange of data and information with other libraries.
Both commercial and open-source LMS vendors and development communities re-invented their offered solutions, focusing on staff and the end-users' new needs, as well as in the integration of technologies that could increase their performance, flexibility, and collaboration. Examples of these technologies are the discovery services, cloud computing, software as a service architecture and electronic resource management systems (ERMS). This long-lasting process of redesigning and reconstructing the libraries' information systems was illustrated in the transition from the Integrated Library Systems (ILSs) to LMS and then to the Library Service Platforms (LSPs), also known as 3rd generation library information systems. Marshal Breeding defines LSPs as ""modern library management systems that place special emphasis on the management of electronic material and resources"" (Breeding, 2015a).
Nevertheless, the concept of LSPs and the features that distinguish them from the previous systems are not clear. One explanation could be that these enhancements and new functionalities have been made available gradually over the last decade, so there is not a strict definition of the Library Service Platform concept and what differentiates it from the previous generation systems such as the ILS or LMS. Also, the simultaneous development and co-existence of the different generation library systems (namely LSPs, LMSs and even ILSs) resulted in the adoption of some of LSPs' innovative features, in the form of modules or external components by the other two (namely ILSs and LMSs), but without being truly unified systems, neither on the architectural nor on the technological level.
The boundaries between ILS, LMS, and LSP are blurred, with researchers, professionals, software vendors, and development communities providing different definitions, names, and lists of features that a system must meet to be categorized as an LSP. Besides, these short-lived features of an LSP have not been adequately assessed globally and much less by the Greek librarian and information science community. Libraries in the process of acquiring a new management system, or when they proceed with a major upgrade of the existing one, which usually occurs every 5 to 10 years, have to deal with new concepts and features that they need to invest in, or on the contrary, they do not need at all. For example, according to the 2015 survey on library automation performed by Marshall Breeding, libraries using Millennium, Aleph, and Voyager ILSs expressed an increasing interest (from 10% to 55%), over a period of 8 years (2007-2015), to migrate to the next-generation products (Breeding, 2015b).
With the target of creating cohesion, the first part of the present research aims to provide a unified conceptual framework for the LSPs, followed by a classification of their main characteristics through an open coding technique of bibliographic data. The second part attempts to verify and assess the appropriateness and importance of LSPs characteristics, through a two-round Delphi survey of experts in the field. The research findings could create a starting point and a guide for librarians and information science professionals who are in the process of upgrading or acquiring a new library management system or are developing new software solutions.
Literature review -Coding, data analysis, and classification
The aim of the literacy review conducted was two-fold: (a) to present and analyze similar efforts to evaluate library information systems and their characteristics and then (b) to collect and group the characteristics of the ILS, LMS and LSP that are dispersed throughout the literature.
The methodology followed for the review process cannot be characterized as a typical systematic review, however it bears many similarities with it such as: (a) time limitations (mainly articles from 2010 and onward, with a few exceptions), (b) types of publications (journal articles, conference papers and web sites of thematically related projects), and (c) specific relevance criteria for inclusion and exclusion during search (e.g., keywords such as LSPs, 3 rd generation systems, the similarity levels between papers, the library management system's popularity, etc.). The initial search retrieved 75 sources which after the methodology application were reduced to 46 (see Appendix).
These papers examined the library information systems and their characteristics, which in conjunction with the works of Marshal Breeding, has provided us with a satisfying and definite number of features shared between the ILS, LMS, and LSP. Furthermore, we have gathered and analyzed 18 previous works that evaluate library information systems. The literacy review included studies on open-source systems, commercial systems, and library software evaluation databases. The most important and indicative of these are presented below.
The vast majority of the LMS's evaluation surveys that have been conducted concerned open-source library systems, mainly due to the preexisting bibliography, the ease of access to their statistical data and because the open-source software is continually gaining ground in the preferences of librarians (Gkoumas & Lazarinis, 2015). In addition, LMSs retailers and manufacturersfor reasons related to their commercial strategies or/and policiesusually reveal little information about their systems, their code and their statistics, leaving no room for fair evaluation and comparison with open-source library systems (Balnaves, 2008).
Open-source LMS Balnaves (2008) attempts to evaluate Emilda, Evergreen, Gnuteca, Koha, OpenBiblio, PhpMyLibrary, and PMB, while he suggests five dimensions in evaluating open-source LMS (functional, architectural, community, code, and schema dimension). The researcher avoids a direct comparison between the LMS mentioned above by using metrics, as each of them addresses a specific audience. Therefore, each system is evaluated based on the size of the organization for which it is intended, as well as its use. Despite the fact that most of the LMSs evaluated by Balnaves are outdated, the methodology followed, and the evaluating dimensions applied have been used as the basis for future research; hence it was essential to include it in the literature review.
After analyzing most of the open-source software evaluation models, Londhe (2015) has created and applied a new model for evaluating LMSs. The Open Source Library Management System Evaluation Model (OSLMSEM) provides a simplified solution for comparing and evaluating OSS LMS in great depth, according to the researcher, as the evaluation is divided into two parts. The first part evaluates the functionality of the LMSs. It applies the same importance weight to primary function (search, cataloguing, etc.), while the importance weighting reduces to half for desired features (user reviews, customizable user interface, etc.). The evaluation scheme uses a 5-point Likert scale scoring system. The second part evaluates twenty different dimensions around OSS LMSs, following again the same methodology. The 20 dimensions are: (1) the community of the LMS, (2) documenting material, (3) the support, (4) the training on the use of the LMS, (5) the ease of use, (6) the usefulness, (7) the endurance in time, (8) the software updates, (9) the roadmap, (10) the websites or Wiki pages, (11) its acceptance by the libraries community, (12) the user licenses, (13) the vendor's professionalism, (14) the overall software quality (bugs, dead links, etc.), (15) security, (16) performance, (17) flexibility, (18) user satisfaction, (19) interoperability, and (20) the cost.
In the research conducted by M€ uller (2011), more than 20 OSS ILS and LMS were evaluated, based on how active their vendors and communities are, and they were ranked as sustainable, emerging, just released and inactive. From this ranking, the sustainable and emerging ILS and LMS (of which the standards were only met by Koha, Evergreen and PMB) were evaluated for their functionality based on 799 different criteria, grouped in 50 dimensions. Thus, these three LMSs were analyzed and evaluated on the standards they support, their report-generating tools, their management tools, the search function and the results representation, the circulation of the material, the journal management processes and their acquisitions modules. Despite Koha receiving higher scores, the researcher states that all three of them are remarkable systems, each with its strengths and weaknesses and that all must be considered when selecting an LMS, as different libraries have different needs. Gkoumas and Lazarinis (2015) have evaluated 13 open-source LMSs, based on the criteria such as the installation simplicity and the small amount of time required to install it, the software features, the users' satisfaction and finally the systems' usefulness in ten different workflow scenarios.
Koha, PhpMyLibrary, and OpenBiblio were evaluated and compared in Kumar's (2013) research on material acquisition, cataloguing, OPAC functions, circulation, serials, information management and system maintenance. Although Koha ranked first with a significant difference from the others, the researcher pointed out that PhpMyLibrary and OpenBiblio are ideal solutions for small scale libraries as they present less complexity compared to Koha.
Commercial LMS
Similar studies have also been conducted for commercial LMS, although as mentioned above, they are rare and, in most cases more specialized.
Fu and Fitzgerald (2013) have compared and evaluated two traditional LMSs (Voyager and Millennium) with three modern LSPs (Alma, WMS, and Sierra), based on (a) their software architecture and (b) their workflow and functionality. The researchers highlight the advantages of the LSP's cloud infrastructure in comparison to the client-server architecture of the LMSs (less time, cost, and staff required for installing, configuring, managing, and maintaining the systems). To measure the functionality of the systems, the researchers identified the 15 most important tasks performed by, or through, the library's management system. Next, they attributed a percentage to them, based on the average time the librarians spend on these tasks every day. The results show that LSPs can save up to 40% more time compared to the LMSs, as they are faster and less complicated in everyday library work, such as using the OPAC and managing electronic resources. Additionally, LSPs eliminate the time required for maintenance tasks. The research concludes by emphasizing that modern LSPs create new opportunities for the libraries, increasing staff productivity, reducing financial costs and improving services to users. However, they pointed out that libraries' staff have expressed concerns about the need to adapt to new technology and the difficulties that always exist when switching to a new system, as well as how technology can downgrade the librarian's role.
Singley and Natches (2017) focused on assessing the functionality of the integrated ERM tools within the LSP and more specifically in Alma, WMS, and Sierra. The purpose of this research is to examine whether the build-in ERM system fully meets the needs of the libraries, without the assistance of external tools. For the study, 299 questionnaires were collected from librarians who use these three LSPs. The LSPs were compared in terms of how well they perform in three tasks:
(1) evaluation of a journal package for purchase, (2) acquisition and implementation of a journal package, and (3) assessment of a journal package for subscription renewal. The answers were divided into ""in library system"" (without the use of external ERM tools), ""outside library system"" (using external ERM tools), ""both in and outside of library system"" (with the built-in ERM but also with partial use of some external ERM tools), and ""don't know or N/A."" The results showed that external ERM applications are used for basic functions such as the renewal of a subscription. Research highlights the weaknesses of LSPs in this area and suggests that there is room for improvement.
The most extensive research on the evaluation and comparison of LMS is the annual report conducted by the Library Technology Guides website (https://librarytechnology.org) which is managed by Marshall Breeding. The website collects data from more than 3,500 libraries from 80 countries each year. There is no separation between commercial and open source systems. Breeding analyses the results of questionnaires and provides statistical and comparative tables, along with the research findings (trends, repetitive patterns, etc.)
In the latest version (2020), 27 systems are evaluated in nine categories: (1) Satisfaction, (2) General functionality, (3) Functionality in the management of printed material, (4) Functionality in the management of electronic material, (5) Satisfaction from the provider/community, ( 6) Satisfaction with the support, (7) Improvement in the support (compared to the previous years), (8) Loyalty and confidence in the provider, and ( 9) Open source interest. Evaluation uses a 0 up to 9 score scale and participants are also asked to reveal if their library is considering migrating to a new system (and which one), if certain aspects of their current system need an upgrade, and if they would be interested in an open-source system. After the results processing, Breeding compares them with the averages from previous year's data.
Data analysis and coding -LSPs characteristics classification
The main output of the literature review was to determine and categorize the characteristics according to the purpose they serve and the functional category to which they belong. The number of characteristics that were identified were 136 in 15 categories (see Table 1), based on their frequency of appearance in the articles that were reviewed. These characteristics were also further categorized based on the library management system types (ILS, LMS, and LSP) that they were first introduced/used.
By selecting from the above set of the 136 attributes only those that were categorized as LSPs, another refinement occurred, resulting in a subset with 23 final characteristics. Furthermore, those characteristics were classified further into three categories as depicted in Table 2: functional characteristics (from A1 to A9), technical characteristics (from B1 to B9) and social characteristics (from C1 to C5).
The categorization of the characteristics was a necessary step before the evaluation of their importance by a group of experts through a ranking-type Delphi survey (Kobus & Westner, 2016). The preparation of three initial lists (functional, technical and social characteristics) prevented experts from getting overwhelmed by the amount of issues they should rank during the Delphi method application. In addition, during the first round of the survey, the participants were allowed to add characteristics they consider important. Applied to the given research context, the ranking-type Delphi study and the methodology that has been followed is described in the next section of the paper.
Methodology
In order for the readers to better understand how the LSPs major characteristics ranking results were acquired, it is necessary to present the phases of the methodology. The proposed plan was based on the work of Kobus and Westner (2016), as they were among the first to provide a comprehensive and precise step-by-step guide on how to conduct a rigorous ranking-type Delphi study around Information Systems (IS) research. Apart from IS research where ranking-type Delphi is frequently used and covers a wide range of topics (Hanelt, 2020   The methodology phases that were followed in the present survey are illustrated in Figure 1 and are slightly simplified in comparison to the ones presented by Kobus and Westner (2016), in order to be more compatible with the time constraints and effort required on behalf of the participants.
During the first phase, the research problem was identified and the development of the three initial lists of ranking issues were prepared (functional, technical and social characteristics). In our case, the primary research aim was to create a unified conceptual framework for the LSPs, followed by a classification of their main characteristics and their importance weights. Due to the unclear boundaries between ILS, LMS, and LSP among LIS community members, it was decided to follow the seeded approach where the important characteristics will be identified by studying prior research efforts rather than having the panelists/experts participate in a brainstorming session to generate the initial lists from scratch (Kasi et al., 2008;Keil et al., 2002). In this sense, the initial lists of characteristics, as depicted in Table 2, was transformed to the initial set of features/issues for the experts to rank.
The choice of the right experts for Delphi studies is one of the most important aspects for the whole process (Phase 2). Okoli and Pawlowski (2004) mention that experts' selection is ""perhaps the most important yet most neglected aspect."" In our case we identified the most suitable expert professional categories (Academics and Practitioners: System Librarians, Librarians, and IT/Software personnel) and then, after performing a thorough check of the available candidates, we proceeded with the selection, based on criteria such as the professional and research experience and the level of involvement with managing and/or operating library management systems. Due to the strict criteria and the relatively small size of the Greek LIS community, 17 experts were invited to the survey, which is within the target size of 10 to 18 as it is mention by Okoli and Pawlowski (2004). The table below (Table 3) presents the exact composition of the experts' panel as well as their main qualifications.
The next phase (Phase 3) included the implementation of the ranking rounds which focused on achieving consensus among the experts on the order of each of the items in the lists. The total number of issues for ranking was 23 which is considered a reasonable size (Schmidt, 1997). In our case, during the first round, experts had the ability to add extra items in the lists under evaluation. The first round was implemented with the use of LimeSurvey online survey tool (https:// www.limesurvey.org), as it supports ranking-type questions. Before running the next ranking round, for each item in the lists the following statistics were calculated:
Statistics for each issue/LSP characteristic Ranking Score (RSlow ranking score ¼ high importance): is calculated by the following equation:
where x the number of answer choice and w the weight of the ranked position (in our case, for the list with 9 items the #1 choice has weight equal to 1, #2 choice has weight equal to 2, etc.). Mean Ranking Score (MRS): is calculated by the following equation:
RS total count of responses
Ordinal Ranking Score (ORS): is the order values (1 st , 2 nd , 3 rd , etc.) of an issue based on the Ranking Score in ascending sorting (from lower to higher). Variance (r 2 ): is the sum of the squared distances of each choice by the experts from the mean value (l), which in our case is the same with MRS
where x i is the value of one choice from an expert and n is the total number of choices from all experts.
Standard Deviation (r): is the square root of the variance (Brancheau et  where a and b are an arbitrary set of values for the rescaling (e.g., for the list with nine items a ¼ 1 and b ¼ 9), RS i is the ranking score for an item and minðRSÞ and maxðRSÞ are the minimum and maximum ranking scores for all items.
Finally, between rounds we calculate (a) the Kendall's coefficient of concordance (W), (b) the Spearman's correlation coefficient r s (based on linear transformation equation), and (c) the Chi-square distribution p value (p-value) (based on chi-square and degrees of freedom), as explained next.
Degree of consensus metrics
Kendall's W: Assume there are m experts rating n issues in rank order from 1 to n and r ij is the rating that expert j gives to issue i: For each issue i, the sums of ranks are calculated as R i ¼ P m j¼1 r ij and R is the mean of the R i values. Next S is the sum-of-squares statistic over the row sums of ranks R i and is calculated as
Finally, Kendall's W statistic can be obtained from the following formula:
Þ , where 0 W 1, with 1 representing perfect concordance (0 indicates no consensus and 1 perfect consensus) Spearman's correlation coefficient (r s ): As there is close relationship between Spearman's correlation coefficient r s and Kendall's W statistic it can be directly calculated using the following relationship
where k is the degrees of freedom (in our case k is equal to the number of issues per list) and C is the gamma function For the interpretation of the results Schmidt (1997) mentions that when Kendall's W is greater than 0.70, it signifies strong consensus; when the value is between 0.50 and 0.70, it signifies moderate consensus; and if the value is less than 0.50, it signifies that there is little consensus among the panel members.
Results
Due to the fact that the initial lists of characteristics (ranking issues) were already prepared through the bibliographic review, only two Delphi rounds were necessary. The response rate between round one and two was 100% meaning that all 17 experts remained in the survey. For each list of characteristics results, sub-results and calculations, necessary data to support the findings are presented in the following figures and tables. Specifically, Figures 234depict the stacked charts with the percentage of answer choices for all lists of characteristics, per Delphi round. Also, Tables 4,6, and 8 present the values of the statistical parameters that were presented in the methodology for all lists of LSPs characteristics for the two Delphi rounds. Finally, in Tables 5,7, and 9 the set of metrics that were used to identify the degree of consensus and were explained in the methodology are being calculated.
In the first part of the survey, entitled ""Functional characteristics,"" experts were asked to define the ranking score regarding certain operational aspects that LSPs offer and to distinguish them from the previous systems (see Figure 2).
The analysis of the results of the second round of the Delphi method (see Table 4) showed that the support for multiple metadata schemes and types of material (A1: RS ¼ 39, MM-ORS ¼ 1, r ¼ 1.6), the support for new conceptual structure of information resources and linking methods (A2: RS ¼ 44, MM-ORS ¼ 1.4, r ¼ 1.2), and the support of unified management workflows for analogue and digital content from various sources (A3: RS ¼ 56, MM-ORS ¼ 2.4, r ¼ 1.9), were ranked 1, 2, and 3, respectively with their ranking values exhibiting moderate differences.
For the 4 th place experts have chosen the integrated Electronic Resource Management Service (ERMs) as core module (A5: RS ¼ 82, MM-ORS ¼ 4.5, r ¼ 1.5), followed by the integrated Discovery service (A6: RS ¼ 83, MM-ORS ¼ 4.6, r ¼ 2.1) and the advanced end-user web interface (A7: RS ¼ 90, MM-ORS ¼ 5.2, r ¼ 2.0) exhibiting similar ranking values. Finally, the support for integration with e-Learning Management Systems was ranked 7 th , while in 8 th position was the advanced reporting system. In the last place was the enhanced application for mobile devices with the lower ranking score.
Concerning the degree of consensus among the experts, both the Kendall's W and the Spearman's correlation coefficient indicate that moderate consensus was achieved at the second polling round (W ¼ 0.54 and r s ¼ 0.48see Table 5). Also, it is worth mentioning that ORS values, for all characteristics, remained constant between round 1 and 2.    In the second part of the survey entitled ""Technical characteristics,"" experts were asked to define the ranking score regarding certain technical and technology-oriented aspects that LSPs offer (see Figure 3 and Table 6).
The second Delphi round gave the highestranking score to the unified metadata model (B1: RS ¼ 44, MM-ORS ¼ 1.0, r ¼ 1.3), followed by the Software as a Service (SaaS) architecture support (B3: RS ¼ 56, MM-ORS ¼ 2.0, r ¼ 2.6), in 2 nd place. The multi-tenant model support with high level of customization (B2: RS ¼ 69, MM-ORS ¼ 3.1, r ¼ 2.4) and the option of having full access to the source code and data schemes documentation (B8: RS ¼ 69, MM-ORS ¼ 3.1, r ¼ 2.3) received the same score during the 2nd round, which resulted in their tied for the 3 rd place.
Additionally, the characteristic entitled exclusively web-based access was ranked 5 th , followed by the centralized management of support, administration and configuration services, in 6 th place. For the last three places experts chose the full access to the data storage and services via open APIs, the high level of scalability, security and performance efficiency via the exploitation of cloud computing and the high level of extensibility via modules, micro-apps and widgets.
A low level of agreement (W ¼ 0.13 and r s ¼ 0.03) was found during the first polling round (see Table 7), which is not uncommon (Schmidt et al., 2001). Nevertheless, the degree of consensus was still low (W ¼ 0.39 and r s ¼ 0.33) after the second Delphi round, indicating the different views of the experts. Also, it is worth mentioning that the ORS values, for 4 out of 9 characteristics changed between round 1 and 2 (B1 ! B3 and B5 ! B7).
Based on the results presented above, another polling round would be considered necessary for part B of the questionnaire. Nevertheless, it was decided that the multi-round polling requires a great deal of time and inevitably would lead to ""survey fatigue"" with some participants dropping out. The fact that the features in the second part of the survey were ""more technically oriented"" (characteristic B3 to B7), combined with the different backgrounds of the experts (librarians, IT specialists, academics) may be the reason for the divergent opinions.
For the last part of the survey, the social characteristics were ranked by the panelists (see Figure 4 and Table 8). The 1 st place was taken by the support of metadata shared management (C1: RS ¼ 29, MM-ORS ¼ 1.0, r ¼ 1.0), the 2 nd place by the virtual networks of Libraries support for resource sharing and unified patrons' services management (C2: RS ¼ 42, MM-ORS ¼ 2.1, r ¼ 1.1), and the 3 rd place was given to the community driven co-design and co-development based on the real needs of librarians and end-users (C3: RS ¼ 54, MM-ORS ¼ 3.1, r ¼ 1.1).
In the 5 th place is the characteristic that was proposed by the participants on the 1 st round of questionnaires, the collection's enrichment with vocabularies and external collections via linked-  data -C6 (77), displacing to the last place the social media paradigms exploitation in order to improve patrons' engagements via mobile apps C5 (89).
A strong consensus was achieved after the second Delphi round (W ¼ 0.73 and r s ¼ 0.67see Table 9), for the last part of the survey, indicating that experts shared common views on the LSPs' social characteristics.
Discussion
From the functional aspect, information scientists and professionals wish that the new generation of library management systems (as the LSPs) could provide modules that are tightly integrated in an ""all-in-one"" solution. This tight integration vision includes both the various content types (digital and analog) unified support (A1, A3, and B1), including the transition to the new emerging metadata standards (A2), as well as the seamless support of all management operations (ERMS, discovery service, reporting, mobile applications, etc.) and the communication with external information systems. The multi-vendor solutions, which cooperate through ""loose"" or ""tight"" integration architectures are not appealing any more, due to their complexity and dependence on vendors' strategies and technology developments.
Additionally, a point for further discussion is that the closer integration with Learning Management Systems was not considered as top priority for the LSPs by the experts (A4 -7 th place in the ranking order). Nevertheless, a significant number of research efforts investigating the academic library's contribution to student learning and success outcomes, highlight the need for the library's integration with the institutional learning analytics (Beile et al., 2017; Oakleaf, 2018; Sant-Geronikolou et al., 2019), while others provide evidence that accessing the library resources though the Learning Management Systems has (Chew et al., 2017;Kampa, 2017) a positive impact on the students' learning process. It seems that the community of librarians (as it is reflected by the opinions of our experts) does not hold in high regard the importance of capitalizing on the learning spaces that learning management systems have created to increase its visibility and to strengthen its role.
The participants gave the highest scores to the native Software as a Service (SaaS) architecture (B2), a model that provides the LSPs with the ability to operate their management system out of the library premises, thus reducing significantly the financial cost and the support needs. Also, the creation of virtual networks (C2) between cooperating libraries through the multitenant mode (B3), also ranked high in the experts' choices. This is also reflected in the high scores that social-oriented services received, such as the shared metadata management (C1), resource sharing and unified patrons' management (C2) under social characteristics.
Nevertheless, although the libraries wish to be members of larger cooperating groups and consortia, at the same time they seek a high level of customization for their LSP, according to each library's individual needs. This can prove to be a challenge for the vendors and the developers, to take into careful consideration during the implementation of their solutions.  Though the ranking procedure followed in the survey it was revealed that experts urge for the adoption of more open solutions by the LSPs' vendors, by providing access to the source code (B8), to the data storage and to a set of APIs (B5 and C4). Also, it is considered important that community co-design and co-development (C3) are essential characteristics that are aligned to the general trends that apply in information systems, with the case of FOLIO being an indicative example of that (Owens & Thomas, 2019).
Finally, it was surprising that experts did not attribute a high ranking position for the need of modern mobile interface or/and application that will support the users' engagement and social interaction with the library (A9 -9 th place in the ranking order and C5 -6 th place in ranking order). Although some efforts have been made to identify the most important components in the design and development of library mobile apps (Mansouri & Soleymani Asl, 2019; Sant-Geronikolou et al., 2019), the absence of solutions provided by the LSPs' vendors may be the reason that the communities of librarians do not yet value the opportunities behind such technology. Modern mobile applications are a gateway to the library and a tool to disseminate knowledge, communication and connected learning through collecting the intellectual capital associated with information about library material and services' utilization and usefulness (Sant-Geronikolou et al., 2019).
Conclusions
The research focused on creating a conceptual framework for the LSPs and their characteris",6862,7608
10.1108/lm-12-2020-0177,,,False,Dimitrios Kouis,Migrating to a shared Library Management System: evaluation from the perspective of librarians and lessons learned,"
Purpose -The purpose of this article is to investigate certain aspects, problems and benefits from the migration to a shared Library Management System (LMS). Design/methodology/approach -A review of the literature and a quantitative survey was conducted, based on a structured questionnaire, with a response rate of 44.7%. Findings -Among the crucial issues that should be taken seriously into consideration when transiting into a new and shared LMS, are the audit of the data quality before migration, the employees' training design and the composition of the LMS central support team. The benefits of a shared LMS are mainly effectiveness in terms of libraries' budgets and time for the employees' day-to-day work. Originality/value -The survey presented in this article evaluates the merits of a shared LMS and contributes innovative aspects to the existing bibliography by investigating issues and problems that arose during the transition. This way, the professionals involved in similar initiatives will benefit by avoiding possible mistakes and drawbacks when implementing such a project.
",2021-05-03,Library Management,Emerald,"Introduction
Library consortia is a concept that is met for over a century now (Liu and Fu, 2018). They are groups consisting of two or more libraries that cooperate in several domains such as sharing resources, exchanging expertise and inter-coordinating activities (Rosa and Storey, 2016). Consortia have been playing a vital role in academic libraries, especially during the last decade, because of the economic crisis that has led them to a tremendous funding reduction (Machovec, 2014). More specifically, academic libraries have started exploring new ways of collaboration by abandoning their local Information Library Systems (ILSs), which provided the proper modules for core operations, such as cataloging, circulation, patrons' management, etc. and by adopting next-generation software, namely, the Library Management Systems (LMSs) and the Library Service Platforms (LSPs) (Kouis and Agiorgitis, 2020). These new systems not only offer additional functions, such as Electronic Resources Management Services (ERMSs), discovery services, advanced metadata management, etc. but they also take advantage of cloud computing and web technologies to facilitate a multi-tenant environment (shared model) for participant libraries (Breeding, 2016).
Continuing with their enduring collaboration practices, libraries have begun to be driven to a shared LMS or, even better, to a LSP operated by a consortium, aiming to reduce the individual library's budget, to respond to new demands more cost-effectively and to secure their sustainability (Machovec, 2014;Fu and Fitzgerald, 2013;Kouis et al., 2018). Indeed, several libraries around the world are now collaborating with like-minded institutions through adaptation of the consortia approach to multi-tenant LMSs and LSPs, in order to provide their users with integrated services (Kouis et al., 2018;Liu and Fu, 2018;Chan et al., 2018;Stanley and Owen, 2018;Stewart and Morrison, 2016). However, after implementing any initiative that serves a community, and in this case after implementing shared LMSs and LSPs, it is a commonly accepted good practice that services should be regularly evaluated to measure their impact on their community (i.e. library staff and users).
Libraries' shared systems operated by a consortium are a relatively new domain of expertise (Liu and Fu, 2018), with most of them not having been thoroughly evaluated yet, in terms of their usability. In an effort to fill this gap, at least to some extent, the present paper aims at evaluating the several aspects related to the Hellenic Academic Libraries Link (HEAL-Link, 2020) initiative for establishing a multi-tenant shared LMS in local infrastructure for its member libraries (Kouis et al., 2018). To this end, a survey has been conducted among the librarians who operated the shared system in their libraries. This survey's main issues concerned the participants' background, professional profile and expertise, their opinions about the previous local systems, the migration process to the shared LMS and their evaluation of the new system modules. Finally, they were asked to evaluate the support they got from the central service team that was also responsible for the day-to-day management and operation of the shared LMS.
Related work
As it has already been mentioned, related works regarding the implementation of nextgeneration shared LMSs/LSPs are rather limited, let alone the evaluation of such services. One of the pioneers in the adoption of shared LMSs, in library consortia, is the Orbis Cascade alliance (OCA), which is a 39 academic libraries consortium in the Pacific Northwest (Zhu and Spidal, 2015). They run a shared LMS since 2015 (Liu and Fu, 2018). Based on the findings of the analysis of the OCA programs, performed by Liu and Fu (2018), it has been found that the shared LMS has a hugely beneficial impact on the community that it serves, and it generates many opportunities and challenges on almost every aspect. More specifically, the aforementioned authors pinpointed particularly the following areas of interest: (1) resource sharing, (2) discovery and user experience and (3) shared content.
Using another approach, the Wales Higher Education Libraries Forum project procured and implemented a shared LMS for all the universities, together with the National Health Service Libraries and the National Library in Wales (Owen, 2016). Wales shared LMS has been in operation since 2016 (Daniels, 2018). Based on the analysis of Stanley and Owen (2018), the shared LMS brought the following benefits to the member libraries: (1) easiness of access to the collections in any format, (2) reduced complexity for the authentication and authorization processes and (3) new functionality and services.
A more recent approach is the implementation of a shared LMS from the Joint University Librarians Advisory Committee in Hong Kong. They started using a shared LMS in 2017 (Liu and Fu, 2018;Lam, 2017). Until now, the benefits from the implementation could be summarized as follows: (1) shared cataloging and collection development, (2) shared workflow, expertise and training and (3) better user experience (Liu and Fu, 2018;Chan et al., 2018).
Furthermore, there are several ongoing projects regarding the implementation of shared LMSs, but they are still in their infancy. For example, there is the SILS Project (Systemwide ILS Project (SILS), 2020), the project from the Consortium of Icelandic Libraries with 300 library members and the project regarding the implementation of a shared LMS from the Ontario Council of University Libraries (OCUL) (OCUL Collaborative Futures, 2020).
In general terms, it seems that all the consortia that decided to migrate to a shared system enjoy the following benefits from the employment of such services. Firstly, they all concluded that there had been a cost reduction due to lower hardware expenses through hosting the software to a cloud-based system and not to local servers (Kouis et al., 2018;Owen, 2016). Additionally, unified search across the collections of several libraries provides benefits to users, since they can retrieve all the information they need through a single web interface (Owen, 2016). Moreover, shared LMS consumes less staff time because the management of electronic and print collections (i.e. acquisition, cataloging, circulation, etc.) is more efficient and more straightforward (Liu and Fu, 2018;Bryant and Ye, 2015;Fu and Fitzgerald, 2013). Finally, the use of a shared LMS enhances deeper collaboration among different libraries by creating common workflows (Liu and Fu, 2018).
However, the survey presented in this article not only evaluates the merits of a multitenant LMS, as the above papers do, but it also contributes with innovative aspects to the existing bibliography by investigating possible issues and problems that arose during the transition to this new schema. This way, the professionals involved in future initiatives of shared models among multiple libraries will benefit further because they will be informed about possible mistakes and drawbacks to avoid when implementing such a project.
Methodology
Toward the investigation of librarians and other professionals' opinions on issues related to the Hellenic Academic Libraries Link (HEAL-Link, 2020) initiative for establishing a multitenant shared LMS, a quantitative survey was conducted based on a structured questionnaire. The types of questions were based on the 5-point Likert scale, which is commonly used in surveys concerning the evaluation of population attitude or opinion (Mircioiu and Atkinson, 2017). The survey was divided into three parts (Part 1-3).
The questionnaire was sent to the personnel of the 26 libraries (273 employees) participating in the shared LMS. The response rate was 44.7% (122). The responses are considered more than adequate to avoid sampling bias in a quantitative survey (Qualtrics.XM, 2020). The majority of the respondents were librarians (114), while the rest (8) were IT professionals. Furthermore, most of them (90.2% -110) were employees in the same library for more than 10 years.
Participants were asked to agree with the terms and conditions of the survey before filling in their responses. The LimeSurvey Online Tool (LimeSurvey, 2020) was utilized to implement the survey. The survey was conducted in January and February of 2019.
Results
Part 1 of the survey (Profile and expertise of participants) aimed to identify participants' experience in the previous ILSs, before migration and their expertise in bibliographic standards and cataloging rules. Based on the participant's responses in Part 1 (Question 1) of the questionnaire (see Figure 1), the vast majority of the participants used and accordingly had experience with the ILSs modules, which mainly concerned classic library functions, namely cataloging (94%), circulation (93%) and authorities (74%). The number of participants who had experience with the less common modules (e.g. serials management -61%, reports and statistics -44%, acquisitions -33% and system management -25%) was significantly lower. This could be explained by the fact that in the Hellenic Academic Libraries' environment, it is commonplace that the majority of librarians are involved in core library functions, such as cataloging, circulation and authorities, while only a minority of them is involved in other functions such as system management and acquisitions.
Another essential information related to the participants' background was the LMS they used before the migration (Part 1 -Question 2). According to the responses, most of them used Sirsi Dynix Horizon (41%), GEAC Advance (34%) and Aleph Ex-Libris (14%). The rest (11%) of the participants used other systems, such as ABEKT (a Greek ILS), V-Smart, etc.
An additional aspect that needed to be investigated was the participants' level of familiarization with bibliographic standards such as UNIMARC and MARC21, because it was closely related to the use of the new system and the migration and training activities. The participants denoted (Part 1 -Question 3) that they were quite familiar with the two predominant standards, namely, UNIMARC (71%) and MARC21 (65%) and the AACR2 (86%), although most of them used mainly UNIMARC with their previous ILSs. This entailed more effort on behalf of the librarians, as the new shared LMS would support MARC21.
Moving to Part 2 (Migration and training) of the questionnaire, initially, the reasons for migrating to a new LMS are discussed (Part2 -Question 1, see Figure 2). The majority (76%) of the participants stated that the migration to a shared system was necessary for the library to reduce operational costs (Figure 2/06). Around 57% of the respondents believed it was important to migrate to a new, modern system (Figure 2/05). This could be attributed to the fact that the previous ILSs were in some point obsolete, dating back to the beginning of the new millennium. Nevertheless, 49% of them agreed with the statement that their previous system could actually support the library needs (Figure 2/04), while 30% of them were against this statement and one-fifth (20%) of them remained neutral. Another interesting finding is that 33% and 43% of the participants stated that there was no support from the vendor for their previous ILS (Figure 2/03) or that it was obsolete (no development or new versions -Figure 2/02). Finally, a quarter of the participants stated that their previous ILS did not support new requirements, while 51% disagreed with this statement (Figure 2/01).
Table 1 gives a more analytical view of the responses regarding the reasons for migrating to a new LMS and it clearly depicts that employees were strongly influenced by the previous ILS that each library used. More specifically, the participants coming from the libraries that Migrating to a shared library system were employing Geac ADVANCE and Horizon tended to support the idea of the transition to a new system (Table 1/05-74% and 64%, respectively), mainly because it was important for the library to reduce operational costs (Table 1/06-78% and 82%, respectively), as well as because these systems were technologically obsolete (Table 1/02-62% and 46%, respectively). Also, participants coming from the libraries that were employing Horizon were not pleased with the vendor's support (Table 1/03-48%). However, respondents who believed that their previous ILSs (Geac ADVANCE/Horizon) did not support new requirements (Table 1/01) were 33% and 24%, respectively, which is considered a low percentage in comparison to those who took a neutral position (29% and 28%, respectively). By this comparative result, one could infer that although the majority of the employees agreed to the transition to a new system for various reasons, only those who clearly stated that their old systems could not support new requirements seem more eager to abandon them. In addition to this, the lower percentage (Table 1/05-6%), in comparison to the aforementioned ILSs, claiming it was important for the library to migrate to a new, modern LMS, comes from the libraries employing Aleph. This can be explained by the fact that they were entirely against the statement that their system did not support new requirements (Table 1/01-0%) and with the statement that the system was obsolete (Table 1/ 02-12%). It seems that for Aleph respondents, the main reason for migrating to a shared LMS was mainly the reduction of the operational costs (Table 1/06-59%, with 29% remaining neutral). Overall, it should be highlighted that almost half of the respondents believed that all three previous ILSs could actually support library needs adequately (Table 1/04, 48%-GEAC, 53% Aleph, 50% Horizon). Such a result leads even more to the inductive conclusion that the decision to adopt a shared LMS was made basically due to the various financial problems that libraries face. Indeed, during the past few years, the funding of libraries in Greece has decreased dramatically. Therefore, libraries had to find ways to reduce their operating costs. One way was to use a shared LMS, where maintenance and operation costs would be shared between libraries-members.
In Figure 3, the responses regarding the data migration and the training services evaluation are shown (Part 2 -Question 2). In particular, only a quarter of the participants (Figure 3/01-25%) argued that the data migration was successful, choosing the following reasons for this. The first reason was the problems that occurred during the conversion of the existing records from UNIMARC to MARC21 and the records' merge from the different ILSs (Figure 3/02-51%). Another reason was that the original bibliographic records were rather problematic as they presented low conformance with the relevant standards (Figure 3/03-46%). Such a statement, though, contradicts the results of the question about the familiarization level with the bibliographic standards and the cataloging rules, where the majority of the participants clearly characterized themselves as experts, both in MARC Migrating to a shared library system and in AACR2. This raises questions about their adequate knowledge of standards and the efficiency of the cataloging workflows that they follow, leading the discussion in favor of creating a centralized, well-qualified support team for these matters (Fu and Fitzgerald, 2013).
As far as the training sessions' duration and the relevant material are concerned, only 18% of the participants (Figure 3/04) stated that they were sufficient. Around 43% of the respondents reported that they would like to have training focused on real case scenarios regarding the system operation (Figure 3/05) and another 46% wished to have a more thorough training related to the LMS shared mode of operation (Figure 3/06).
Next, in Part 3 of the questionnaire (New LMS and Support evaluation) participants were asked to evaluate both the shared LMS main modules and the central team that was created to support and manage its operation. Before proceeding with the evaluation of each module, participants had to select first which modules use in their day-to-day work (see Table 2).
Based on their responses, they were allowed to evaluate only the modules they use. The modules' usability aspect was rated with a scale from 1 to 5 (1-low/5-very high). The results of the evaluations, including useful statistical parameters are depicted in Table 3. For example, the Shapiro-Wilk test was selected to test the normality of the gathered data in each item. That is, the closer the metric to 0.999 and its p-value to <0.001, the higher the likelihood to have a dataset with items that follow a normal distribution (Shapiro and Wilk, 1965). As shown in Table 3, the participants expressed their satisfaction with the new LMS modules, despite the problems that arose in data migration and staff training (Part 2 of the questionnaire). The Circulation -Patron management and Acquisitions modules received the highest scores (3.88 and 3.79, respectively). On the contrary, serials module received the lowest score (3.31). Also, the shared LMS modules Finally, participants were asked to evaluate the central support service of the new shared LMS (Part 3 -Question 2). Only 23% of the participants stated that the supporting team personnel (number of members) is adequate for the workload of this project (28% remained neutral). Consequently, additional staff should be added to the support team to respond on time to the problems that may arise. However, around 52% of the participants stated that the response time of the central support team is satisfactory, while another 32% remained neutral. This leaves only a low percentage that is not satisfied with the central support team's response time. Such a result probably implies a solidarity between colleagues, who understand the effort that a small group of employees takes to manage the workload of a collaborative schema. This is also the possible reason why 57% of the participants appeared more satisfied with the actions and the guidance the supporting team personnel gave them for solving problems (30% remained neutral).
Discussion and conclusions
The recent economic crisis accelerated the decision of the Hellenic Academic Libraries to adopt a more sustainable schema regarding their core and future functions by forming a consortium. The consortium endeavored to replace the libraries' local systems with a multi-tenant shared LMS in a local cloud infrastructure. After five years since the transition to this new era, a survey was conducted to evaluate the general feel in the libraries' environment and the specific benefits and drawbacks the shared LMS may have entailed.
The cost-effectiveness, in terms of budgets and time, was the most significant reason for migrating to a new shared LMS (Liu and Fu, 2018). In our survey, the cost factor was so crucial that it even surpassed the satisfaction that many employees stated that they had with their old ILSs (Gutierrez, 2019).
Additionally, collaborative schemas are more effective for the employees' day-to-day work when it comes to a system's core and advanced functions, because common standards and policies are gradually being established (Stewart and Morrison, 2016). Again, in our survey participants realized that a collaborative system could reduce the cost and time of the cataloging process through the reuse of already created bibliographic and authority records.
Almost all similar efforts for adopting a shared LMS identified the importance of the migration phase. For example, the data quality should be very carefully considered before migration, as the records exported from the old systems will usually affect the efficiency of the new system (Shadle and Davis, 2016). For reasons already mentioned in the results section, data migration was not very successful, with 51% of the participants attributing it to their bibliographic records errors and quality. Training should also be taken seriously into consideration when transiting into a new and shared LMS (Liu and Fu, 2018;Shadle and Davis, 2016;Lam, 2017). The staff involved needs thorough training following real case scenarios.
The decision to evaluate the central support team was taken on the basis that their role is considered crucial for the successful implementation and the sustainability of a shared LMS. The consortium support team has the responsibility to operate the centralized data services and the unified workflows of a shared LMS as described by Fu and Fitzgerald (2013). Based on the present survey responses, the support team must be consist of an adequate number of experts to respond timely and accurately to the questions/problems that arise during the migration and later on, when the new system is fully operational.
Migrating to a shared library system",4331,4906
10.1108/jd-08-2016-0102,,,False,Guihua Li,New service system as an information-seeking context,"
Purpose -The purposes of this paper are to understand the user information seeking (IS) process under a new service system, to explore how users construct strategies and to identify the influences of the new service system on the IS process and behaviour. Design/methodology/approach -A quality research study design that included questionnaires survey, observation, thinking-aloud and interviews was employed. In all, 20 participants from 12 social science disciplines at Sichuan University, Chengdu, China were observed using the proposed Discovery Service system. The user IS process characteristics and taxonomy were analysed, and the stages matrix of IS under new system was built. Findings -Users' IS processes under the new system proved to be very complex. The features of three process stages, i.e. searching, scanning and verifying stages, and four different behaviour patterns were identified. Moreover, characteristics of IS behaviour under the new service system were described. Originality/value -User IS behaviour was addressed in a new service system context in this study, as has seldom been done in previous IS research. A comprehensive and user-centred understanding of users' exploratory practices in a new service system context was obtained, which will inform the development of information services for digital libraries. In addition, it indicated that the uncertainty of the IS process should be addressed by considering the relationships among IS, information retrieval and user-computer interaction.
",2017-10-03,Journal of Documentation,Emerald,"Introduction
New internet-based information services and applications are increasing in number and variety. Understanding the information-seeking (IS) process and developing systems and strategies to support it are central goals of information science. A new information service system could influence user IS behaviour significantly; however, such a system has seldom been considered in IS research despite the current rhetoric that more human-centric systems and research are required. In this study, this gap was addressed by investigating users' IS behaviours relative to a new information service, such as a Discovery Service system. A quality research study design that included questionnaire survey, observation, thinking aloud and interviews was employed, and content analysis was conducted to understand and classify the users' IS behaviours.
Background
The objective of most IS studies is to discover useful information about the habits and preferences of individuals or groups in order to design appropriate systems and services that can facilitate IS behaviours (Belkin et al., 1982a, b;Ellis, 1989Ellis, , 1993;;Kuhlthau, 1993). Previous recognise that users do not fully exploit these systems; however, because they do not observe users' complete interaction processes, these studies do not explain why this lack of full utilisation occurs.
More work is required because till date, research on user behaviours relative to a new information service has focussed on users' adoption behaviours rather than observing users' IS processes on new service systems. To fill the existing research gap, this study attempted to answer the following questions. What are the activities, strategies and behaviours used and perceived by users under the new service system. Moreover, how does the new system compare to existing IS models? What are the relationships among the core processes, contexts and behaviours relative to information seekers under the new service system? What implications do these processes, contexts and behaviours hold for enhanced information service and system design?
To answer these questions, we conducted an investigation based on the Discovery Service used by the university library at Sichuan University, Chengdu, China. Sichuan University Library implemented the Ex Libris Primo discovery and delivery system on November 18, 2013. To the best of our knowledge, this search channel was not known or used by most students at Sichuan University.
Methodology
To gain a thorough understanding of IS behaviours, an in-depth study that complements survey methods is required (Gable, 1994). Therefore, a mixed-method approach that includes interviews, user observations, thinking aloud and survey was employed in this study.
In this study, users' IS behaviours under an unfamiliar Discovery Service were investigated qualitatively in a laboratory context. In total, 20 participants of Sichuan University were recruited through purposive sampling. Sichuan University is located in Western China and has more than 3,900 teachers and more than 60,000 students and offers a comprehensive range of disciplines from 30 colleges. The criteria applied to identify participants were as follows: students representing many social science disciplines; students currently completing their graduation theses or other academic projects and students who did not use Minyuan Search (the library's Discovery Service) frequently. Participation was restricted to social science students because it was assumed that IS behaviour differs between social sciences and physical sciences (Ellis et al., 1993).
To ensure a user-centric research approach and gain an in-depth understanding of IS behaviours with a new service system, a four-phase study was designed:
• Phase 1: a researcher introduced the investigation content, programme and the purpose of the research, and described the use of the screen capture tool and the requirement that participants think aloud.
• Phase 2: a pre-survey questionnaire was administered and participants were interviewed. Questionnaire A (Appendix 1) was intended to gather information about participants' backgrounds, including their academic disciplines, grades, current academic tasks, library instruction sessions attended and self-evaluations of their information search skills. Then, based on questionnaire results, a researcher conducted a 5-10 min interview to investigate the participants' knowledge structures and their task perceptions.
• Phase 3: participants were observed performing a self-defined task. Participants used the Minyuan System to perform the task identified in the questionnaire. To eliminate pressure and maintain a free and relaxed atmosphere, participants were not required to achieve the task goal. This task took participants about 15 min. While the participants conducted the task, a researcher monitored their search operations step by step using a computer networked to the participants' computers, and also took notes on what the participants said. Simultaneously, the screen capture tool recorded all of the participants' screen activities. Rather than designing a task for the participants, participants were asked to identify a task related to their current academic project because it was expected that a self-identified task would lead to a more natural search process.
• Phase 4: a post-survey questionnaire (Appendix 2) was administered and interviews were conducted. The second questionnaire was intended to examine participant perception and assessment of the Discovery Service, including its functional effect, interface design, information resources and so on. In a 5-10 min interview, participants were asked to describe their usage experiences and to provide suggestions to optimise the Discovery Service. In addition, participants were asked to explain their special activities or emotions expressed during their IS process.
Findings
Process of IS under unfamiliar Discovery Service IR can be regarded as an IS process; thus, the latter may include one or more IR activities (Ingwersen and Järvelin, 2005;Vakkari et al., 2003;Wilson, 1999). In this study, user search behaviour was observed for a limited time. Consequently, it should be regarded as a series of IR rather than a complete IS process. Therefore, the observation model proposed by Karlssona et al. was adopted in this study because Karlssona et al. observed similar participants in similar circumstances. Karlsson et al. (2012) identified ten activities involved in a scientific IS process using a virtual university library, i.e. searches for the database or tries to use the interface with no results, searches for search terms outside the database and its thesaurus, builds or rebuilds the search query, executes the search, no search results or thousands of results, limits the search results, searches for search terms in the results, browses through search results, reads or browses through abstracts, changes the database. In this study, user IS behaviour was observed using a discovery system rather than a special database system, as was the case for Karlssona et al. Therefore, there could be differences between our user behaviour process and their ones. After all of the participants' IS activity records were reviewed, nine common activities were extracted as core behaviour features: choosing (or changing) the search portal, constructing (or reconstructing) the search strategy, searching, browsing the search results, filtering the search results, selecting a specific item, reading the summary information of the selected item, accessing the text of the selected item and browsing the text.
To compare the IS process under the new system with conventional IS and IR models and extract common features of the aforementioned activities, it is necessary to further classify the activities. Kuhlthau's (1988) information search process model proposed a six-stage mode comprising task initiation, topic selection, pre-focus exploration, focus formulation, information collection and search closure. We found both consistencies and differences between our observation results and that of Kuhlthau. First, this investigation primarily involved users conducting a certain project; thus, the task initiation process did not clearly emerge in our data. Second, activities Nos 1-3 were highly consistent with the topic selection of Kuhlthau's model, and searching was found to be a core process of this stage. Thus, we grouped these three activities as the first stage of the IS process, which we refer to as the searching stage (activities Nos 1-3). Third, we did not observe a clear boundary between prefocus exploration and focus formulation, as was observed with Kuhlthau's model;however,activities No. 4 and No. 5 showed a certain consistency with these two features. Therefore, we grouped activity No. 4 and No. 5 as the second stage. Considering that the scanning was the main process of these activities, we refer to this stage 1085
New service system
as the scanning stage (activities Nos 4-5). Fourth, activity Nos 6-9 were obviously consistent with the information collection stage of Kuhlthau's model. Moreover, verification of the information value was a characteristic of these activities; thus, we identified them as the third stage of the user IS process, which we refer to as the verifying stage (activities Nos 6-9). Fifth, our investigation stopped when users stopped seeking using the Discovery Service; thus, according to our data, there was no independent search closure stage (Figure 1). After obtaining more detailed information about the participants' IS behaviours, particular characteristics of the basic stages could be identified.
In the searching stage, understanding the function button of the search box and reconstructing the logic of all functions are critical activities prior to constructing a search strategy. Most users spent some time observing the discovery tool's function buttons; however, for the most part, participants relied on previous experience rather than considering the current context. For example, Minyuan Search's basic menu presents five options: articles, books, journal, dissertation and advance search. Most participants who selected journal input keywords for an article rather than a journal title because they were familiar with that procedure based on their use of CNKI and other databases. When they did not obtain satisfactory results, they appeared to be frustrated. In addition, users often forgot there should be consistent logic among the functions of the advance search. For example, even though they selected English as their language, they would actually input a Chinese word as search term. In fact, multiple function buttons and options might confuse users; therefore, reconstruction logic among these items is a difficult task for users of the Discovery Service.
The scanning stage involves a transition from summary information to detailed information, which appeared to be a difficult transition for most participants. Because most users expected that would be able to determine the value of an item based some critical information, such as abstract, journal, keywords and so on, they needed to manage access to such information. In fact, in Minyuan Search, a direct path, i.e. ""detailed information"", linked to the critical information; however, fewer than half of the participants noticed this option. Most participants took three unnecessary steps to obtain detailed information. They would use the location information button to access another database, such as CNKI, and then search for relevant documents, find them among the new search results, and select and open the desired document. It was also found that, compared to participants who did not find the ""detailed information"" button, those who did rated Minyuan Search more highly. In other words, the transition between searching and scanning stages will require significant time and will influence their overall perception of the Discovery Service.
In the verifying stage, a user will verify the value of the selected item and the value of the Discovery Service's special function. When a participant could find a valuable article using a previously unfamiliar Minyuan Search function, he or she tended to use that function again to find additional resources. However, even if a participant found valuable information, they did not rate Minyuan Search highly because the search process was more difficult than other more familiar processes. According to this appearance, satisfaction was related to both search results and the search process.
Taxonomy of user IS
According to the nine activities, which were derived from the users' IS processes, we coded each participant's records as new data. Through brainstorming with four experts, two dimensions were identified as the basic factors representing the differences among the participants' IS processes, i.e. their search strategies and search outcomes. To represent these dimensions, we counted the activity frequency of each stage and the total activity frequency of all stages. We used the ratio of the searching stage's activity quantity to the total activity quantity to represent each user's search strategy and employed the activity quantity of the verifying stage to indicate the user's search outcome process (Table I).
Considering that the units are incompatible, the values were converted into new standard values (Table II). The new sets of values were treated as the coordinates of points in a plane. A distribution diagram of user IS behaviour was constructed (Figure 2). As can be seen, the 20 samples are distributed into four quadrants and can be placed into four groups accordingly (Figure 2). The different behaviour modes suggest personality differences among the users. Combined with their interview data and background information, more knowledge was obtained from these behaviour records.
Group 1: the users in the first quadrant tended to take more time to construct and reconstruct their search strategies, and they downloaded more search outcomes than the average level. We refer to this search mode as ""Group-harvest mode"". Users employing this mode include participant No. 10, 12 and 14. 
New service system
The group-harvest mode users had strong exploring spirits. They noticed each new function button and tended to find new search words from search results and the reference bibliography of these search results (six search words on an average). As a result, they changed their search strategies repeatedly (Figure 3). They found new and efficient functions; consequently, they always returned to Minyuan Search after downloading the text of valuable items from other databases. However, their experiences with the output results of Minyuan Search were negative because they were highly capable of identifying a critical defect, i.e. that Minyuan Search only integrates a small number of databases. Group 2: the users in the second quadrant take more time to browse and review their search feedback. They downloaded more outcomes than the average level. This search mode is named ""Scan-harvest mode"". Users employing this mode include participant No. 1, 2, 6, 9 and 13.
The scan-harvest-mode users had confidence in their ability to find valuable information. They tended to use certain fixed search words (average: 2.4 words) or simply to make small changes (Figure 4). They also always neglected the new function button in Minyuan Search, causing a detour between the searching stage and the scanning stage. However, they could still obtain some valuable information because they had explicit search goals and selected the correct word. In fact, they were not involved in the Discovery Service; they employed their established search habits. As a result, they would give a worse comment to Minyuan Search.
Group 3: the users in the third quadrant just tried few search strategy during their IS process. They took more time to browse the search results, in which they found less useful results than the average level. This search mode is named ""Scan-pick mode"". Users employing this mode include participant No. 11, 15, 18 and 20.
The scan-pick-mode users had less experience with academic IR; thus, they spent more time understanding the functions of Minyuan Search. Despite spending more time, they could misunderstand the meanings and the relational logic of these function buttons. As a result, they often obtained zero feedback after using deliberate search strategies (Figure 5). However, ultimately, they could construct some knowledge about this service and obtain some interesting results. Therefore, using the Discovery Service was a good learning experience. As a result, they gave unexpectedly high evaluation to Minyuan Search.
Group 4: the users in the fourth quadrant tended to take more time to construct and reconstruct their search strategies. However, they obtained less relevant results than the average level. This search mode is called ""Group-pick mode"". Users employing this mode includes participant No. 3,4,5,7,8,16,17 and 19. More group-pick mode users thought it was difficult to conduct IS. They noticed the new function buttons in Minyuan Search and reconstructed their search strategies repeatedly. However, they rarely changed their search words. The wrong search words yielded many uncorrelated results. As a result, efficiency in the selecting stage was very low because they rarely picked information just like gems among dross (Figure 6).
Discussion

Characteristics of IS behaviour under new service system
We have identified nine activities in the IS process under the unfamiliar Discovery Service and grouped them into three stages: searching, scanning and verifying. However, these activities were described using an action unit. In fact, it is more important to understand users' behavioural objectives and identify their behaviour feature from an IS perspective, and then compare it to other conventional IS&R models. Thus, we observed the three process stages based on user objectives and activities. Unlike the abovementioned process characteristic observation, we attempted to extract behaviour features that provide a more universal understanding of the IS behaviour under the new system. By reviewing the interview transcripts and activity records of all four user varieties, we found that the reasons the user IS programme in the new system differed from other IS systems was that users had to handle three objects simultaneously, i.e., system, strategy and information. It also complicated the users' emotions because they often took a different attitude towards these three objects. In turn, their interaction with these objects could   III). In fact, user activities and emotions towards these three objects would inevitably influence each other.
In the searching stage, users worked on identifying their problem, finding the match point between their information needs and the system functions, and exploring ways to achieve an effective probe. First, they would be curious about the novelty of the service system and question its fitness for their task. Then, they would observe the system and understand its main function, particularly unknown functions, and then attempt to find the correct use method. Second, they had to determine a trial search strategy, which could be a more difficult experience than with a familiar system. Third, they would browse search outcomes and justify their relevance to their topic. In addition, they find and correct problems with their search strategy.
In the scanning stage, users worked on developing a correct search path and building a knowledge profile of the target topic based on this path. Once they found some prospective search outcomes, they would attempt to access its detailed introduction webpage, read the abstract and keywords, and often conduct more exploration after obtaining this information. This scanning progress helps users quickly construct a knowledge profile  for the given topic. Simultaneously, they would perfect their search strategy according to new knowledge about the service system and target topic until a satisfactory search strategy was determined. Users would experience more polarising emotions during this stage. With an appropriate search strategy, users would be surprised by the effect of the new system, which would increase the user's confidence relative to finishing the search task. This would indicate that users have adjusted to the new system context. Users who struggled to find an effective search strategy experienced anxiety and became disappointed, which may motivate them to use a familiar service system or search another topic. This phenomenon is similar to culture shock, which is defined as anxiety resulting from losing the familiar signs and symbols of social intercourse, and their substitution by other unfamiliar cues (Oberg, 1960). We refer to this as context shock.
In the verifying stage, users worked to verify the value of the prospective literature and collect various answers from the literature to solve their research problem. In fact, they would develop a final evaluation about the service system, search strategy, and information. To achieve this, the users would read the full text of the literature, select valuable content, or download literature for further reading. They also reviewed their search strategy in this programme. For users that considered that the search outcome was sufficient, they would achieve a higher sense of self-efficacy for their right strategic decision and would also confirm the value of the new system. Users that were unsatisfied could blame system error and may reject the system if they were searching a topic that they are familiar with, or they    could blame their own search strategy, if they are unfamiliar with the given topic. For users who were seriously dissatisfied with the literature, they could become frustrated and deny their own learning ability, or they would evaluate the service system as poor and not consider the system in future if other choices were available.
In general, the IS behaviour under the new system differs from normal IS in at least three ways. First, three factors, i.e., system, strategy and information, are interrelated and jointly influence user activity and the emotions related to IS with the new system. Second, user objectives relative to the system and strategy will become core objectives in the early stage of the IS process in a new system, and this leads to some special activities that seldom emerge in a normal IS context. Third, it is the user's task to solve a problem by using an unfamiliar system and strategy, and user affection is likely to be more polarised. In other words, context shock and context adjustment have a strong impact on user confidence in the IS process and subsequent use of a new system.
Implication for the development of service system
Studying IS with a new service system is of great importance because contemporary users must deal with an increasing number of new service systems. The results of this study contribute to improving understanding of the characteristics of user IS processes with new service systems, such as Discovery Service. In addition, this paper argues the process of IS under an unfamiliar Discovery Service, and presents a taxonomy of user IS behaviours. Such results are essential to the development of successful Discovery Service and the advancement of IS studies, as well as information service development for digital libraries. The analytical results reveal the following.
First, since IS is a dynamic interactive process, considering IS, IR and human-computer interaction (HCI) will enable the design of internet-based information service systems that can satisfy user information needs more effectively than systems designed only considering interface usability.
Similar to previous studies, the results of this investigation indicate that there are common IS activities and problems that influence the usability of new service systems, and further found when facing similar challenges, user perceptions and IS processes differ. Therefore, efforts should be made to understand such uncertainties through observations of IS behaviours combined with IR and HCI.
Second, user IS is a fluid and situation-dependent activity. Strategy selection is influenced by context and is a learning activity whereby users can gain thorough understanding of new service systems. Therefore, system support must be integrated into both service tools and service processes, especially for new service systems.
Our results demonstrate that users may highly evaluate a Discovery Service even if they obtain few useful search outcomes, which indicates that positive evaluation of a service depends on user discretion and their learning performance. This result suggests that people require real-time help with new systems to determine effective search routes, so user learning performance improvement should be considered by new service suppliers.
Third, as Nahl (2001) suggested, user IS behaviour is affected by internal and external contexts; therefore, information system designs need to capture more context information and to contextualise and personalise interactions with the service system.
To enable holistic portrayal of IS behaviour in a new service context, both internal contexts (early user experiences, service perceptions, knowledge and skill levels, tasks, etc.) and external contexts (discovery tools and other related services), as well as individual activities, were explored in this study. It was found that, in a Discovery Service context, various IS paths exist among user groups with different personality characteristics and levels of search experience. Thus, understanding these patterns and the user characteristics behind them and constructing timely interaction with them is indispensable.
1093

New service system
Limitations and future research This research represents an initial examination of IS behaviours with a new service system. Although a more holistic and user-centric understanding of users' exploratory practices was achieved, there are still some limitations.
The most obvious limitation is that this study was conducted in a laboratory setting. The time constraints limited the search strategies available to participants. To reduce the negative effects of the laboratory setting, the participants were permitted to select their search tasks. Consequently, search complexity could not be controlled. In the future, a longitudinal study could be useful because user efficiency should evolve with use experience, and the interaction between IS and service system features could be determined.
Another limitation is the small number of participants. This study provided limited qualitative insights regarding related questions. Thus, further empirical research and trial implementation should be conducted. A similar investigation with a larger sample and a quantitative analysis method would be useful to identify whether IS patterns were similar to the results obtained in this investigation.
In addition, this study suggested that some user factors, such as learning style and cognitive ability, could play critical roles in relation to user IS patterns with a new service system. Further research is also required to determine their effect path on the new service system context.",5025,5467
10.1016/j.procs.2015.07.574,,Lindy et al 2015,True,Cinthya Dwi Lindy,Library System Development Implementing Integrated Book Circulation for Interlibrary Loan,"
The purpose of this research is to develop a library system by implementingintegrated procurement flow, return and borrowing books, so that the inter-library loan can be done in an integrated manner, with the hope to facilitate members to borrow varied books thereby increasing library transactions. The methods used include the method of analysis in the form of literature and field studies to observe the activities of the running system, interviews with stakeholders, user surveys and design method using the Unified Modelling Language, and design the system architecture and features. The result of this research is the development of the library system by implementing an integrated flow of circulation of books between libraries. The conclusions of this research is to implement an integrated book circulationbetween libraries, then the inter-library loan can be done in an integrated manner.
",2015,Procedia Computer Science,Elsevier BV,"Introduction
Since the early 21 st century, web technology has been developing rapidly. Xiaohua Li 1 says that the Web 2.0 concept has reign all over the internet which emphasizing on users online experience, forcing the use to interact and contribute actively rather than leeching for information. The Web 2.0 concept contributes greatly to the development of Library 2.0. Library 2.0 itself focuses on developing user-centered services where the user can access and interact with libraries whether physically or virtually. However, library development with online public access catalog (OPAC) faces stagnation and Library search mechanism slowly deteriorate. Libraries, which is faced by declining budget and complex library system 2 , in need of effective technological aid to manage circulation. Also, the libraries need to do slimming on staff, but still able to handle growing collections. Because of the reasons mentioned previously, there is a need of an effective technology aid to manage the circulation flow.Wang, cited by Xiaohua Li 1 mentioned that Integrated Library System (ILS) is really affected by web development to develop an integrated system with newest technology and interesting features. The main challenges faced by ILS are static and outdated interface, aren't scalable, and not user friendly.
Users who are searching for book often searching for the book via provided catalogue. After obtaining preferred book code, they go to designated bookshelf and keep searching for the book until the book is found. This kind of searching might take some time if the collections are huge. The other problem arises when the library users busy with their activities and as the result, they can't visit the library very often. Limited collections are another big problem nowadays on smaller libraries, making their appeal less interesting on library users.
It is libraries' responsibility to accommodate library workflow, serve interactive web, and improving collections in facing problems such as advanced technology, economy impact, and growing collections. Hopefully by applying recommended service of integrated library system, all problems mentioned are resolved.
Problems
Problems that will be explained in this paper are: How to integrate book circulation system which are on certain territory How to implement easy to use and understand online book procurement flow
Purpose
The purpose of this research: Develop web-based integrated library circulation software. Building a library system to facilitate an automated interlibrary-loan without needing too much library staffs manual approval.
Designing and implementing interface and functionality to front-end interactive and informatively in order to appeal users to this library system.
Develop an integrated payment system with e-wallet feature.
Methodology
The objective of the survey is to assess problems faced mainly by library users. Data were collected by means of a questionnaire containing closed-ended questions and provides 'Other' option when the provided answers don't satisfy participants. The analysis on survey therefore involved quantitative data only. The data obtained were used as feature design analysis. The survey was conducted by online and are shared among students who have access to library. The question mainly asks about experience in using participants' accessible libraries and highlights features available on said libraries and improvements that could be done on the system. Other methods used is direct observation to libraries on circulation process, procurement, and internal activities happened in a library system. The library staffs were also asked on how should a process be done and simplified. The observed processes are generalized and used as the basis for implementing interlibrary system workflow.
To analyze library needs as one of the stakeholders, we conducted one-on-one interviews consists of several questions and focuses on existing implemented library system with libraries' supervisor. The questions asked are mainly about what information should be viewed on pages and rules on acquiring new collections.
Lastly, some literature reviews were conducted to summarize both strengths and weaknesses of past library applications or which are still in active development but support none or partial interlibrary circulation transactions.
Results
The following are results for survey and observations done. The graphics below explain suggested system that can be implemented to library system. Modular system design is used rather than centralized system design. With modular design, the library system will be divided into two subsystem: intra-library and inter-library respectively.Libraries interested in joining the system will have to install application provided in their intra-library network and registerin the inter-library system. The application provided itself is a library application which supports both intra and inter-library circulation.
Features
The features of the system are: 
Intra-library
Every local circulation such as intra-library loan, book reviews, and member management 3 will be done locally. Both database and application will be hosted on local library's network. The local system doesn't need to contact the bigger inter-library system when doing local transactions.Diagram shown below is a system design in which the intra-library system is implemented with centralized design where the web application is hosted in the main server and can be accessed via browser from staff's or user's PCs. 
Inter-library
The interlibrary system covers transaction that is done using internet connections. When doing transaction, intralibrary system can access other library's system by accessing inter-library system hosted in the cloud. Below is the diagram of inter-library system. Members of a library can loan other library's book by requesting the book via their local library system. The local library system then will contact the book owner library and asks for approval on the request. The book owner library then will send the book where the requester asked. The book tracking then will be handled by the system as there will be interaction in the system by the librarian every time the book arrived, sent, or being picked up by the requester. The diagram above is the implementation of the modular system. Each library will have their own intra-library system where all of them are registered in a single inter-library network. The inter-library network itself is a collection of API which enables the inter-library transactions such as inter-library loaning. Additional services outside of intra-library core service implementation, such as mail server for mail notification, are provided in the inter-library server rather than in every intra-library server.
Discussion
The decision on creating a simple application that supports only essential features are based on the need of masses. Most of the target library mentioned that either the current library application are too hard to learn or hard to maintain because of the complexity of the application. With this in mind, the application was created as simple as possible to ensure easiness in development, maintenance and scaling. Also the feature ""Booking book online and book delivery to preferred library"" are the most popular feature requested. Often users are forced to come to library and fill booking form so that the librarian could contact other library to borrow the book. The booking online feature eliminates the need of going to library, making busy user able to borrow books not available on their accessible library despite their activity.
Modular design is used to design the system in order to minimize the application unavailability when one or more system is down. For example, when the inter-library system is down, the normal intra-library system could still operate normally. Otherwise, if the system is designed with centralized system, failure in the system means every library connected to the system will be unavailable until the system is fixed.
Conclusion and future work
This paper has conducted survey on how improving a better library, which is able to compensate on library's user daily activities. Most of the existing library application solution freely available on the internet are centralized system. By developing the system with modular design, the library application will be easily maintained and are more scalable. The only problem that persists are the agreement between libraries and the infrastructure of the library itself. Without a good network connection, libraries will not be able to maximize the advantage gained by implementing this library system.",1526,1674
10.23974/ijol.2018.vol3.2.94,,Liu and Fu 2018,True,Guoying Liu,"Shared Next Generation ILSs and Academic Library Consortia: Trends, Opportunities and Challenges","
Next generation Integrated Library Systems (ILSs) have been maturing and adopted by more and more academic libraries. Many academic libraries have joined a consortium to collaboratively move towards a shared next generation ILS that sustains a deeper collaboration. Has this been a trend for academic libraries to share the new system in consortia? This article examines the adoption of the leading products in next generation ILSs to reveal the trend. Two case studies are conducted on A) a pioneer consortial adopter and B) a newly formed partnership on shared next generation ILSs, for further investigations on the impact on consortial members, the challenges the new shared system may cause, and the opportunities it brings to academic library consortia and their members.
",2015,Collaborative Librarianship,,"INTRODUCTION Library Consortia
Library consortia are groups of two or more libraries that ""partner to coordinate activities, share resources and combine expertise"" (Rosa and Storey, 2016, p93). Partnership, or collaboration, is the main purpose of a consortium. Other names have been used for collaborating libraries in library literature as well, such as cooperatives, networks, collectives, and alliances (Horton, 2015). In this article, we adopt the term consortium and/or consortia for libraries that collaborate with each other to achieve common goals.
Library consortia have existed for over a century. Their scope, type and size are varied. The collaboration of libraries can be at the local, regional, national or international level. Some consortia serve exclusively one specific type of libraries, such as academic or public, while others include multiple types. Many of them started as academic only and later expanded to include public, special and other types of libraries. (Bostick, 2001;Horton, 2015). The size of a library consortium could be as huge as Online Computer Library Center (OCLC) which had over 22,000 members in 2012 (Horton, 2015), or be very small like Keio-Waseda Consortium which only consists of two members (ProQuest, 2018). Library consortia's activities are varied as well. A 2006-2007 American Library Association (ALA) national survey on consortia revealed that the most common services and activities within consortia are communication, resource sharing, professional development, consulting and technical assistance and cooperative purchasing. Other less common activities include automation, networking and other technology services, etc. (Davis, 2007). A more recent examination by Rosa and Storey (2016) found that American libraries all face the challenges of funding, evolving role of the library, and the changing nature of scholarly communication. They are ""more connected than they have ever been in the history of library and information science"" (p85). Resource sharing, cooperative acquisitions and e-content licensing, as well as shared online catalog are among the most used services in library consortia.
Next Generation ILSs
Library automation has experienced several phases of development since its beginning in 1960s (Borgman, 1997). Earliest library systems were created to provide a specific function or resolve a particular issue, such as circulating materials or creating catalog cards. The following efforts are to integrate these separate pieces of software into one system, the Integrated Library System (ILS). A standard traditional ILS normally contains the modules of cataloging, circulation, serials management and open public access catalog (OPAC). This type of systems were designed for print resources management. With the advances of information technology, library automation products based on server/client emerged on the market around mid-1990s. In the meantime, electronic resources started to grow. However the main functional modules of ILSs remain unchanged. A number of add-on library systems were developed to address various library needs beyond what a traditional ILS can meet, such as link resolvers, electronic resource management systems, digital asset management, institutional repositories and discovery interfaces. (Breeding, 2013;Liu, 2015) The year of 2011 witnesses the start of a new cycle of library automation -the emerging of next generation ILS. Meanwhile, Breeding (2011b) proposed a concept of Library Services Platform (LSP) to differentiate the next generation ILS from the traditional ILS. In this paper, we use next generation ILSs for the new emerging systems to emphasize its integration feature and avoid any confusion it may cause because both LSPs and next generation ILSs have been used often in library literature and libraries. The next generation ILSs are able to manage all forms of library collections including print, electronic and digital resources. They can introduce pertinent workflows according to the type of resources (print or electronic), the call of services (local or remote) and the acquisitions models (purchasing or licensing) (Breeding, 2011b;Liu, 2015). The new systems shall also take advantage of cloud computing and other latest technologies and architectures (Grant, 2012).
Alma from Ex Libris and WorldShare Management Services (WMS) from OCLC are the two earliest products that have been developed from ground up in this area (Wilson, 2012). Since their launch in 2012, both products have been maturing. By the end of 2016, Alma has been adopted in over 800 libraries and WMS in over 400 libraries. Other two products active in current market are Sierra by Innovative Interfaces and FOLIO, an open source initiative supported by EBSCO (Breeding, 2016;Breeding, 2017b). Sierra took a different approach from Alma and WMS. It reused many Millennium system functions rather than creating original innovations (Grant, 2012). Sierra has been installed in over 600 libraries since it was introduced in 2011. FOLIO entered the market in 2016 and is currently under development (Breeding, 2017b).
Academic Library Consortia and Library Automation
""The growth of information technology has increased the importance of consortia"" (Kopp, 1998, p.7). Library automation started around 1960s when most of the early academic library consortia were formed (Borgman, 1997;Kopp, 1998). Libraries were motivated to collaboratively develop systems and share automation techniques to computerize manual, labor-intensive operations to improve overall efficiency. Automating large-scale technical processing was the primary concern of large consortia at the time (Bostick, 2001;Kopp, 1998).
In 1970s and 1980s, computer hardware became less expensive and the automated library systems emerged. ILS was born combining automated back room operations (Borgman, 1997). It became unnecessary for libraries to cooperate to acquire automated systems. Libraries tended to focus on the development and implementation of their local ILSs rather than consortial activities. As a result, the growth of consortia slowed down to some degree (Kopp, 1998).
By the late 1980s to 1990s, most libraries had ""achieved certain levels of local systems and networking sophistication"" (Kopp, 1998, p.14). Combining with fiscal, political and other factors, academic library consortia re-flourished with an emphasis on acquiring and providing access to electronic resources via the Internet as well as sharing physical resources (Kopp, 1998;Potter, 1997).
From the 1990s into 2000s, many libraries had their own separate standalone ILSs in house. However, the development of cloud computing pushed libraries to reconsider the remotely hosted library systems supported by vendors and consortia (Machovec, 2014). Next generation ILSs were introduced. In his 2011 automation market report, Breeding (2011a) predicted that more libraries would consider adopting the cloud based, multi-tenant automation products as well as participating in shared automation systems in consortia to save cost. Libraries have become ""willing to look at much more profound and fundamentally ground-breaking collaborations"" and demand automation vendors to offer collaborative functionality to support library success (Horton, 2012a, p.130). In 2011, Orbis Cascade Alliance (OCA) decided to create a ""truly shared integrated library system"" for all of its 37 member institutions (Horton, 2012b, para. 3.). This would allow for deeper collaboration among its members, including unified collections and shared technical services. According to an OCLC survey (2013), in response to what the most valuable aspect of joining a consortium is, 12% of U.S. library consortia leaders chose a shared ILS, which is on par with e-content purchasing and third to professional networking (30%) and cost savings (23%).
In his regular column in the Collaborative Librarianship, Ayre (2015) illustrated all compelling features for library consortia to collaborate deeper by adopting shared next generation ILSs. It was stated that all sharing activities and services would be streamlined and simplified, including user access and staff workflow. A shared system would save individual member libraries not only on hardware, software and licenses costs but also personnel for system administration, cataloging, collection development, and even selections and acquisitions.
Would it be a new direction for academic libraries to collaboratively select and manage shared next generation ILSs? This article aims to analyze the trends, impacts, opportunities and challenges for academic libraries in the shared next generation ILSs.
LITERATURE REVIEW
A great deal of research articles and presentations have addressed the topic of next generation ILSs and consortia in library literature. Although the concept of next generation ILSs is still relatively new in the profession.
OCA is a pioneer of library consortia in the adoption of shared next generation ILSs. It has served as a model for many other academic library consortia (Helmer, et al., 2012).
From system selection and migration to its impact on the library operations and various functional areas, librarians and other researchers from OCA have contributed a number of papers sharing their experiences and insights of a shared next-generation ILS in a large academic library consortium. Cornish, Jost and Arch (2013) detailed the process for selecting a shared next generation ILS for all 37 OCA members, including the foundational steps, Request for Information (RFI), teams and processes of Request for Proposal (RFP), and negotiation with suppliers. Steve Shadle at the University of Washington Libraries presented the migration experience to the next generation ILS and a single shared catalog in OCA, including the motivation for the consortial migration, the implementation process and lessons they have learned (Shadle and Davis, 2016). Stewart and Morrison (2016) from the same institution further examined the consortial migration and its impact on acquisitions workflows and collection building in the shared system. Shared ILS migration is also investigated from a technical services perspective by the staff from another OCA library (Zhu and Spidal, 2015).
The consortially shared ILS has also changed the library operations in OCA (McKiel and Dooley, 2014). Librarians from pioneering libraries in the OCA looked at the acquisitions policies and workflows in the new system (Spring, Drake and Romaine, 2014). The challenges and opportunities for collaboration on acquisitions have also been discussed (Spring et al., 2015). Romaine and Wang (2017) analyzed the serials and electronic resources management (ERM) functionality and workflows in a shared ILS. The discovery end accompanied with a next generation ILS and its impact on library database usage is included in the literature as well (Evelhoch, 2016). In addition, Fu (2017) investigated the impact of next generation ILSs on the U.S. library consortia.
Literature sees newer articles coming from other consortia who have selected or are interested in a shared next generation ILS. Deng, Sotelo and Culbertson (2018) at the University of California, San Diego, conducted literature review and a survey on cataloging consortial collections in preparation for the upcoming migration to the next generation ILS in the consortium. Five trends have been identified, including the outlook that local library catalog is not dead yet, as well as several approaches for consortial cataloging. Cote and Ostergaard (2017) from the Treasure State Academic and Information Services (TRAILS) Consortium examined the role of electronic resources librarians in the process of consortial migration to next generation ILSs. They concluded that the North American Serials Interest Group (NASIG)'s Core Competencies for electronic resources librarians ""provide a framework from which to approach"" the next generation ILS implementation (p. 228).
Consortia from other regions or countries, such as Hong Kong, Canada and South Africa, are also interested in this topic. Eight universities in Hong Kong in the Joint University Librarians Advisory Committee (JULAC) started a new adventure in 2013 aiming to collaborate on a shared next generation ILS. After several years of planning, consultation and RFP process, JULAC selected Alma and Primo in 2016 and went live with the shared system in July 2017. Major challenges they encountered include merging bibliographic records, user account authentication, user-initiated borrowing, data migration, and multilingual authority control, etc. Opportunities are also presented to participating libraries, such as shared cataloging, shared collection development, shared workflow, expertise and training (Chan and Lam, 2016;Lam, 2017).
In 2016 Library Technology Conference, Anika Ervin-Ward and Amy Greenberg (2016a) presented the Ontario Council of University Libraries (OCUL) Collaborative Futures (CF) project. The OCUL CF project aims to collaboratively adopt a shared next generation ILS. The OCUL Case Study section in this article will detail its goals, approaches and status along with the discussion on challenges and opportunities of this provincial project in Canada.
In South Africa, Mfengu (2014) interviewed senior library management teams in four institutions of Cape Library Consortium and found that these institutions were willing to adopt the next generation ILS in the next five years. They were in a process of preparing for this move in terms of staff and infrastructure change. The member institutions would like to take advantage of consortial approach and still function individually. Machovec (2014) listed the following challenges facing consortial solutions of next generation ILSs: selecting a system, determining costs, defining levels of collaboration, security, scalability and performance of the solution, and the integration with other library applications. Although Alma is the dominating product selected by home institutions of the authors of the related literature, Machovec (2014) did name a couple of examples other than Alma, such as the Private Academic Libraries of Indiana (PALNI) who migrated to WMS and MOBIUS consortium who have upgraded to Sierra.
Rarely does research in the literature target the trend of academic library consortial adoption of the next generation ILSs. More investigations from various perspectives and environments would provide further expositions on the impacts, challenges and opportunities of such a substantial joint adventure for academic libraries around the world.
METHODS
To identify the trend for academic libraries to adopt a shared ILS within a consortium, the authors of this article collected and analyzed the number of academic library consortia that have moved to a shared next generation ILS in the past few years. Marshall Breeding's annual product reports are a good source for the adoption number of next generation ILSs. The library automation statistics tracked on the Library Technology Guides (https://librarytechnology.org/) are another source of data for this article.
However, these sources do not provide separate information on academic consortia. The press releases on individual products have been collected and reviewed for the analysis. In the next generation ILS market, only Alma, WMS and Sierra have sufficient installations in libraries to be meaningful for this study (Breeding, 2017a). Although WMS has gained sizable market in academic libraries, it ""has had few selections by large academic libraries or consortia"" (Breeding, 2017b, 2 nd para. under Academic Libraries/OCLC). There is little information on academic libraries adoption of WMS either on library literature or on the Internet. It lacks literature on Sierra as well. The Press Center of Innovative Interfaces (https://www.iii.com/press-center/) contains news releases on the selection and migration of Sierra by libraries but the data are available only from 2016. It appears the number of press releases is not complete for all library adoptions of Sierra.
Good news is that it looks like all press releases on Alma adoption since 2011 are preserved and accessible via the News and Events on the Ex Libris website (http://www.exlibrisgroup.com/press-releases/). The number of total adoptions of Alma on the press releases also agrees with what has been presented in Marshall Breeding's reports and statistics (Breeding, 2018a;Breeding, 2018c). In addition, the number matches what is described in the internal document of the Ex Libris' response to OCUL CF RFP for a next generation ILS in January 2018 (one of the authors of this article sit on the OCUL CF RFP Requirements and Evaluation Working Group). Therefore, data collected via the press releases on Alma adoption on the Ex Libris website are quite reliable.
This article reviews all available data on the adoption of Sierra (from January 5, 2016 to April 9, 2018) and Alma (from January 6, 2011 to April 3, 2018) on the Internet. The analysis mainly relies on Alma's adoption data during 2011-2018 with a particular focus on the consortial adoption. All adoption numbers from the websites of Ex Libris, Innovative Interfaces, and Library Technology Guides are collected and verified during April 1-15, 2018.
In addition to the analysis on the adoption number, two cases, OCA and OCUL, under different stages of consortial adoption of a next generation ILS are studied to provide in-depth analysis on the impact of shared next generation ILSs on consortia and their members as well as the challenges and opportunities to them. OCA is the pioneer in this area in the world, who have gone live with Alma for a couple of year; whereas OCUL is among the first consortia in Canada aiming for a completely shared next generation ILS, and is currently selecting a shared system.
ADOPTION OF NEXT GENERATION ILSS IN ACADEMIC LIBRARY CONSORTIA

Data from Library Technology Guides by Marshall Breeding
Table 1 lists the number of consortia respondents and the total number of all respondents to the annual International Survey of Library Automation in 2012-2017. The respondents come from all types of libraries primarily in English speaking countries (Breeding, 2018d).
The data include a variety of library automation products, such as traditional ILSs and next generation ILSs.
Some comments in the 2017 survey state they are part of a consortial shared system, but responded as individual libraries (Breeding, 2018b).
Table 1 shows that the number of consortia respondents from 2012 to 2017. Although it appears that during 2012-2017, the number of consortia respondents to the annual library automation perceptions survey goes up gradually (see Figure 1), the percentage of consortia respondents among the total number of respondents (both consortial and individual respondents) does not support such trend (see Figure 2).   
Year

Press Releases for Alma Adoption
The Press Releases from News & Events on the website of Ex Libris (http://www.exlibrisgroup.com/press-releases/) were examined for the product Alma since its beginning, January 6, 2011 when the first announcement was released on Alma.   The press releases on the year of 2018 have also been reviewed carefully. Till April 3, 2018, five individual institutions and three consortia have selected Alma in the year of 2018. The three consortia include two Japanese universities, and seven university members of Michigan Shared System Alliance, and 64 campuses in the State University of New York. There are 78 academic libraries have selected Alma in total in 2018, and among them 73 are part of a consortium who selected the product together. That means among Alma adopters, about 94% academic libraries have joined a consortium for a shared next generation ILS in 2018.
CASE STUDIES
Case Study 1 -OCA: Experience, Impacts, Opportunities and Challenges OCA is a library consortium of 39 academic libraries in Oregon, Washington, and Idaho, serving faculty and the equivalent of more than 280,000 full-time students. From January 2013 to January 2015, the then 37 OCA libraries completed migration from three different locally hosted ILSs and four different discovery platforms to a shared next generation ILS, 
Number of Consortia
Alma, and a single discovery platform Primo. The Alliance and its member libraries took a multi-year process for exploring systems options, creating policies and standards for data cleaning, planning and organizing the migration, and creating collaborative programs and teams after migration. The Alliance created numerous working groups to perform collaborative tasks at each stage of the project.
Through analyzing the OCA programs, documents, reports published on the OCA websites, conference presentations authored by the OCA members and delivered at the Ex Libris Users of North America conferences, in-person interview, and journal articles authored and published by librarians and professionals of the OCA member libraries, we find, particularly, from a member institution's perspective, the shared next generation ILS has made significant impacts and generated numerous opportunities and challenges on almost all aspects, particularly on the following areas of the OCA and its member institutions:
Resource Sharing
According to the OCA Alliance-Wide Summit Borrowing 2017 and Summit Lending 2017, there were a total of 312,874 borrowing requests and a total of 261,372 lending requests received among its 39 institutions in 2017. The fill rate was 80% and 93% respectively.
Our study shows that the new shared system has greatly improved users' access to information through resources sharing in the OCA libraries. Compared to the previous OCA's resource sharing system called Summit, the new Summit seamlessly integrated consortial borrowing and lending systems and interlibrary loan system with the shared ILS Alma and front end system Primo. The new Summit allows patrons to easily search and request library materials owned by consortial members or other libraries outside the consortium through a single Primo user interface. Every member library can follow the same procedures and policies to achieve efficiency and predictability.
During an in-person interview, Erin Bledsoe, a senior circulation staff at Central Washington University, who participated in the OCA Alliance Resource Sharing Implementation Team, responded that the new shared ILS brought significant benefits to both patron and library staff. She emphasized that the new shared ILS ""allows the user to access all of the library and consortial holdings; physical, electronic and digital, by searching in one search box."" Patrons ""no longer have to interface with third party vendor (i.e. WorldCat)."" The new system provides ""real-time availability, not requestability."" The shared best practices also ""allow for similar experiences throughout consortium."" Erin Bledsoe added that ""detailed audit trail can help staff troubleshoot problems. General messages and notes can be used to indicate damage, multiple parts."" She recognized that shared creation of documentation and best practices are helpful for staff training and professional development.
Discovery and User Experience
Our study shows the new shared next generation ILS provides a single high-quality, webbased discovery and delivery platform for all consortium member institutions. It enables discovery of resources, regardless of format or resource type, in local or consortial collections and beyond. It enables member institutions to customize the search experience by controlling for preferred formats and locations and implementing individual institution's needs and brand components. The new Primo interface has become a central portal for access to unique local information resources, including digital collections from member libraries. It provides users with a web-based portal for assistance in conducting searches on the internet, evaluating the quality of information resources, learning how to use various databases and linking them to library resources throughout the consortium. It also provides users with the advantages of a union catalog, such as consistent query interpretation across multiple libraries with quick response time across a large number of library records. It supports efficient computerized library services, including up to the minute information about the availability of library materials, circulation information, journal collection status, and computerized checkout. It offers access to an array of online user-initiated services, such as the ability to review materials checked-out, renew books, and request books from other libraries.
However, Zebulin Evelhoch (2016), an e-resource librarian from Central Washington University, one OCA member, through his analysis, noted that ""the first year postmigration ( 2015) compared to the two years pre-migration (2013-2014) saw a decline in web page views of database (A-Z) web pages, journal full-text article requests, and database record views and result clicks. The implementation of Primo thus had a noticeable negative impact on both direct database access and overall electronic resource usage during the first year post-migration"" (p16). However, for the second year and third year after golive, the access numbers were back to normal. Our study suggests that patrons need to be educated and trained to be familiar with a new discovery system during the transition of migration from a traditional OPAC.
Shared Content
Our study finds that the new shared ILS allows the consortium to continually assesse, manage, and develop initiatives that broaden access by providing cost-effective sharing, licensing, and description of such content. 
Collaborative Technical Services
Our study finds that OCA's Alma implementation differs from a stand-alone institution's version. According to the OCA Strategic Agenda, the OCA wanted to ""manage and build the combined collections of members as one collection""; however, the shared bibliographic database environment still allows OCA member institutions to retain some local control and to provide a place for local order and holdings records. In order to accomplish this, Ex Libris created a three-layer system. The first layer, called the Institution Zone (IZ), houses local institutional holdings, inventory, and order records. Each OCA member institution has its own IZ. The second layer, called the Network Zone (NZ), which houses the bibliographic records of OCA's member libraries, separate but linked to the local/institutional repository (IZ) for each OCA member. While the third layer, called Community Zone (CZ), composed of e-resource records, the Alma Knowledge Base (KB), is available to all Alma users, not only for OCA members. The three-layer system made it possible and easier for OCA member institutions to work closely on collaborative technical services such as the ebook, Chinese, Japanese, and Arabic cataloging pilots. These technical services were difficult when more than 30 local ILSs were in use and workflows varied across the consortium. Thus, the shared next generation ILS opened up new opportunities and made collaboration in acquisitions, cataloging, collection development, circulation, systems, and other areas easier to achieve. It allows libraries to streamline staff operations and realize cost savings through sharing standardized bibliographic and authority records.
As a result of this type of collaboration, since the implementation of the shared ILS, the consortium members have worked together and developed a number of bibliographic records polices and shared ILS operational policies, best practices, procedures & workflows, normalization rules, NZ account configuration & procedures and guidelines. The consortium member libraries also share a single normalization rules for converting source records in Alma and publishing to Primo.
Systems
The maintenance and upgrades of the OCA shared ILS are centrally managed by the vendor. The cloud shared ILS lessens the necessity for each library to maintain the full complement of experts and hardware to operate their own. Our study shows that systems staff at OCA member libraries have more time to develop local applications and support customizations. The shared ILS enables sharing customization and distributed testing of new release, central monitoring, deploying and publishing. The systems staff of the OCA member institutions also collaborated on user roles management, systems authentication, systems configuration, Alma/Primo API development, primo new UI customization, resource sharing configuration, integration with other systems, etc. It enables the OCA Systems Program Manager and the Alliance Systems Team to provide centralized library automation support and services to various types of libraries in the OCA.
Unique & Local Digital Content
Each OCA member institution has its own archival collection, institutional repository and other unique digital repositories. The new shared ILS enables the alliance to consider aggregating its member institutions' local and unique digital content. In order to achieve this goal, the OCA formed a few of working groups on archival collection, digital content metadata standards, preservation and aggregation. The OCA Council approved an AADC/DPLA proposal proposed by the Content Creation & dissemination Team in February 2016. AADC stands for Aggregate Alliance Digital Content and DPLA stands for the Digital Public Library of America, a national-level portal for digital content from libraries and other cultural heritage institutions with value-added search and browse features. The AADC/DPLA project allows all Alliance members to share digital objects in Primo and DPLA. The project adheres to national standards for library automation and digitization to ensure compatibility and transferability of records and links members to regional, state, and national library networks which increase the OCA member institutions' visibility and brand awareness by providing open access to their unique and local content via the DPLA.
Case Study 2 -OCUL CF Project
OCUL is an academic library consortium of 21 university libraries in Ontario, the largest province in Canada. It has been existing for over 50 years for the collaboration and cooperation among Ontarian institutions to enhance services to students, faculty and researchers in Ontario and beyond. The collaboration activities in OCUL include group purchasing, shared digital information infrastructure, collaborative planning and professional development, etc. (Ervin-Ward and Greenberg, 2016a;OCUL, 2018a;OCUL, 2018b) One example of shared systems is the SFX link resolver. Academic libraries in the OCUL consortium implemented a shared SFX in 2004 (Cheung, Thomas and Patrick, 2010). Each institution has its own instance while Scholars Portal maintains the central instance. Institutions relied on SFX for e-resource managing and linking for many years since then. Other innovation or collaboration based on SFX have been developed, such as the integration of SFX with Evergreen open source ILS for the unified view of print and electronic serials created by a local member, the University of Windsor, and the OCUL Usage Rights Database implemented consortially for institutions to display licensing terms on various databases to users in library catalog, journal A-Z list or other search interfaces (Liu and Zheng, 2011;Scholars Portal, 2017).
With the emergence of discovery layers and next generation ILSs, libraries in OCUL started to adopt other link resolvers or knowledge bases. It became a big burden for libraries to maintain multiple knowledge bases and link resolvers. Libraries began questioning the future of SFX and some other services offered by the consortium (Ervin-Ward and Greenberg, 2016a). From 2012, the OCUL Technical Advisory Group initiated discussions on cloud computing and web scale library systems across the province. A Unified Resources Management (URM) Summit was held in Toronto in February 2013. As a result, the OCUL Collaborative Approaches Task Force was established to identify potential opportunities by the new type of systems (Ervin-Ward and Greenburg, 2016b). In the meantime, one OCUL member, the University of Windsor contracted with Alma as an early adopter. University of Windsor is the first university in Ontario that selected a next generation ILS. Alma replaced several separate systems at the University, including Evergreen open source ILS, SFX and Syrup, a homegrown course reserve system (Liu, 2015).
With the efforts of the OCUL Collaborative Approaches Task Force and consultations with OCUL members, the ",6564,7497
10.1108/ilds-12-2013-0039,,,False,Silvana Mangiaracina,Assessing the effectiveness of a national resource sharing system,"
Purpose -The purpose of this paper is to report on the surveys, carried out during 2011 and 2013, regarding the functionality of and possible improvements to Italy's nationwide resource sharing service NILDE (Network for Inter-Library Document Exchange). Design/methodology/approach -The methodology comprises both an analysis of quantitative data about ILL transactions initiated during a 2011 study and a qualitative assessment of the system based on information obtained from the surveys and a SWOT analysis. This proved to be an effective methodology, and a new survey was launched in 2013 to verify whether the choices made and the projects undertaken were in line with user expectations. Findings -The results turned out to be particularly interesting and a source of hints for planning future improvements. Originality/value -Italian studies for assessing user satisfaction of library services, based on user surveys, often relate to a single library or a single University. The NILDE survey was delivered nationwide to all the registered users of NILDE. This is the most extensive survey for the number of libraries and the various types of end users involved.
",2014-08-12,Interlending & Document Supply,Emerald,"Background
The NILDE (Network for Inter-Library Document Exchange) system was initially developed at the Italian National Research Council Bologna Research Area Library with the aim of improving ILL services and promoting cooperation among Italian libraries (Mangiaracina, 2002;Mangiaracina et al., 2008). At present, 830 libraries belong to the NILDE network, of which 77 per cent are university libraries, 9 per cent health research institutes and hospitals, 8 per cent public research institutions and 6 per cent other public and not-for-profit organizations.
In 2005, an end user interface was added which is presently used by more than 18,000 end users registered at their libraries. The direct use of NILDE has obvious advantages for all users (Cocever and Chiandoni, 2008). Advantages for the libraries are:
• the ability to receive completed request forms in the correct format through a single channel;
• the ability to update users automatically on the status of their requests in real time;
• the availability of detailed statistics on user transactions and on user profiles; and
• access control to the service based on institutional authentication (for the libraries which form part of the Italian IDEM Federation).
Advantages for the users are:
• interoperability with bibliographic databases which facilitates the automatic transfer of bibliographic data into the NILDE request form;
• a personal workspace which incorporates an embedded reference manager system (added in 2011);
• the availability of an automatic communication flow, which provides real-time updates on the status of transactions; and
• the use of institutional credentials to access the services to avoid multiple authentication (for the institutions belonging to the IDEM federation).
The current NILDE 4.0 software, released in 2011, introduced new features and innovations to the user interface to make it a more user-friendly tool for ILL and scholarly activities (Mangiaracina and Tugnoli, 2012). NILDE's present features comprise:
• a complete suite of ILL manager software modules, including statistics/history to monitor ILL performance indicators such as fill-rate and turnaround time;
• a secure electronic document transmission module (PDF files are ""digital hard-copied"", that is, automatically transformed into graphic files, to comply with ILL clauses in electronic licenses that usually do not allow the sending of the publisher's original PDF file, but only of a printed copy);
• a dedicated end user module to manage personal bibliographical references, allowing users to easily import, organize and export references, and to initiate an ILL request from any bibliographic electronic resource based on the Open-URL standard;
• federated end user authentication based on the Shibboleth framework; and
• multilingual support, including user interfaces in Italian, English, Spanish, French and Greek.
The NILDE end users module, initially conceived as a basic tool simply for managing user requests to the library ILL service, has evolved into a reference manager which allows the user to organize their own bibliography as well as to formulate a request to the library's ILL service if the document is not locally accessible. Bibliographic references can be inserted either manually by the end user or automatically from any OpenURL-compliant database. The most important feature added to the end user module is the facility to manage their entire bibliography by labelling, sorting and exporting references as well as inserting, modifying and deleting them.
The user is also allowed to send DD/ILL requests during new insertions or to request any item which is already in their bibliography. In this case, NILDE provides functions to track and cancel requests, send email notifications about delivery status, inform users of payment options and maintain a history of the user's DD/ILL requests.
End user institutional federated authentication based on the Shibboleth framework is also supported, which allows a user to register only once at their home organization (i.e. their university), receive a username and password and then use these to access other resources.
Early efforts to assess NILDE effectiveness
NILDE's success is illustrated by the steadily growing number of NILDE users, both libraries and end users, as shown in Table I. However, in 2011, less than one-third of networked libraries accepted requests directly from their end users; such a low use of the user-initiated mode seemed to place doubt upon its real usefulness.
A ""listening"" approach is essential for building a service to meet stakeholder needs, and with this aim in mind, after the launch of NILDE 4, two Web surveys were carried out in 2011, one with librarians and one with end users. The surveys were designed to help understand user behaviours (Chiandoni et al., 2013), in particular the low use of the direct user mode of NILDE: Was it caused by user dissatisfaction or more by insufficient promotion by the library staff?
The two most important findings of the 2011 surveys were (Chiandoni et al., 2013): 1 User viewpoint:
• appreciation for the new interface;
• low use of the reference manager embedded in the new release (64 per cent state they do not use specific reference managers, it is doubtful whether this tool is really useful; maybe it would be more useful to focus on interoperability with existing software);
• strong desire to access the publisher's original full-text file; and
• expectation for ever higher disintermediation (users appreciate a Web service connected to the main information sources, i.e. databases, which is user-friendly and supplies a help guide to make independent problem solving possible); this mirrors a widespread attitude among Web users, who are accustomed to independent searches for specific needs and likely to use tools where no direct assistance is provided. 2 Staff viewpoint:
• appreciation for the new interface;
• the need for more and more usable and simplified procedures which empower users to tackle and solve problems. This is the case with small organizations where staff often perform multiple duties; and
• difficulties in communicating and promoting services and understaffing are the main obstacles that prevent widespread promotion of the direct user module. Some comments posted on the blog highlight that:
• email and direct contact are preferred;
• email is considered to be a more convenient channel;
• user education is not organized;
• some categories of users, such as academics, find it difficult to understand the advantages; and
• end users are suspicious.
It seems, however, less a matter of user mistrust and exertion than a matter of staff resistance to change, as they do not want to give up work habits that are considered to be more viable and agreeable.
After the surveys held in 2011, there were initiatives aimed at promoting the direct use of NILDE. Four targeted staff training courses were organized at a national level. Many other courses were organized in the main Italian universities. The goal was to explain the functions and advantages of the NILDE-Users module.
A conference was held in Bari in 2012 to show librarians the analysis of the data collected during the 2011 surveys and to discuss the strengths and weaknesses of the NILDE network. In this instance, particular attention was drawn to the issue of lack of communication and motivation of librarians and on their resistance to full use of the software[1].
The impact of all these advocating activities can be seen in Table I, which shows that libraries that implemented user-initiated requesting through NILDE in 2012 had expanded their user base one year later and the number of libraries with 10 or more end users increased by 32 per cent. In addition, the total number of end users ballooned by 62 per cent in that same year. Likewise, Figure 1 illustrates the one-year increase in NILDE end users per library.
There is still, however, a big share (more than two-third) of networked libraries which do not use the end user module. Analysis of the quantitative data from ILL transactions proves that there is a significant difference between the libraries that enable users to submit requests directly and libraries that do not, and shows that the average number of ILL requests more than doubles when users have direct access to the software, as can be seen in Table II. This phenomenon may have multiple explanations. It could be that libraries which have not chosen NILDE to communicate with their users may employ other channels to forward ILL requests (or may use email to a great extent, both to receive user requests and to forward them to other libraries). On the other hand, it may be that users who have access to NILDE's end user interface and are satisfied with it are encouraged to exploit it, and therefore, the direct use of NILDE promotes an increase in library ILL requests (and consequently a greater use of library services). This suggests the end user interface is helping libraries meet previously unmet user needs.
Although libraries which implement the direct use mode may seem more virtuous than the others, much more can still be done in terms of service quality improvement: in fact, only one-third (32, 385) of user-initiated ILL requests have bibliographic metadata loaded from a database, which means that the remaining two-third (69, 826) of bibliographic references were manually entered by the end users. Moreover, 91 per cent of the references entered through the OpenURL protocol come from PubMed, while the connection between NILDE and other bibliographic databases, even the most popular ones such as Web of Science or Scopus, is of little consequence, as shown in Table III.
The quantitative data highlight the effectiveness of staff training initiatives over the past year, but they also emphasize the overall underuse of the software by end users. We wonder, for instance, why users prefer to enter manually the bibliographic records of their requests, given that they could load the metadata directly from bibliographic databases, or else why the embedded reference manager is not perceived as a value-added service. To answer these and other questions, another survey targeting end users only was carried out in 2013.
Assessing user satisfaction

Survey methodology
A focus on library-user satisfaction in Italy was introduced at the AIB (Associazione Italiana Bibliotecari) annual congress in Villasimius in 1984 (Santocchini, 2010). The relationship between library and perceived service quality by the user has been   (2006). Oliva (2012) discusses customer satisfaction in Italian academic libraries, specifically a 2012 survey of user satisfaction at the Milano-Bicocca University library; qualitative and quantitative results are then presented and discussed. Italian studies for assessing user satisfaction of library services, however, often relate to a single library or university. However, a careful approach is needed when conducting such surveys on a nationwide scale, as the NILDE network represents a heterogeneous environment comprising public, academic and healthcare sectors, and scientific research institution libraries, whose collections span a broad range of subject areas and whose constituencies are widely divergent with respect to numbers and types of users.
The 2013 survey was delivered nationwide to all the 17,792 registered users of NILDE; this is the most extensive survey for the number of libraries and the various types of end users involved.
The data were collected via the Web, and the questions were hosted on a special WordPress blog. A blog enables users to add free comments instead of merely answering the closed-ended questions of the survey, and this is why it was chosen.
All comments, and especially the critical remarks, were particularly valuable inasmuch as they provided useful hints on how to improve the service and started a discussion among stakeholders, i.e. librarians and end users.
The two-part survey was created as a Google form (available within Google Drive).
The first set of questions, eight in all, was aimed at gathering both quantitative and qualitative feedback from end users on the NILDE service and at finding out how they had come to know about the NILDE service. The second set of questions, six in all, was designed to know end users better (age, gender, role, etc.).
The initiative was publicized through a message on the NILDE home page, visible to everyone who accessed the Web site while the survey was open. During that period (27 May 2013-2019 July 2013), the libraries in the NILDE network were asked twice to remind their users of the survey through the communication channels normally used for similar purposes.
The outcome of these reminders was successful: 1,178 people, representing 17 per cent of active users, completed the survey. Active users were defined as users who had requested at least one article through the NILDE service between 1 January 2013 and 30 June 2013; their total number amounted to 6,987. The percentage of responses was weighed against active users, on the assumption that they were more loyal and sensitive to service improvement than the total number of users, who amounted to 17,792.
When the survey was closed, the final data, available in real time and also displayed in chart form, were linked to the NILDE home page [2].
Survey results and discussion
The data gathered by the 2013 survey make it possible to build a profile of the users who are loyal to NILDE. The term ""loyal users"" is applicable because the respondents to the survey were the most intensive users: 70 per cent requested more than four articles in the first six months of 2013, 38 per cent more than ten. This is even more apparent when compared to the figures extracted from the NILDE database (see Table IV).
Quantitative data are important, but it is also important to point out that these users perceive the service as a useful tool for their work. The question ""To what extent has NILDE contributed to your research in 2012/searches?"" scored an average of 3.64 on an assessment scale ranging from a minimum of 1 to a maximum of 5. The typical user is a female researcher in science and technology, between 25 and 34 years of age, working at an Italian university located in the Centre/ North of the country. Analysis of the sample taking part in the survey highlights another very interesting feature: libraries play a major role in promoting and advocating. As mentioned above, while the survey was open, the organizers urged the libraries several times to prompt their users, and this resulted in an increased response rate.
These initiatives had in some cases greater impact; this is highlighted when comparing the data related to the participants to the total number of active users.
Table V shows that the percentage of the survey participants affiliated with public research institutions is much higher than what it was compared to the total number of active users on 30 June 2013. These libraries were clearly more effective in promoting the survey.
Generally speaking, communication turns out to be of paramount importance for libraries, which perform a key function in informing users about services. In this respect, it is significant that 49 per cent of the end users stated that they had been informed about NILDE by the library staff and 24 per cent through their website. If on the one hand the survey represents the most loyal segment of users, on the other hand, to some extent, it also represents the subset of networked libraries that mostly perceive NILDE as an important service for their target community. When profiling loyal users, it is interesting to note that responses were split almost equally between genders: 51 per cent female respondents and 49 per cent male respondents. It is significant that among the participants, 69 per cent of the women were between 25 and 44 years of age, while men within the same age range amounted to 56 per cent. On the contrary, women Ͼ 45 years of age were 29 per cent while men Ͼ 45 years of age were 41 per cent. The users of NILDE consequently mirror the general trends; women are increasingly present in the Italian scientific research landscape, especially with ages between 25 and 44, i.e. 62 per cent of the participants in the survey.
As mentioned before, the survey was aimed at getting to know users better, and consequently, some questions were asked about their behaviour when seeking papers.
A total of 67 per cent chose mainly NILDE and 23 per cent of them NILDE exclusively. It is interesting also to notice that 81 per cent state that they always -or almost always -surf the net to check whether papers are freely available. This behaviour is obviously encouraged by the outcomes of Web searches; very many digital documents are freely downloadable. The consequence of increasing Open Access to research outputs is, ironically, a prospective decrease in interlibrary loan services. In the information society, the transition from a former economy based on shortage to a net economy based on plenty is questioning current paradigms and economic models. Libraries also have to readjust their objectives and to focus on the documents for which they may be the sole provider and which are difficult to find. Some comments on the blog which hosted the survey stress one of the main strengths of NILDE, i.e. the possibility to find papers which are old and therefore not in electronic journals, and not available elsewhere. The interoperability of NILDE with the main national online catalogues (see, for example, Mangiaracina and Tugnoli, 2012) makes it easier for staff to search for documents and check their availability. The effectiveness of the service is therefore dependent on the comprehensiveness of these sources, and libraries have a prime role inasmuch as they update and maintain their catalogues.
As for other behaviours, purchasing papers is the least chosen option (84 per cent state they never do it); next, using the credentials of other institutions (86 per cent never do it or do it rarely).
The starting point for improving a service effectively is to ask users first. Therefore, there were questions about the most important features and functionalities of a document delivery service; first, generally speaking and then focusing on the specific characteristics of NILDE.
A four-point grade scale was used to assess first a document delivery service in general terms; scores ranked from 1 (very low importance) to 4 (very high importance); and the average responses are shown in Figure 2.
The most important features are, in order of importance, the online access to the service, availability of electronic papers, response time and integration with bibliographic databases. Users consequently expect a service which enables them to obtain documents quickly through a single tool linked to bibliographic databases, where the descriptive metadata of the requests are extracted automatically, and which make it possible to track the transactions until the articles are delivered electronically.
NILDE is, in fact, a Web service that performs all the abovementioned functions, while a rapid response is ensured by the delivering libraries spontaneously maintaining a high standard of performance. The availability of electronic documents directly to the users is contrary to the present copyright law; 55 per cent of the respondents declare they know about it, but, if that were true, there would not be such an urgent request of electronic papers, which was echoed both in responses and in blog comments. Users often complain about having to receive the paper document, suddenly plunging back into the space-time constraints of physical reality, and they perceive it as if it were the library's fault.
Researchers need to perform their activities anywhere and at any time, and they are connected through the Web to research laboratories disseminated over vast geographic areas; it is, therefore, understandable that such constraints are not considered to be acceptable. Here too, librarians have the task of informing and making users aware of the constraints of copyright law, which often does not protect the rights of authors who need to re-use research outputs for their own work, but protects the economic interests of the commercial publishers which control the scientific literature market.
The response to the features of NILDE which need to be improved back up what has been said above: the starting point for requests must be the tool used for bibliographic searches (25 per cent of the responses); one concern is the need to exploit the Web to its full potential through applications for mobile devices (15 per cent of the responses); and one of the main concerns is interface usability (11 per cent of the responses). Software underuse is a particularly evident problem regarding the interoperability with bibliographic databases, which is already there but which is patently unknown to users. It will be useful to undertake initiatives aimed at exploring the causes for this knowledge gap soon, beginning with librarians themselves who have the task of imparting knowledge useful for the optimal use of bibliographic services.
Strengths, weaknesses, opportunities and threats analysis
Analysis of the quantitative and qualitative data collected led to identification of the critical features which need priority action. An excellent planning tool is SWOT (strengths, weaknesses, opportunities and threats) analysis, as it helps to sum up the features which may affect the outcome of ongoing and future initiatives both positively and negatively. It is also very important to analyze the external context which presents favourable or unfavourable circumstances which cannot be overlooked, lest the outcome of ameliorating initiatives be jeopardized.
Figure 3 shows SWOT analysis of NILDE, conducted by Cristina Cocever and Silvana Mangiaracina, based on the aforementioned considerations.
Conclusions
Assessment of the NILDE service over the past few years is based on a comparison between the quantitative data gathered from the NILDE databases and the qualitative data collected through user surveys. The results turned out to be particularly interesting and a source of hints for planning future improvements.
In 2011, the quantitative data stressed the high productivity of the few libraries which had a significant number of users submitting requests directly; the survey highlighted, on the one hand, user appreciation and, on the other, staff resistance, as users were not considered ready to use the system directly.
Library staff training was carried out in 2012 which led to an increase both in active users and in libraries enabling users to submit requests directly; yet, there is still much more to do. Less than one-third of the networked libraries are involved, and the results of staff encouragement still have to be seen. Re-engineering a service depends on many factors and requires time, because librarians also have the task of being the agents of change, and this involves not only users, but also those in charge of their structures, whose support cannot be taken for
The 2013 survey made it possible first of all to identify the features of the typical user loyal to NILDE: a female researcher in science and technology between 25 and 34 years of age working at a University in the Centre/North of Italy. Much could be done to involve also other potential segments of users, such as students in the case of universities. However, as mentioned above, NILDE has an extremely heterogeneous target audience and all initiatives can only be started by single libraries on an individual basis. An important feature, highlighted in the 2013 survey, is that organizational problems can occur even in libraries which adopt and promote the direct end user mode.
One of the features most requested by the survey participants is integration of NILDE with bibliographic databases, which is, in fact, already possible; this incongruence had already been highlighted by the quantitative data too. Future staff training will have to include the technologies that enable data interoperability, to make it easier for librarians to interact with system administrators, who have to configure the connections.
The unavailability of the electronic document for the end user, widely indicated as a big issue, cannot be easily solved because it stems from the constraints of copyright law and thus is beyond the power of NILDE to fix.
In the near future, resources will have to be invested to create NILDE applications for mobile devices. Periodical monitoring of quantitative data, as well as listening to end users, makes it possible to determine choices and optimize resources. SWOT analysis is a very good planning tool because it helps to assess the information gathered and to identify the elements inside and outside the service which may affect future action.
Notes
",4868,5414
10.1080/07317131.2020.1810439,,,False,Geraldine Rinna,Migration as a Catalyst for Organizational Change in Technical Services,"
This article describes the changes within the Western Michigan University Libraries Technical Services Department during a fiveyear period from 2012 to 2017, which involved a reorganization of staff and the migration to Alma/Primo, Ex Libris' nextgeneration library management system and discovery solution. The aim of this case study is to understand the role these two significant events played in the transformation of the department.
",2020-10-01,Technical Services Quarterly,Informa UK Limited,"Introduction
When Western Michigan University (WMU) began investigating replacing their aging Integrated Library System (ILS), the Technical Services Department (TSD) had a decades-old hierarchical organizational structure built to support the systems and work of the time. Workflows mirrored the organizational structure, with redundancies, policies, and procedures that enforced a siloed chain-of-command and compartmentalization of duties. While following the status quo supported the department's high level of quality for years, the rapid changes coming to the systems and staffing were about to make following the status quo impossible.
This article examines a five-year period from 2012 to 2017 where WMU's technical services experienced a transformational change: a library systems migration, a reduction in staffing, and a reorganization. The TSD went through a perfect storm of overlapping change. Hindsight allows for the analysis of how the department took ownership and facilitated changes, emerging with a modern department capable of harnessing the new systems with a much smaller workforce. The authors' focus here is how the TSD was able to manage this amount of change and use the interplay between migration and reorganization to create a more efficient, equitable, and entrepreneurial department.
Literature review
After a roughly ten-year period of stability and steady growth in WMU's Technical Services Department, there followed a five-year period of drastic change. Looking back, trends and their potential correlations and causations were recognized. Technical services should be adaptive to new technologies, changing cataloging rules, and emerging methods of information consumption, in order to meet developing user needs. Increased budget constraints required work to be efficient and cost-effective. But this particular confluence of technological and workforce change was making a significant impact in academic libraries, and widely so, from what was being discussed informally through listservs, message boards, and at library conferences. By examining the literature, the authors aimed to see if others in the field were drawing similar conclusions. Because the theory presented here had to do with the relationship between events like the migration of a library systems platform and the reduction and reorganization of staff, it was helpful to examine the literature addressing each issue individually, and then other published case studies where these issues converged.
Evolution of the integrated library system (ILS)/library services platform (LSP)
When the literature on migrating from a traditional ILS to a cloud-hosted LSP was initially reviewed prior to the selection process, what was available was mainly non-scholarly in nature, and aimed at a library market still eyeing the options. Marshall Breeding's Library Technology Industry Report provides a consistent guide to new directions in library automation and insight into where the technology is headed (Breeding, n.d.). Breeding's 2007 article, ""It's Time to Break the Mold of the Original ILS,"" recognized the present library environment as one of accelerating change. With a focus on offering new services, a move to web-based discovery, and a continued reduction in staff, there was a need to streamline library systems and create efficiencies through automation. His vision for a next-generation ILS was a much more flexible and open system, a fully integrated platform of library services. ""In contrast to the current monolithic approach, we might consider the ILS as a suite of lightweight applications woven together in a service-oriented architecture"" (Breeding, 2007, p. 41).
Early library management systems, designed around the management of print materials, were not easily reconfigurable to meet the mixed needs of print and electronic resource management, and some companies took the opportunity to start over from scratch, using a new approach. ""As a result, these systems build upon the advances in architecture that allow for multi-tenant operations, data aggregation, analytics, and redundant and secure data centers"" (Grant, 2012, p. 7). The newly emerging Library Services Platforms (LSPs) also took advantage of the benefits of cloud computing. In his 2010 code{4}lib article, Mitchell outlined the benefits for libraries in moving their technological infrastructure to the cloud: ""Cloud platforms enable organizations to use external expertise and resources to deliver complex services, remove the need for organizations to invest in server infrastructure, and lower the cost for organizations seeking elastic computing resources"" (Mitchell, 2010, Introduction, para. 2). As many libraries were eyeing their costly and aging servers and the staffing needed to maintain them, cloud computing was beginning to look like a more sustainable future.
The first-generation ILS was not prepared for the proliferation of electronic resources that are now the mainstay of academic libraries' collections. Electronic Resource Management (ERM) systems were running concurrent to the ILS, creating inefficiencies. The literature leading up to the development of the next generation of library management systems agreed that libraries needed comprehensive resource management which included electronic and digital resources. Wang and Dawes described their ideal system where e-resources workflows integrated acquisition, description, and access provision, relieving what they called a ""messy and redundant workflow"" (Wang & Dawes, 2012, p. 79). Cote and Ostergaard describe the next generation LSP as being able to manage print and electronic resources in a ""single, unified platform, replacing traditional integrated library systems (ILSs) and electronic resource management (ERM) systems"" (Cote & Ostergaard, 2017, pg. 223). An important component in making these integrated platforms work would be the utilization of knowledgebases. In order to truly integrate functionality in one system, the knowledgebase needed to be central to ERM. ""By using the knowledgebase to create consistent, reusable representations for electronic resources holdings, these systems promise to offer libraries new levels of efficiency, interoperability, and automation"" (Wilson, 2017, p. 113).
ILS/LSP migration
Library system migration is an event with extensive ramifications for an academic library, and one that should not be entered without significant research and preparation. The investigation into migrating from WMU's current ILS to a next-generation option is documented in an article that appeared in Computers in Libraries magazine in 2011 and included a survey of academic libraries on their readiness to migrate. In the survey results, approximately 64% of migrating libraries had or expected to restructure workflows, and 25% were expecting to reorganize their units (Kelley et al., 2013). When it comes to change as pervasive and impactful as an ILS migration, no matter the amount of time put into planning and preparation, it is difficult to know where unforeseen problems will arise. The fear becomes not knowing what you do not know. This is where case studies of other migrations can be especially useful.
Day and Ou chronicle their process of determining readiness for migration at the University of Nevada, Las Vegas (UNLV), starting in 2013 with a strategic approach. UNLV developed a task force, representing an organizational cross-section of staff, which assessed the marketplace, identified and reviewed vendors and projects, and ultimately communicated back to the library as a whole on the library's readiness to make what they called ""a significant change"" (Day & Ou, 2017, p. 105). Cote and Ostergaard's (2017) case study on the migration of Montana's Treasure State Academic Information and Library Services (TRAILS) Consortium to an LSP examined the role of the electronic resources librarian (ERL). While the TRAILS migration to Ex Libris's Alma and Primo was complicated by the additional complexities of migration planning within a consortium, the case they made for the important roles an ERL can play in the process are valid across library types. The ERL was able to explain to the implementation team the lifecycle of electronic resources, and the ""possibilities and advantages to be gained from the ERM functionalities in the new system"" (Cote & Ostergaard, 2017, p. 225). Cote and Ostergaard also highlighted Electronic Resources Librarians for their high tolerance for change, their analytical and critical thinking skills, and their experience working with outside vendors. These qualities and more were given as reasons for why an ERL was especially qualified to provide leadership in an LSP implementation.
In Fu and Carmen's (2015) case study of Central Washington University's migration to Ex Libris's Alma and Primo, migration was divided into three phases: pre-migration preparation, testing, and finally the cutover to the new system and post-migration cleanup. They found that the work involved in each phase was, unsurprisingly, extremely time-consuming, and that it took cross-departmental teamwork to complete most events and tasks. Collaboration was named a ""top essential element"" that contributed to the success of the migration (Carmen & Fu, 2015, p. 2).
In Dula & Ye's (2012) case study of Pepperdine University Libraries' migration to OCLC's WorldShare, migration created changes to their workflows that ""instigated cultural change"" (Dula & Ye, 2012, p. 130). Reference librarians had to rethink how searching functioned, and circulation practices changed also, though this ultimately led to improvements in areas like reserves and simplified student training. Technical services had the most dramatic changes, and it was noted that their processes continue to evolve postmigration. In technical services, the areas of acquisitions and cataloging blended and the kind and amount of work was refocused and streamlined. Cataloging became minimal, and the ERM workflow simplified by utilizing the OCLC knowledgebase. What they considered ""revolutionary"" about a cloudbased system was the increased data-sharing ability, which allowed for peer-comparison and a more strategic approach to future acquisitions (Dula & Ye, 2012, p. 132).
Automation and workflows
Within certain areas of technical services work, like acquisitions, working in a cloud-based LSP versus previous library systems required new perspectives on process. Automation of ordering procedures are more inventory-driven than past systems, which previously hung everything on the bibliographic record (Parent & Maclean, 2014). Workflows were often predetermined by the systems functionality, and the compartmentalized modules often increased the siloed nature of the work. Add in the periphery systems needed to perform ERM, and workflows became even more divided.
New acquisitions models complicated matters further -""moving from ownership to access, from just-in-case to just-in-time delivery"" (Gregory, Pastva, & Fox, 2016, p. 30) -and libraries needed to keep up with user expectations in an on-demand digital environment. The very relationship of libraries to their collections was changing. Something had to give, and switching up systems can be a prime opportunity to change workflows and mental models. ""The nature and scale of an LSP implementation is such that there is often an opportunity to re-evaluate workflows with an eye for making processes more efficient"" (Cote & Ostergaard, 2017, p. 225).
Culture of change
Purposefully building a culture that welcomes change is not an easy task. People fear the unknown. One style of managing change might work well at one institution but not another. At Butler University, a change management process was implemented by new library leadership to assist in creating a positive environment for change (Petrusa, 2016). Leadership used the ideas of William Bridges, in his book Managing Transitions, to facilitate managing the large-scale change they were entering, including a migration to a cloudbased library management system. The three phases of transitions outlined by Bridges were used in planning the migration. Bridges' transition model provided guidance for library leaders to acknowledge and address the emotional side of change. Through change style assessment, staff were able to understand their role in the new culture of change in the library. Dulaney (2016) applies the work of Peter Senge to technical services, showing how transformational change can be achieved through Senge's model of a learning organization. The work of technical services has evolved, and ""these changes require continuous creative thinking and experimentation in order to adjust workflows and staff skills"" (Dulaney, 2016, p. 1). Dulaney illustrates how each of Senge's five disciplines can be applied to staff in technical services, and how the culture of a learning organization can provide the creative, flexible, team-based environment needed in contemporary library technical services. Collins and Wilson (2018) use the agile management philosophy to support change readiness in technical services. With its use of iterative decisionmaking and an emphasis on being adaptive, the agile approach can easily be applied to technical services. Collins and Wilson's agile formula (knowledge + readiness = transformation) creates an environment that supports change, and can be a useful tool for people ""struggling to re-align their work and processes,"" because ""at its heart, agile is a cultural solution, which helps to create an iterative mindset"" (Collins & Wilson, 2018, p. 10). Applied to project management, workflow mapping and analysis, agile strategies worked well in technical services, and created a culture ""ready to move away from legacy practices and embrace innovation"" (Collins & Wilson, 2018, p. 18).
While each institution approached the problem differently, all ultimately found that people are the assets, and can make or break transitions. In describing staff participation in the ILS migration at Washington State University Libraries, Zhu and Spidal emphasize the importance of employee involvement: people are the ""solid foundation of any successful migration"" (Zhu & Spidal, 2015, p. 256). People that are not just casually involved, but engaged contributors empowered at every step of the process are ""more likely to embrace changes even if they do not agree with the decisions"" (Zhu & Spidal, 2015, p. 256).
Restructuring and reorganization in technical services
The literature clearly supports the connection between the changing nature of library resources, the expectations of users, and the reevaluation of roles within technical services (Gregory et al., 2016). Selecting, acquiring, describing, managing and providing access to these materials requires a less-siloed, more adaptable kind of technical services. With most resources now being electronic, and new technologies streamlining workflows and automating straightforward tasks (and plenty of the ambiguous tasks too), the work has become even more invisible, making technical services staff and support easy targets when budgets are tight. The areas of cataloging and acquisitions are especially singled-out for reductions, merges, or outsourcing (Gregory et al., 2016).
In ""Transforming Technical Services,"" Jeehyun Yun Davis (2016) identified ""technological innovation and budget constraints"" as the primary drivers of change for an academic library, with the labor-intensive processes prevalent in technical services being impacted by reductions to staffing and budgets (Davis, 2016, p. 52). Davis felt this offered an opportunity for reevaluation and change, and ""to transform the structure of technical services to effectively deploy staff to manage the transformation of its functions through reorganization"" (Davis, 2016, p. 52). Reorganizing or restructuring, however the authors name it, is being used in technical services departments as a tool to bring people and the work they do into alignment with the advancements in technologies. If done right, the organizational changes can take full advantage of the inherent potential for efficiencies offered by new library systems (Petrusa, 2016).
Scope and methodology
The case study method was chosen to review and analyze the broad changes in the Technical Services/Resource Management Department at WMU Libraries. Technical services was chosen as a focus of the case study since effects of migration and reorganization were seen strongly in the day-to-day functions of the department.
The time period analyzed in this case study was a five-year span that began in 2012 with the start of the ILS selection process. The study tracks changes to organizational structure, staffing levels, culture, and library systems during this time.
The data presented in this article were gathered from the WMU Libraries statistics and official and unofficial documents, including strategic plans, meeting minutes, and organizational charts. Qualitative data were gathered from technical services staff using an informal survey and the first-hand experience of the authors.
Background
Western Michigan University is a medium-sized Carnegie Classified R2 institution, offering over 260 degree programs to a student body of around 22,500 (Western Michigan University, 2020). The University Libraries' main library houses the technical services support for the main library and three branch libraries.
In 2012, the Technical Services Department included four fulltime faculty members, twenty fulltime and one part-time staff, and nearly 10,000 student hours annually. One faculty member served as the head of the department, which was divided into three units, Cataloging, Monographic Acquisitions, and Electronic Resources and Serials, each with a faculty lead. The Cataloging unit was the largest, with ten staff members, while both Monographic Acquisitions and Electronic Resources and Serials each had five staff members.
Pre-migration, the servers and support for the ILS were in the Systems Department. Staff maintained the systems and software that provided resource discovery, link resolution and authentication for the WMU community. The position of Systems Librarian, vacated in fall of 2011, and largely responsible for configuring the VuFind discovery layer, was never refilled and was eventually eliminated. Prior to migration, the work of the former Systems Librarian was taken over by Systems staff.
Leading up to 2012, the Libraries were addressing the demand for a more digital information landscape by adding additional systems, mainly locally hosted and maintained, often stand-alone products, which led to a very complex grouping of systems. (see Figure 1: 2012 Library Systems Architecture) While the goal was to create more online access and choices for users, the multiple discovery layers and many siloed systems were confusing, and required an unsustainable amount of system maintenance and configuration. Some interoperability between systems was achieved through writing custom code and scripts, but they were increasingly difficult to maintain. And though many of these systems were used most heavily by both the Technical Services and Systems departments, the lion's share of the decision-making and work surrounding configuration occurred in the Systems Department, which also maintained the servers.
In 2012, all configuration and maintenance work in the ILS, Ex Libris Voyager, and the discovery layers WebVoyage and VuFind were handled in the Systems Department. Bulk importing and exporting of electronic bibliographic records and bulk changes to existing records was done by systems programmers. In Technical Services, the Electronic Resources (ER) team was using a suite of systems for electronic resource management and off campus authentication: USTAT and 360Counter products, which were cloud-hosted and maintained by the vendor; SFX and EZproxy, which were maintained and configured by ER, with server maintenance the responsibility of Systems; and SerialsSolutions 360 Resource Manager, configured by ER, who also configured its companion discovery layer, Summon. Summon, which was cloudhosted, was the third discovery option offered to library users.
With the loss of the Systems Librarian, the custom code that was written for VuFind was no longer supported. For example, a script that communicated journal holdings data from the SFX link resolver to VuFind stopped working. No one on staff had the time and expertise to troubleshoot and repair the code. This model of highly customized, locally-hosted systems produced a technical debt that was unsustainable for a library operating with fewer and fewer staff and a flat or reduced annual budget.
Migration
By 2012, it was evident the Libraries required a new systems model. The Director of Systems had been advocating for the adoption of a cloud-based ILS for some time, and it was an exciting time in the development of library systems, with some next-generation library system contenders developed enough to consider as viable options (Kelley, et al). The move to the cloud was appealing since the WMU Libraries' hardware was nearing end of life and the cost of replacing it would be substantial. Additionally, the hope was that next generation functionality within a single unified platform would reduce duplication of work related to electronic resource management and discovery layer configuration. These issues, and the expectation of upcoming retirements with the potential for recruiting different skills into the technical services department, provided serious motivation when the ILS Selection Overview Committee assembled in late 2012, and the focus was on the new generation of library services platforms.
The Selection Overview Committee was comprised of a cross-section of library faculty, staff, and administration. The committee outlined a project plan that was broken down into five phases: Research and Education, Selection and Preparation, Migration and Integration, Cutover and Shakeout, and Project Evaluation. Leading into the Research and Education phase, members established and chaired subcommittees, representing thirteen areas of library work and technologies. Roughly half of the people working in the Libraries served on at least one of the subcommittees, which aimed to be as inclusive as possible with the goal of finding the best possible system to fulfill all the institution's requirements, and ultimately produce buy-in through participation in the selection process. A similar mind-set of encouraging staff to be engaged, and empowering contributors in the process was used in the ILS migration at Washington State University (Zhu & Spidal, 2015). The participatory nature of the project acknowledged that the migration was much more than a software upgrade. It could fundamentally change how Libraries' staff did their work, and therefore should be supported by all parties. Having a say in the selection process meant that the final decision was not something that just happened to library staff, but rather something which they had taken an integral role in.
During the Research and Education phase, subcommittees familiarized themselves with the available products by reading literature and marketing materials. They developed a list of requirements for a new system, and a list of features they could not live without, or ""deal-breakers."" The committee attended live demonstrations and webinars. Questions regarding desired functions and features were compiled and sent to potential vendors.
After gathering information about each of the candidate systems and lists of requirements from the library, the Selection and Preparation phase began. The data was analyzed, evaluating the pros and cons of each system, while considering the subcommittee feedback. Through process of elimination, certain systems were immediately dismissed based on ""deal-breakers."" The remaining list was narrowed by evaluating the functions and features of each ILS, how it would fit into the Libraries' mission, and if the work required to maintain the new ILS would be possible given the current structure and staffing of the Libraries. Discussions took place about how, or if, data would be brought over from disparate systems into the single platform also. By mid-2014, the ILS Selection Overview Committee and the subcommittees had completed their work, a new SaaS LSP was selected, and university approval and funding was granted.
Once the system was selected that could fulfill immediate requirements and had the potential to provide desired features in future developments, the Libraries began the Migration and Integration phase of the project. Final decisions were made about what data would be export from outgoing systems, and what data would be left behind. Parts of this data became a ""test load"" in the new system to evaluate for completeness, accuracy, and quality of the migrated data. Staff partnered with the ILS vendor to configure interoperability with third-party systems used within and without the library. Extensive training began for all staff members in the use of the new system, utilizing vendor videos, documentation, and a sandbox.
By March 2015, the ILS Migration Team (formerly the Selection Overview Committee) and the ILS vendor had completed the work of integrating thirdparty systems being retained. A portion of data to be migrated was identified, cleaned, and loaded to a test instance and configured in the new discovery layer. In mid-2015, the Migration and Integration phase of the project was completed and the Libraries ""went live"" with the new system. The Libraries' systems architecture looked drastically different once the number of systems used was effectively cut in half: two systems eliminated, four systems moved from the ground to the cloud, and three cloud-hosted systems no longer needed after migrating to a single platform that provided the same functionality in an integrated system.
After the go-live date, during the Cutover and Shakeout phase, staff were working in the new system, but retained access to the old system for a few months. This allowed for data comparisons and time to correct any errors caused by the migration. Before going live, staff received group training, but working independently with the Libraries' collection data was considered the best way to learn their work in the new system. This built confidence and trust. In Technical Services, supervisors were especially empathetic to the widespread changes affecting people who had spent up to fifteen years working within a largely unchanged system and allowed for this prolonged cutover time for staff to become accustomed to performing their work differently. There was no urgency to quickly change physical materials workflows, but to allow for a natural evolution between past and future workflows. Electronic Resources used the Cutover and Shakeout phase to automate record loading, additional electronic data interchange (EDI) invoicing, and other work within the new LSP.
The final phase of the migration project, Project Evaluation, did not occur due to turnover in the positions responsible for the project's management. Informally, staff and faculty in the Libraries provided feedback to the vendor and input to other migrating libraries, but mainly were getting on with the business of learning the new system and its capabilities, and discovering new, sometimes easier ways to complete their work. In some ways, the Shakeout portion of the plan was still underway, especially in Technical Services, as staff became more comfortable with the new system, as new features and functionalities became available, and as everyone became more accepting of iterative change.
Staffing reduction and realignment
By the time the migration was complete in 2015, the Technical Services Department had experienced staff loss, mainly due to retirements, that reduced staffing levels by 12%. These vacancies were not filled; nonessential work was eliminated, and essential work reallocated to other staff. Additionally, the amount the library was spending on physical materials had been reduced by 38% by 2015. A shelf-ready workflow had automated most copy cataloging and processing, so the department needed less student labor, which in turn warranted a reduction in student hours by 47%. Overall, the technical services workers were reduced between 2012 and 2017 (see Figure 3: 2012-2017 technical services staffing levels).
Another department to experience dramatic staffing changes during this time was the Systems Department, which by 2016 experienced an almost complete turnover. As positions were refilled, they were redesigned to fit the department's shifting focus from back-end system support to front-end user and emerging technologies, as illustrated by their name change to IT Services. Since many of the locally-hosted systems had moved to the cloud, been rendered obsolete, or fundamentally changed in structure, the volume of work performed in Systems related to the bibliographic and discovery systems was greatly reduced.
Reorganization
In February of 2016, University Libraries welcomed a new Dean. She began leading the Libraries in the next strategic planning cycle, which included a plan to reorganize the Libraries. The new strategic plan for the Libraries, adopted in 2017, included the goal, ""Reorient the Libraries' organizational structure to better align with our mission and vision, and foster a user experience culture"" (Western Michigan University Libraries, 2017a, Goal 5). Objective 5.A of that goal was to ""identify opportunities to transform work processes, build efficiencies, and realign human resources to focus on strategic priorities and more effective ways of working.""
A Reorganization Task Force (RTF) was formed from a large representation of Libraries staff, faculty, and administration, including members of the Technical Services Department. The task force was charged with documenting all library services and activities and examining their continued relevance based on the newly adopted strategic plan. One strategic theme was to redesign organizational communications, operations, and structure. The task force was further charged with identifying where the Libraries could ""break down siloes and bring like-functions into closer alignment; streamline workflows; reduce redundancies and eliminate outdated or unnecessary processes, services, and activities; and remove unnecessary layers within the organization"" (Western Michigan University Libraries, 2017b, Charge to the group). To say this team was charged with a big task with far-reaching implications is an understatement.
As a result of the recommendations of the RTF, significant changes to the overall structure of the Libraries were implemented. Fourteen departments were consolidated down to six over a transition period of four months. While some areas of the Libraries were extremely restructured, the Technical Services Department, now renamed Resource Management, underwent less dramatic changes as a result of the reorganization, having already undergone staffing level adjustments due to retirement and necessary workflow changes instigated by the migration to the new LSP (Garrison & Bundza, 2020).
In May of 2017, tasked with discussing and implementing the changes on a more granular level, the Resource Management Department met in their newly arranged units: Cataloging and Metadata, Collections and Stacks, and Electronic Resources. Each unit answered the following questions: What is the work of the unit? What are the strengths, limitations, demands for new skills, and plans for developing or recruiting these skills? Then together as a department, these findings were combined and refined, creating a focused statement on how Resource Management fit within the new Libraries' organizational structure. This provided a strategic foundation for how to move forward post-reorganization and identified goals and gaps in staff expertise to focus on for future training and hiring.
Structurally, staff hierarchy was collapsed, flattening a staffing structure originally designed for more positions and siloed workflows. Now all staff report directly to their unit's faculty supervisor. In addition, the Department Head positions were to be eliminated throughout the Libraries in favor of a term-appointed department lead, a non-supervisory position that would coordinate departmental work and represent the department at the newly created Libraries Council. This is a 2-year rotating positions among the faculty members with 20-30% of their assigned workload allocated to cover the additional administrative dutie",6180,7017
10.1016/j.lisr.2019.02.005,,,False,Mehri Shahbazi,Development of a scale for data quality assessment in automated library systems,"
A credible scale based on the opinions of system users was developed to evaluate and assess data quality in automated library systems (ALS). Development and testing were carried out in two stages. In the first stage, 77 dimensions for data quality which had been previously identified through a systematic literature review were used to develop scale items. The first draft of the scale was then distributed among a target population of ALS experts to solicit their opinions on the scale and the items. In the second stage, a revised version of the scale was distributed among the main study population, which included end users of the target systems. This stage used factor analysis to determine the final draft of the scale, which consists of 4 factors and 62 items. The 4 factors were named after the qualities of their associated items: Data Content Quality, Data Organizational Quality, Data Presentation Quality, and Data Usage Quality. This scale can help system managers identify and resolve potential problems in the systems they manage and can also aid in evaluating the quality of data sources based on the opinions of end users. suggests this inconsistency is rooted in the quality of data entered into the system (e.g., Dalcin, 2005;Fadli, 2013). Attention to, and assessment and evaluation of, DQ can be important in improving information retrieval in information systems and saving time for end users. However, to solve problems with regard to DQ in any organization or system and to bring about improvements, it is necessary to understand the dimensions of DQ.No previous studies have attempted to assess DQ in ALSs, taken in the present study to mean computer-based library systems that manage input, processing, and bibliographic output. To reduce DQ problems in these systems it would be useful to have a scale which could assess DQ along different dimensions of ALSs. Given that the point is to facilitate
",2019-02-16,Library & Information Science Research,Elsevier BV,"Introduction
Developers of information systems are usually concerned about end user satisfaction, however, there are few studies available which develop evaluation criteria for information systems based on user satisfaction. Evaluation criteria usually address concerns in categories such as management, technical issues, usage, boundary issues, policy, and customer issues (Farajpahlou, 2002). An evaluation is basically a judgment of worth; the ability to evaluate the return on investment provides the basis on which to choose between alternatives. Comparison of evaluation results with external standards, in the light of existing institutional realities which may be relevant, offers a path to evaluating the future trajectory of a program or service and provides an objective basis for decision making (Sharma, 2007).
The main goal of library-related information systems, whether database, website, portal, or automated library system (ALS), is to allow for search and retrieval from what have become enormous volumes of data. Since the success of data retrieval rests mainly on the soundness of the retrieval path, solving information retrieval path problems is one of the major concerns for developers of such systems and can lead to consumption of large amounts of time and financial resource for the individuals and organizations involved. However, despite the expenditure of much effort and money, end users of these systems still experience problems with data retrieval and quality. This study focuses on the different dimensions of data quality (DQ) and their significance according to end users.
In the information science literature the terms data, information, and even knowledge are often used interchangeably, which can lead to confusion. However, the literature that focuses on evaluation of quality generally agrees that dimensions of data and information quality are similar, and so in the present study the same stance is assumed. Also, according to Smart (2002) and H. Chen (2009) there is no standard, uniform and universal definition for DQ; however, the concept of ""fitness for use"" (Wang, 1998) has widely accepted for some time, and represents the spirit of the present study.
Problem statement
As stated earlier, satisfaction of end users is one of the most important concerns of information system designers and manufacturers, and it remains an elusive goal. One aspect of the problem is data quality. In many ALSs, search results do not match searchers' requests and expectations. Evidence success and satisfaction for end users, it would seem obvious that their opinions should be taken into account in devising a scale.
Such a scale would benefit librarians, designers, and developers of ALSs. Not only might it elucidate the attitudes and opinions of different users about different dimensions of DQ in ALS, but it would also serve to identify potential system problems and promote improvement.
The following questions guided the present study: RQ1. What are the most important dimensions of DQ in ALSs according to experts?
RQ2. What are the most important dimensions of DQ in ALSs according to end users?
RQ3. What would be the factors and items of a credible scale for assessing DQ in ALSs according to end users?
Previous research
Studies of different dimensions of DQ, especially in information systems, have gained increased attention. ""Dimensions"" are the factors that can help evaluate the quality of data. An extensive content analysis of DQ studies by Shahbazi (2017) identified 77 dimensions of DQ referenced in the literature, though there are admittedly some fine shades of meaning between some dimensions (Table 1). Other studies have attempted to propose models, methods, or scales for assessing and measuring the quality of data or information in various information systems (Rahimi, Farajpahlou, Osareh, & Shahbazi, 2017). A selected list of such studies can be seen in Table 2.
Most studies have considered primarily the structure, services, and presentation of models for assessing ALSs and similar systems. Some also have examined customer satisfaction, but most of these only address one or a few aspects of DQ as part of a larger list of aspects (Bhardwaj & Shukla, 2000;Joint, 2006;Osaniyi, 2010;Ramesh, 1998;Sharma, 2007;Taole, 2008). No studies have specifically attempted to assess DQ in ALSs.
Methodology

Scale development
This research used both bibliometric and survey methods, applied in two stages. First, using resources from the literature in the field of data quality, various dimensions of data quality were extracted and used to create items for a questionnaire, which in this first stage consisted of 77 dimensions and 147 items. The items in the scale were informative statements created by the researchers based on the definition of each dimension of DQ. The factors were sets of different items which emerged after categorization using exploratory factor analysis and were used as subscales to assess DQ in connection with end users. The validity of the questionnaire was determined using content validity at each stage by consulting experts and faculty members in the fields of informatics, scientometrics, and computer sciences. For this purpose, at each step, after refining each item, the list of factors and relevant items was prepared and was given to these experts to resolve grammatical and conceptual issues that could be spotted in the content. Experts' views were then applied in revising the survey. The modified version of the scale which was then used in the second stage of the study had 70 dimensions and 127 items. This scale formed the basis for a final scale that was developed on the basis of the opinions of end users about DQ in ALSs.
Cronbach's alpha was used to determine the reliability of the questionnaire. Usually the reliability of a questionnaire is considered acceptable if this coefficient is higher than 0.75 (Dayani, 2005). Cronbach's alpha calculated in each stage of the study showed the questionnaire to be highly reliable in both stages (Table 3).
The main tool used for data collection was a Likert-type scale that included a number of factors and items. The population for the final stage consisted of two groups: end users of the ALSs in libraries both in Isfahan and Ahwaz in Iran, as well as systems experts in the same libraries, i.e., managers of ALS and those involved in design and development of the systems. End users included both library customers and employees.
Populations
The academic libraries of Isfahan and Ahwaz included libraries in public (Payam-e-Noor University, the University of Applied Science and Technology) and nonprofit universities supervised by the ministries of Science, Research and Technology, and Health, Treatment and Medical Education as well as libraries in Islamic Azad University branches. These libraries were included because they use the most common ALSs available in Persian. They were also more accessible for the researchers.
The population in the first stage consisted of experts in the fields of scientometrics and informatics. Given the small number of members in this population, sampling was not necessary and the census method was adopted. All managers of libraries and information centers, all librarians with experience with ALSs in Isfahan and Ahwaz, and all experts in the firms developing ALSs made up a total of 90 individuals, of whom 79 agreed to participate in the study. The majority of participants (78.5%) had a degree in librarianship or informatics. Most participants held either a bachelor's (53.2%) or master's (29.1%) degree; only 3.8% held a PhD degree, and 6.4% held either high school or associate degrees.
The population for the second stage consisted of 120,849 potential users of ALSs in academic libraries of Isfahan and Ahwaz, that is, all the academic members in various departments and faculties in the five universities. Since exploratory factor analysis was used in this stage to modify the questionnaire and prepare the final scale, the sampling ratio of 1 over 5 was used based on the number of items against the relevant population (Beshlideh, 2012). Hence, given the 127 items in the questionnaire, against 120,849 potential individuals for the study, 635 potential end users were selected using stratified random sampling; all then completed the questionnaire. Among these participants, 57.9% had bachelor's degrees, 21.9% held a master's, 16.7% held PhDs, and 3.5% had associate degrees. Fields of study included humanities (34.5%), medical sciences (26.9%), technology and engineering (25.7%), sciences (7.7%), and arts (5.2%). 
Findings

Most important dimensions according to experts
The main goal of the first stage was to categorize different dimensions of DQ based on the opinions of the experts who were involved with the ALSs. The questionnaire at this stage consisted of 147 items (using Likert-like scales) that were categorized in 77 dimensions. Representational consistency, with an average score of 3.71, proved to be the most important dimension of DQ in experts' views. The next 10 important dimensions were documentation, structural consistency, compatibility, conformity, response time, confidentiality, storage capability, semantic consistency, homogeneity, and comparability.
The principal purpose of this stage was to derive a modified questionnaire with high reliability for application in the second stage of the study. This was achieved by using correlation testing, which resulted in removing some unimportant dimensions among which low correlations existed. This method also identified items whose elimination would lead to increased correlation and reliability (through Cronbach's alpha).
Data analysis in the first stage showed that eliminating 21 items in 7 dimensions of unambiguousness, customer support, interactivity, ease of manipulation, correctness, redundancy, and recoverability actually increased the value of Cronbach's alpha for the questionnaire. Therefore, these items and dimensions were eliminated and a modified version of the questionnaire was developed with 127 items in 70 dimensions, and was administered in the second stage.
Most important dimensions according to end users
The second stage of the study sought to identify the most important dimensions of DQ based on the views of end users. The modified questionnaire resulting from the first stage was distributed to the 635 potential end users.
In this stage, exploratory factor analysis was used to allow the researchers to categorize the scales and observed variables without applying a precondition to the data. After gathering the necessary data, principal component analysis (PCA) was used to analyze the results. Fitness of data for factor analysis was investigated using anti-image correlation, Kaiser-Meyer-Olkin testing, and Bartlett's Test of Sphericity (Beshlideh, 2012;Pallant, 2010). From the correlation indexes that were higher than 0.3 and calculation of the anti-image correlation matrix, it was concluded that sample size was sufficient for factor analysis. Also, all diagonal elements of this matrix were greater than 0.50 and between 0.70 and 0.99. The Kaiser-Meyer-Olkin value (0.941) also showed the credibility of data and a sufficient sample size for factor analysis (Table 4). Further, the calculated value for Bartlett's Test of Sphericity (0.000) showed that data had reached statistical significance level and factor analysis on the correlation matrix was possible. If Bartlett's test of sphericity is lower than 0.05, this shows that the component matrix is not an identity matrix, and adequate correlation exists for factor analysis. Also, the determinant of the data matrix (6.35) is higher than 0.00001, indicating a lack of multiple linearity problems, which confirms that the data were suitable for PCA.
The communalities table in this analysis showed that the largest factor loadings belonged to understandability, traceability, availability, novelty and freshness. In the view of end users, these items and their dimensions are the most important items and dimensions of DQ in ALSs. The lowest factor loadings belonged to security, duplication, concise representation and credibility (Table 5).
Final scale
In order to determine the items and dimensions for the final scale, PCA and extraction of primary factors were carried out using eigenvalues and the Scree test (Beshlideh, 2012;Gildeh & Moradi, 2012;Pallant, 2010). Eigenvalues and total variance explained for factors were calculated. The first three columns of Table 6 show factors, their eigenvalues, total variance explained, and cumulative percentage. The other columns of this table show total explained variance for four main factors before and after rotation. Eigenvalues showed that 32 factors had variance larger than 1 (only the top 10 factors are presented in this table). The explained variance of the first four factors is the highest before and after rotation and these four factors together can explain 34% of total explained variance.
A scree diagram (Fig. 1) also shows the eigenvalue for each factors. The slope of the curve toward the x-axis changes between factor 3 and 4. This figure shows that the first factor is a dominating factor. Based on  0.000 the eigenvalues and the scree diagram, the first 4 factors were extracted for further analysis. The researchers, considering the survey items and the factor loading, explored the proper categorization for the factors and the survey items by several proper rotations. The component matrix shows that pure variables with load factors of 0.3 or higher only happen in one factor. Therefore, in order to better interpret and categorize factors and items in a factor, rotation was required. Hence, at this stage data analysis began with a Varimax rotation. In categorization of factors and items in this rotation (Table 7), some items had two or more factors with factor loading higher than 0.3. Furthermore, the component matrix also showed a correlation higher than 0.3 among the factors. This allows application of the Oblimin rotation in the analysis process (Pallant, 2010) and so the component matrix was repeated using Oblimin rotation (Table 8).
The Oblimin rotation was carried out once with a rotation of 0.33 and then again with a rotation of 0.4 The values resulting from this rotation and the factor categorization matrixes were analyzed by researchers and several experts and finally an Oblimin rotation of 0.4 was accepted as the proper rotation at this stage. This rotation led to categorization of the items into 4 factors and 62 items (Table 9). Thus the final DQ scale for assessing data quality in the ALSs according to end users was developed.
Each of the 4 factors form a subscale. In this stage, 24 dimensions of quality were eliminated: concise representation, obtainability, security, believability, complexity, duplication, compatibility, structural consistency, semantic consistency, attractiveness, currency, organization, adaptability, ease of operation, precision, naturalness, applicability, reliability, identifiability, homogeneity, definition, density, age, and uncertainty.
The final stage of the scale creation usually includes naming of the factors or subscales that turn up in the final rotation. To this end, researchers used the opinions of professors and faculty members as well as considering the nature of each subscale to create the factor names seen in Table 9. The subscales (factors) were accordingly named Data Content Quality (17 items), Data Organizational Quality (6 items named), Data Presentation Quality (12 items), and Data Usage Quality (27 items).
Discussion
Interestingly, the opinions of experts were different from those of end users about the most important dimensions of DQ. Experts felt that the most important dimensions mostly related to structure and content of data in the target systems, while end users believed that dimensions related to data retrieval as well as the novelty of data in the systems were important. Furthermore, dimensions of compatibility, structural consistency, semantic consistency and homogeneity, which were some of the most important dimensions in the views of the experts, were found to be the least important according to end users. The final four categories or subscales of DQ, derived from extensive analysis, seem to make intuitive sense, as they are concerned with content quality, quality of content organization, quality of data display and organization, and data utility.
More attention to the dimensions eliminated after analysis of end user data also revealed that these dimensions fit in categories that are less concerned with quality and therefore less likely to be used for evaluation of DQ by end users. The list of the least important dimensions according to experts and end users complied with findings reported in over a decade of studies, including Najjar (2002); Missier et al. (2003); Rajamani (2006); Stvilia (2006); Martinez (2007); Herrera-Viedma, Peis, Morales-del-Castillo, Alonso, and Anaya (2007); Xiaojuan, Shurong, Zhaolin, and Peng (2008); Calero, Caro, and Piattini (2008);Caro, Calero, Caballero, and Piattini (2008); Haug, Arlbjørn, and Pedersen (2009); Michel-Verkerke (2012); Saberi and Mohd (2013);Guerra-García, Caballero, and Piattini (2013). Some of these studies studied DQ dimensions in their own fields, however, unlike the current study, none of them focused on end user views.
The items included in the scale proposed in this study also pinpoint some problems with DQ in information systems. These are problems mentioned by researchers in the literature but no study has attempted a comprehensive categorization of these problems. The results of these studies, based on their views of the problems and related dimensions, are comparable to the items designed in the proposed scale in the present study. Strong, Lee, and Wang (1997) divided problems with DQ into three categories:
• intrinsic problems, such as lack of match between similar data from different sources;
• accessibility problems, for example lack of access to computerized data (often due to low system resources), time and effort spent to gain access to information, limited processing capacity for image and text data, and high volume or the low processing speed; and
• data usage problems, such as the existence of incomplete data as well as contradictory displays of data which leads to weakening of the quality of text data. This includes some mal-presentations of text that are ambiguous either in content or in form, while important in essence as values. Such data could potentially mislead or be misunderstood by users.
These problems are directly and indirectly represented in the design of the items of DQ evaluation mentioned in the present study. Stvilia (2006) focused on a number of problems with data usage, including accessibility, accuracy, authority, cohesiveness, complexity, consistency, informativeness, naturalness, relevancy, and verifiability. He believes that these problems derive from complexity, language ambiguity, poor structure, typographical errors, the use of alternative words, confusion or software clutter, lack of supportive sources, lack of accurate resource review, bias in review of resources, multiple perspectives and unbalanced coverage of different perspectives, lack of details, low readability, use of different terms for similar concepts, use of different structures and styles for particular types of data, mismatch between recommended styles and guides, differences in cultural or linguistic meanings, standards, content redundancy, non-fluent text, unrelated or out of context content, lack of references to original sources, lack of access to original sources, and instability caused by editing sabotage (p. 160). The problems mentioned by Stvilia are found similarly among the items of the proposed scale in the present study.
Berti-Équille ( 2007) discusses the problem of the existence of similar records with different titles and believes that this can be related to such procedures as reconciliation, refinement or unification, subject matching, duplicate removal, citation matching, identifying uncertainties, identifying nature, and separation by nature. He also emphasizes problems related to obsolescence and outdated data (p. 192). The important thing to note when comparing the results of current study with those of Berti-Équille is his emphasis on duplication, which turned out to be considered not important by participants, hence it was eliminated from the final scale.
Limitations
Some limitations of the present research resulted from the research design. Some items were eliminated in stage 1, before end users saw the scale, and so end users did not have the opportunity to present their views on those items. Therefore for those items there could be no   comparison between experts and end users. Also, some dimensions were conceptual or abstract and therefore more open to personal interpretation than other more concrete items.
Conclusion
This research has led to the construction of a scale that can be used to evaluate the quality of data in ALSs. The proposed scale is unique in its method of DQ assessment. The main and the most important benefits of this scale lie in the help it can provide to library managers and system staff in identifying potential problems in ALSs and resolving these problems, especially when they affect user satisfaction. The dimensions and items covered in the proposed scale make it a useful tool for evaluating DQ in ALSs, especially where views of end users are desired. Furthermore, along with the content analysis by Shahbazi (2017), the fairly comprehensive coverage of items and their organization into meaningful categories would be useful for anyone studying the evaluation of data quality in information settings. There is data on library hosting the resources 27
The credibility of each data can be determined using scientific evidence 28
There is an option near each retrieved result which shows the table of content or abstract of the resource in a separate page 29
There is information in the system on how to enter data 30
There is an option near each retrieved result which shows the bibliographic information of the resource in a separate page 31
The standards used help users access to other information in the system 32 When data are entered, all subsystems are accessible 33
There is data on resources' authors 34
The number of retrieved records relevant to search keywords is higher than the number of irrelevant results 35 Data are presented in detail 36 4th = Data Usage Quality Using cataloging number, it is easy to find the resources 37
All retrieved records are understandable for users 38
The language used in this systems is understandable for users 39
All ",4611,5112
10.1108/lht-06-2017-0131,,,False,Aleksandar Simović,A Big Data smart library recommender system for an educational institution,"
Purpose -With the exponential growth of the amount of data, the most sophisticated systems of traditional libraries are not able to fulfill the demands of modern business and user needs. The purpose of this paper is to present the possibility of creating a Big Data smart library as an integral and enhanced part of the educational system that will improve user service and increase motivation in the continuous learning process through content-aware recommendations. Design/methodology/approach -This paper presents an approach to the design of a Big Data system for collecting, analyzing, processing and visualizing data from different sources to a smart library specifically suitable for application in educational institutions. Findings -As an integrated recommender system of the educational institution, the practical application of Big Data smart library meets the user needs and assists in finding personalized content from several sources, resulting in economic benefits for the institution and user long-term satisfaction. Social implications -The need for continuous education alters business processes in libraries with requirements to adopt new technologies, business demands, and interactions with users. To be able to engage in a new era of business in the Big Data environment, librarians need to modernize their infrastructure for data collection, data analysis, and data visualization. Originality/value -A unique value of this paper is its perspective of the implementation of a Big Data solution for smart libraries as a part of a continuous learning process, with the aim to improve the results of library operations by integrating traditional systems with Big Data technology. The paper presents a Big Data smart library system that has the potential to create new values and data-driven decisions by incorporating multiple sources of differential data.
",2018-04-19,Library Hi Tech,Emerald,"Introduction
Traditionally configured systems for data storage and analysis prevent libraries from achieving competitive advantages. Over the past decade, many academic libraries have struggled to shift the value and utility of collected data (Buckland, 2017). Library users have switched to online scholar sources of information, and academic libraries have lost their monopoly over the provision of scientific information (Chambers, 2013). Library users retrieve new data necessary for learning, evaluate new theories, or discover a new addition to knowledge. Each of these functions involves determining the specific knowledge, professional literature and other learning materials that may not be available in the library (Feisel and Rosa, 2005). Identifying and analyzing data beyond the library, through campuses and external aggregations, can develop effective services and systems bringing value to the wider institution (Showers, 2015). Large amounts of available data and the implications of differential resources increase the complexity of their collection even further (Showers, 2014). Library catalogs need to carry enough information about items and users preferences to have the capacity to determine a potentially ideal result and to respond adequately to the given query (Horstmann and Brase, 2016).
Nowadays, the cumulative increase in the volume of data from various sources relational and non-relational (stored in local and in the cloud environment) has given rise to the problem of providing an efficient library service to the users. According to 498 LHT 36,3 Rubin (2017), educational institutions are in a period of transition in how they deliver their library services. New methods of content delivery allow e-libraries to substitute traditional face-to-face librarian recommendations. Recommendation methods need to adapt/respond to continually improving new technologies offering the possibility of new forms of delivery just like the nature and structure of educational library catalogs might well change so as to offer content of greater interest in line with the emergence of new economic models. These challenging conditions make a smart library an inescapable part of a modern library business system, which itself is an integral part of learning processes and the educational institution.
The approach used in this work is based on the idea that the integration of Big Data technologies into smart library data management ecosystem solves issues such as: how large amounts of data from different sources can be collected and connected, integrated and stored, and analyzed and visualized; and how to display the content of more interest to users through a recommender system?
In order to improve the process of meeting user needs in the continuous educational cycle, the proposed Big Data recommender system enables data integration from various sources (e.g. Learning Management Systems (LMS), University online bookstore, Internet of Things (IoT), data from social media networks, and traditional library) into the smart library, making the approach particularly suitable for application in educational institutions.
The main contributions of this paper are:
(1) Recommendation systems were made based on practical requirements as personalized e-services that have application in different domains. Existing recommender systems mainly focus on well-known approaches and they are reviewed in the next section. This paper proposes an integrated recommendation system that complements existing systems and provides a useful guide for librarians, practitioners, and researchers in developing a Big Data smart library model, and creating a new service that will improve the educational process.
(2) This paper provides a framework for an efficient application of four independent data sources into the Big Data ecosystem making the smart library an integrated part of the educational continuum.
(3) For each data set, it effectively identifies the specific details and requirements for an integrated recommendation with the aim of improving the results of library operations by merging the traditional library and information systems (ISs) of an educational institution with a Big Data framework. This will motivate and support researchers and practitioners to promote the popularization of this approach.
(4) It particularly suggests possible further research of integrated recommendation systems in the Big Data era with the proposition of developing a smart library suitable for an educational institution.
The motivation behind this study is the lack of a comprehensive survey in the field of Big Data smart library recommenders that approaches the issue from the perspective of an educational institution as a foundation of knowledge creation and dissemination in society.
The paper is organized as follows. First, it provides a review of the literature related to Big Data technologies and recommender systems with a comparison of traditional systems and other research fields in diverse domains where have found application. The next section presents the Big Data smart library model with the ultimate aim being the development of an integrated recommender system suitable for an educational institution with a detailed outline of its implementation. That is followed by an evaluation of the system and its results. The next section is a discussion. Finally, in the conclusion there is discussion which includes the limitations, theoretical and practical implications of this research and outlines future work.
499

Big Data
Literature review Nowadays we are confronted with a lot of different types of data which is generated at a high rate. According to Gartner, Big Data is classified as one of the most important strategic technology trends in 2017 (Panetta, 2016). International Data Corporation (IDC) in its study estimated that the overall growth of the digital universe by 2020 would reach 40ZB of data (Edjlali et al., 2012). Research and scientific institutions, as well as companies in the public and private sector, generates large amounts of data on various topics (Fey et al., 2008): climate change, video surveillance of transport infrastructure, history of patients in health institutions, purchase history from e-commerce stores, social behavior through interaction on social networks, etc. (Boyd and Crawford, 2012). This is the result of more and more devices being integrated into the business processes of organizations (Cukier and Mayer-Schoenberger, 2013). Laney (2001) has defined the challenges that bring large amounts of data through the ""3Vs"". Volumethe total data set size and cumulative volume beyond the capacity of existing relational database management systems (RDBMS) to process it; velocitythe speed at which new data is created which represents dynamic data-use through the interaction of participants; and varietydifferent formats of incompatible and inconsistent data structures, unorganized and large, carrying information of importance waiting to be analyzed and used (Buyya et al., 2016). In 2012, Gartner expanded the Big Data definition and declared that ISs need new forms of processing to allow improved decision making, gaining insights and optimizing business processes (Beyer and Laney, 2012). IBM added another V attribute to Laney's definitionveracity. This is known as the ""4Vs"" (Erl et al., 2016). Zikopoulos et al. (2012) explained the reason behind IBM's additional V and introduced the dimension of accuracy in relation to the quality of data sources in facing Big Data initiatives. According to some researchers, there are as many as ""10Vs"" in Big Data analytics in scientific papers (Markus, 2015). Based on the historical growth rate of the overall data generated and data flow, Cisco claims that humanity entered the ZetaByte era in 2015 (Cisco, 2017).
Big Data technologies have great application in recommender systems. They have taken the place of widely applied tools in various domains of digital businesses (Philip Chen and Zhang, 2014). However, they also have great potential to be applied in library ISs and to contribute a better understanding of user needs by proposing content of interest. The goal of the recommendation system is to reduce information overload by extracting the most relevant content and information from a vast amount of data. Adomavicius and Tuzhilin (2005) presented an overview of content-based (CB), collaborative filtering (CF) and hybrid-based recommenders, describing the limitations of these approaches and discussed possible improvements. These approaches prognosticate the level of user interest or the usefulness of a particular item and rank them according to predicted values (Bernardes et al., 2014). CB is based on the assumption that the usefulness of the item will be similar to those that the user has preferred in the past, while CF predicts the usefulness of a particular item for a user based on the evaluation of that item by other system users. The combination of these algorithms creates a hybrid-personalized system of recommendations which calculates both, the user's rating, and the function of the content (Herlocker et al., 2000;Adomavicius and Tuzhilin, 2005;Lu et al., 2015). Recommender systems have a common feature that is recognized as an important source of information to offer to users to rate items and post reviews according to their opinion (Lin, 2014). Lee et al. (2015) advocated the need for new techniques to reduce bias in movie ratings, raising questions about the reliability of ratings as an impartial quality indicator. They also found that a prior rating by an online community as crowd vs friends can have a varying impact on subsequent user's ratings. Gao et al. (2018) investigated the influence of cultural factors on the users' online rating behavior focusing on how cultural values affect hotel ratings. They empirically found that reviewers from countries with high power distance give lower online hotel ratings.
500
LHT 36,3
Recommender systems are widely used to suggest contacts, or activities on social media platforms (Wu et al., 2014), and to improve targeted ads by the advertising industry (Buyya et al., 2016). Practitioners can develop new marketing strategies by integrating users' current situations and future needs by offering contextually relevant socialized recommendations (Shen et al., 2013). Lu et al. (2015) presented a comprehensive analysis of recommender systems applications reviewing up-to-date application developments and clustering them into eight main categories.
In accordance with the above, even from Ranganathan's (1931) ""The Five Laws of Library science"", the question of intelligent exploitation of new technologies is raised to improve user service for human needs and creating the future. Alvin Toffler in his book Future Shock (1971) set the theory that the information overload will lead to decision-making conflict. A comprehensive overview of information overload referring to too much information identified by the academic community was presented by Eppler and Mengis (2004): excessive communication overload (Meier, 1963), information overload of sensors (Lipowski, 1975), cognitive overload (Vollmann, 1991), information knowledge overload in medicine (Hunt and Newman, 1997), syndrome of information fatigue (Wurman, 2001). Also, there are studies that have confirmed and located other types of information overload within the professional service sector (Srinivasan, 2016) ranging from business consulting (Hansen and Haas, 2001) to management meetings (Grisé and Gallupe, 1999).
According to Teets, the collation of information has been going on for centuries. One of the first recorded attempts was in the middle of the third century BC when Pinakes organized a library catalog, by listing author's names in alphabetical order (Weinberger, 2007). Little seems to have changed in the meantimeinformation in library catalogs were still primarily organized for the physical world (Eliot and Rose, 2009). The significant innovation occurred in 2010 when the German National Library released a library catalog linking authors' works to others in the same fields (Svensson and Jahns, 2010). In 2011, the British Library published national library data as a linked data, describing the model of Things of Interest, where the book title linked to people, events, and places (Hodder, 2013;Teets and Goldner, 2013). In 2009, Tim Berners-Lee recommended the first step which was to set up data and information on the web in a form in which machines can naturally understand or convert into an understandable form that will lead to the end result of this process -Linked Data (Bizer et al., 2009). This is precisely the issue because modern data library systems should provide insights by utilizing various techniques, such as Big Data analytics of all type and sources of data; they should enable business analytics and real-time processing whilst increasing the capacity to deliver significant data and content of interest through the recommender system. Such system provides comprehensive logistics and an analytical platform with the fully featured tools and solutions to meet the needs of the most sophisticated and demands of modern, smart library systems.
Traditional and e-library recommender systems Traditional library systems of educational institutions are usually configured as RDBMS, and they are still very wide-spread. As such, they lose the ability to process large logs, text, images, audio, video, sensor records and other complex types of data that arrive at a high rate from different sources. At present, the structured database systems that store the vast majority of organizational data are unsuitable for analytical processing. Difficulties also lie in data capture, sharing, and visualization (Ahrens et al., 2011). Big Data applications and systems are built to respond to these emerging challenges. Using Big Data frameworks allows practitioners to make decisions based on evidence rather than on intuition. Traditional library systems often suffer in both, memory use and storage capacity (Bekkerman et al., 2011). Valuable data in those systems that may carry important 501 Big Data information, collected and stored at a high cost is ignored and finally deleted because of limited warehouse space (Worlton, 1971). Now, when libraries are faced with the Tony Hays fourth paradigm of data-intensive science and overwhelming data sets from many different sources, it becomes more and more difficult for a library to provide enough space to store all the necessary information that is important for users, and submitted for its long-term keeping. (Hey et al., 2009;Horstmann and Brase, 2016;Bhat, 2018). Furthermore, in the traditional RDBMS of the library, the recommendation is made by the librarians based on their experience, user's physical cards, and the relational database server logs. The suitability of such a recommendation is not knowledge-based and consumes both, a lot of time and other resources. Recommender systems are used in e-library applications to help users locate and select information and knowledge-based sources (Porcel and Herrera-Viedma, 2010). The hybrid-based recommender system Fab, part of the Stanford University Digital Library Project, which combines CB and CF recommendation algorithms, was presented by Balabanović and Shoham (1997). Mooney and Roy (2000) developed a book recommender system with a machine-learning algorithm for text categorization. A Naive Bayesian text classifier utilized information extraction to build features of books and user preferences to find the best matched books for the observed user. Later, a personalized e-library service called CYCLADES was proposed by Renda and Straccia (2005). CYCLADES provides recommendation algorithms which rely on personalized information of the organization and users' opinions in an integrated environment. Based on the research of Porcel et al. (2009), who developed a hybrid-based recommender to advise research resources in University Digital Libraries (UDL) to handle flexible information by means of linguistic labels by creation users' preferences relation, Serrano-Guerrero et al. (2011) presented a recommender engine which can incorporate GoogleWave technology in UDL.
All of the discussed e-library recommender systems are mostly using hybrid-based recommender approaches which combine CB and CF techniques (Lu et al., 2015).
E-learning and Big Data recommender systems
Recommender systems have found applications in diverse domains such as e-learning and Big Data science. Since the early 2000s, e-learning recommender systems have been increasingly popular. Based on the development of traditional e-learning systems and more than fifteen years of research studies on this topic, many practical and applicable solutions of e-learning recommenders have been developed. This type of system aims to help students choose courses, and find learning materials that they are interested in. Zaíane (2002) proposed building a software agent that uses data mining techniques, based on association rule algorithm for constructing a model which represents the behavior of a user.
A personalized e-learning material recommender system was proposed by Lu (2004). When a student's registration is obtained in the database, the system uses a computational model to identify user learning preferences which it combines with matching rules to generate a recommendation. Chen and Chao (2008) developed a system that augments traditional books with online discussion forums and learning communities. Based on their preferences, members receive messages from a web-based learning community which includes links to additional online resources. Romero et al. (2009) proposed an e-learning recommender system that utilizes Web usage mining to recommend links in a Web-based educational system. Further, a hybrid-based e-learning recommendation approach was developed by Capuano et al. (2014). This system prototype recommends learning goals generating recommendations through learning experiences and user needs. A hybrid-based approach with a three-step recommendation: mapping, utility estimation, and higher level learning goals. Extending web-based educational systems with personalized support through user centered designed recommendations was proposed by Santos et al. (2014). This study shows 502 LHT 36,3 that the building of a personalized e-learning system is a process that needs to address students' needs throughout the e-learning life cycle. Recently, De Meo et al. ( 2017) formed e-learning classes by evaluating trust and skills of learners and proposed a model aimed at managing the formation and the evolution of e-learning classes based on the information available on online social networks. Some of the relevant research studies that have linked Big Data recommenders with data analytics in other areas include (Khan et al., 2017): analytics in business (Duan and Xiong, 2015), climate changes (Lu et al., 2011), analytics of bank customers (Sun et al., 2014), smart cities (Khan et al., 2015), social media analytics (Burnap et al., 2015), healthcare analytics (Raghupathi and Raghupathi, 2014), railway management system analytics (Thaduri et al., 2015) and intelligent transportation (Chandio et al., 2015). Also, Saboo et al. (2016) proposed a time-varying effects model for handling the complexities associated with Big Data analytics for resources (re)allocation in marketing strategies. He et al. (2014) presented a social recommender system based on Hadoop (SRSH) to generate recommendations of similar users and user communities for finding friends and content. Ismail and Al-Feel (2015) proposed a Hadoop-based recommendation system for research papers. In the same year, Yao et al. (2015) developed a Big Data-based Hadoop ecosystem for facilitating data processing in healthcare services and clinical research. Wang (2016) designed and implemented a network recommendation system based on the Hadoop platform. Yi et al. (2017) presented a library recommendation method based on association rules combined with an artificial bee colony algorithm with the aim of producing personalized booklists using historical borrowing records. A multimedia recommender system for online social networks named SOS was presented by Amato et al. (2017). Integration NoSQL and relational database into the Hadoop ecosystem was the project of Rodrigues et al. (2018) with the aim to implement an e-commerce prototype system to manage credit card transactions, involving large volumes of data by using different technologies.
The structure of each of these works is different. It is important to understand that one of the lesser-explored applications of Big Data analytics lies in the smart library recommenders of an educational institution. Moreover, the use of this synergistic approach for developing a Dig Data smart library platform for the implementation of diverse data sets and sources needs to be explored. The need for research in this field can be summarized as a lack of a comprehensive survey of the use of huge amounts of data from differential sources when creating smart library applications in a learning continuum that can benefit both, educational institution and users' needs.
Big Data smart library model This paper presents a Big Data smart library model with the aim of building a system that can recommend personalized content to users with increased precision by analyzing users interests collected from multiple sources, as well as the characteristics of content from different types of data. This process enhances the quality of the recommendation scenario as treats the relationship between the libraries and education as inseparable from one another.
It can be said that there are three key strands which make this system critical for information companies which manipulate large amounts of data such as libraries. They are: switching to a scalable and elastic infrastructure, the complexity and diversity of available data, the power and value of combining different types of data.
The proposed model shown in Figure 1 contains the main components necessary for the creation of a smart library. As shown, data is collected into the smart library from multiple data sources including the LMS (Despotović-Zrakić et al., 2012), the educational institution IS, social media networks (Hargittai, 2007), online bookstore server logs (Menascé et al., 1999), and the IoT (Barnaghi et al., 2012). The primary focus of the smart library shown in 503 Big Data Figure 1 is the integration of multiple sources of different data and providing personalized recommendations to the user. The goal of the new Big Data smart library is the realization of a more precise and efficient recommender system. The interaction between different data sources in generating personalized recommendations is presented in an innovative way with the accessible system throughout data integration which achieves the most interoperability.
The smart library connects and combines data from the following sources: information that students exchange between themselves and teachers via the news forum on the Moodle platform, including selected student courses during the learning cycle; the data set from the IS of the educational institution; information collected from social media networks; the server log files from the University online bookstore that contains personalized customer information; information collected from IoT sensors, i.e., location of the printed edition of a book in the library and its usage. This data is gathered, processed and analyzed in the Big Data ecosystem which has been selected for the realization and visualization of the final contentpersonalized content-aware recommendations to the user based on his/her interests.
Figure 2 is a flowchart of the realized Big Data ecosystem for the smart library. Data sets from various sources are loaded and integrated into the HCatalog. These different data sets include: borrowed books from the University library; downloadable e-books via student accounts through the IS of the educational institution; server logs from the university online bookstore that contain information about the users' personal preferences; and the student's preferences in the current school year on the LMS platform.
With every new access of a registered user on the LMS platform, the Hadoop ecosystem analyzes the following: information about books attributes from the user's logged book usage in the library; data from the IS of the educational institution about the selected courses of the student in the current school year; the purchase history and user attributes from the University online bookstore logs; performs CF on multiple data; and at the completion of the processing of given data, the user receives a recommendation of greater interest and precision with two available choices:
(1) Choice 1: to reserve the recommended books in the library of the educational institution in which case the librarian performs a reservation for the user under his ID for a specified time period for borrowing.
(2) Choice 2: to buy the recommended books from the University online bookstore.
Data sets description
A data set of the educational institution for three years of undergraduate studies contains 160 courses, seven study programs and ~2,250 students from which approximately available books to download from a student service per student in the current school year is 5-15, depending on selected subjects and available literature, with the overall of over 10 million records. The library data set contains 470,571 users and 3,955 book titles. The University online bookstore logfile that was used contains ~3,900 evaluations, ~450 users and ~1,495 items that were taken for the calculation. For these data sets and their analysis, an important factor is coverage (Sarwar et al., 2000). Coverage is a percentage of the total number of items for which the system will generate recommendations. The basic measurement of coverage is the percentage of available items. It is the percentage of all users of the system for which the forecast is requested. The common characteristics of systems that can reduce coverage are small dimensions of similarity of users and their sampling. Logfile has a high coverage of approximately 95 percent. Maximum coverage is not provided for the following reasons: there are certain items without evaluation (purchased or rated); small number of users who have evaluated a particular item; and low similarity of a particular user in the system.
Each student is required to apply to a course on the LMS Moodle platform for the subjects he has chosen in the current school year, which may be between 10 and 12 courses. The LMS Moodle platform contains the following information: three years of undergraduate studies (seven study programs), one year of specialist studies (six study programs) and two years of master studies (three study programs)which is a total of 440 courses, ~3,750 students, ~900 teachers, and over 1,500 records per day.
Implementation and results
For the pilot implementation, the chosen platform was open-source Apache Hadoop Hortonworks. Apache Hadoop is a widely adopted and one of the most well-established Big Data software platforms that support distributed data-intensive application and the MapReduce computational paradigm that allows parallel processing of a huge volume of heterogeneous data. MapReduce and Hadoop are considered to be the most effective and efficient framework for Big Data management (Khan et al., 2017).
505

Big Data
In the implemented ecosystem, data are stored in HDFS, which provides scalable and fault-tolerant storage. HDFS detects and compensates cluster errors, splitting incoming files into blocks, and storing them redundantly over clusters. The files are divided into the blocks (64MB or 128MB each), and each block is copied on more than one node. A projected system with replica possibilities allows fault tolerancewhere the loss of one node will not destroy the file, and performancewhere any block can be read from one or more nodes while improving the data flow. HDFS in this model of the library ecosystem provides data availability by continuously monitoring the nodes in the cluster and the blocks they manage. The individual blocks are subject to checks and controls. When a block has been read, correctness is determined (whether the recorded value is correct). If the block is damaged, it has been replaced with one of its replicas from another cluster clone (Olson, 2010;White, 2012). Parallel data processing is executed by the MapReduce programming model. MapReduce provides large data sets to be shared in clusters for parallel processing. The master node assigns tasks to the slave node and then collects the results. The model thus defined has two main steps: mapthe job distribution and reducecollecting results (Dean and Ghemawat, 2008).
The Hadoop technology stack with modules of importance for the creation of a Big Data smart library is shown in Figure 3. MapReduce programming in the proposed library ecosystem is performed by the Hive module that allows for the execution of queries over large data sets and provides a mechanism for data structure projecting. The prominent feature of this layer is a structure subjected to parallelization which enables management of the large data sets in the ecosystem (Thusoo et al., 2009;George, 2011). The specified data sets were loaded into the HCatalog, ready for further processing on the management layer of The query performed on the library data (shown below) aims to display users who have borrowed specific books in a given time period in 2017, and to prepare the data set for further processing (Note: The query was translated into English for easier understanding. At the Ambari module in Figure 4, the query is shown in its original form): The search and selection of the most appropriate items were performed according to the following six steps:
(1) Step 1: the results of the executed query show book titles and the ID of a user who has borrowed the most books in the specified date range (shown in Table I).
(2) Step 2: monitoring and administration in the ecosystem are executed by the Ambari module which enables installation, management, and monitoring of the Hadoop services in the cluster. The integrated module: coordinates distributed applications, synchronizes and centralizes services in the cluster, and coordinates and monitors the workflow of mutually independent Hadoop jobs (White, 2012). The result of the executed query from Ambari module is shown in Figure 4. (4) Step 4: on the online student services of the IS of the educational institution, students could, by using their accounts, download e-books on the basis of selected subjects for the current and previous school years. The available e-books for download for the current school year per student could be in the range from 5 to 15.
(5) Step 5: the recommender system which is integrated into the University online bookstore uses item-based CF (Linden et al., 2003). At any new access by the user to the online bookstore, the system analyzes the users' purchase history, attributes, and personalized preferences to generate a list of recommendations and content of interest. The item-based CF compares each purchase made by users, ranks the items by similarity, and by combining the similar products generates a recommendation (Simović, 2014). To every user who has purchased or rated items from the University online bookstore, the system recommends three books.
(6)
Step 6: at a user's new logon the LMS Moodle platform, the proposed Big Data recommendation system processes the data from the University library, the IS of the educational institution, the online bookstore server logs, and the LMS platform to generate a recommendation. The application is developed using the Moodle API.
The algorithm calculates similarity to determine which items are most suitable for the user based on all the items in the four separate systems. The way in which the algorithm calculates the similarity of items with all others in HCatalog is shown in the following pseudo-code.
Where the ProductCatalog1 is the library data; the ProductCatalog2 is the IS data of the educational institution; the ProductCatalog3 is the University online bookstore server logs; the ProductCatalog4 is the LMS data.
Where the Selected Item is: Item borrowed from the library; or available for download from the IS of the educational institution; or purchased Item from the University online bookstore; or the student preferences in the ",6297,7047
10.1080/1941126X.2017.1304763,,Singley and Natches 2017,True,Emily Singley,"Finding the gaps: A survey of electronic resource management in Alma, Sierra, and WMS","
The objective of this study was to determine whether libraries that have implemented a next-generation library system are able to complete electronic resource management (ERM) workflows entirely within that system. A survey of librarians received 299 responses from users of Alma, WorldShare Management Services (WMS), and Sierra. Responses indicate that there are gaps in workflows and that many libraries are still performing core ERM tasks outside these three systems. The study concludes that these systems may require further development before they are able to fully support complex ERM processes.
",2017-05-08,Library Technology Reports,,"
The rapid growth of electronic resources over the past 2 decades has fundamentally transformed how libraries acquire and manage their collections. To support increasingly complex electronic resource management (ERM) workflows, libraries have had to supplement their traditional integrated library system (ILS) with multiple additional software products such as ERM systems (ERMS), link resolvers, proxy servers, and knowledge bases. The lack of interoperability between these systems has also required librarians to invent manual workarounds such as storing data in spreadsheets, shared drives, or e-mails. For ERM processes to be optimally streamlined and efficient, more sophisticated applications-designed to support every aspect of ERM workflows-are needed. One measurement of success of any application designed to support ERM workflows should be the elimination of the patchwork of systems and workarounds that have plagued electronic resources librarians for years.
In the past several years many academic libraries began retiring traditional print-resource-based ILSs in favor of next-generation systems that promise to more fully integrate electronic resource workflows. These new systems, dubbed ""library services platforms"" or LSPs by library automation consultant Marshall Breeding (2011), are designed to provide improved system functionalities that meet the current realities of complex collection management across all content formats. These new platforms can replace not only the library's ILS, but the ERMS and link resolver. As of December 2014, three LSP systems have been fully implemented by U.S. libraries: Ex Libris' Alma, OCLC's WorldShare Management Services (WMS), and Innovative's Sierra (Breeding, 2015). Two other systems, Kuali OLE and Proquest's Intota, are partially developed but not yet fully implemented (Breeding, 2015).
Although these new LSP systems have been implemented in academic libraries for more than 3 years (Breeding, 2015), there has been little written to evaluate their impact on ERM processes. In particular, there has been no study that measures whether the external tools needed in a traditional ILS/ERM environment-such as homegrown systems, spreadsheets, e-mails, and so on-are still needed. The objective of this study was to determine whether libraries that have implemented an LSP (Alma, Sierra, or WMS) are still using external tools or whether they are able to complete their ERM workflows entirely within the library services platform. It is hoped that the results of the study will be of use to any library considering LSP products, and that the results also will provide LSP vendors with valuable information they can use to continue to refine their products.
The study seeks to answer the specific research question: Do LSPs provide libraries the ability to manage electronic resources completely within the system without the need for additional tools? By surveying staff at libraries that have implemented Alma, Sierra, or WMS, the study was designed to measure what specific ERM tasks are performed inside or outside the LSP systems, and thereby capture valuable data that can help librarians determine how these newest library systems affect ERM.
Literature review

ERM in library services platforms
There has as yet been little written on how LSP systems are used by libraries for ERM. Much of the available literature consists of accounts of library migrations to LSPs or reviews of LSPs. Both of these sources, however, can provide some insight into how ERM differs in the new systems.
Wilson's 2012 survey of LSP vendors includes descriptions of electronic resource functionality within Alma, Sierra, and WMS. All three vendors claim that their systems support robust ERM functionality: Alma functions include providing cost-per-use data and usage statistics, license management, and the ability to create multiple accounts per vendor; and Sierra functions include license management, usage statistics, and streamlined workflows. Wilson notes that WMS requires purchase of a separate License Manager module to manage electronic resources but that the module includes license management, vendor management, and an integrated knowledge base. While these descriptions provide some insight into how electronic resources could be managed within LSPs, they were obtained from vendor representatives and may or may not represent the actual experience of users.
There have been multiple studies published on library migrations to Alma, Sierra, and WMS that offer limited but interesting insight into the benefits and limitations of ERMs in LSPs. The most detailed accounts are from WMS libraries-not surprising considering that WMS came to market a full 2 years before Alma or Sierra.
Several case studies written specifically about the migration process to WMS provide some useful detail on ERM in the system. Erlandson and Kuskie (2015) believe that WMS's unified framework allowed their library to streamline workflows and reduce redundancy. Except for realizing a reduction in the number of products and vendors required, however, no additional details are provided. They do note the inability of the system to harvest and store statistics or to link usage data to cost data. Dula and Ye (2012) provide a detailed account of Pepperdine University's WMS migration, complete with screenshots of the old versus the new system. They state that WMS has improved their acquisitions workflow and describe two improvements to their electronic resource processes specifically: Librarians no longer need to manually load records but can just mark titles as ""owned"" in the WorldCat Knowledge Base, and they are now using OCLC's PubGet service to automatically update serials holdings. They do note, however, that they are still ""ironing out some wrinkles"" with the PubGet service (p. 132). Bordeianu and Kohl (2015) also describe their library's migration to WMS in detail and note that OCLC provided an automated matching process for migration of electronic resource records. They also describe, however, the order functionality in WMS to be ""clumsier and less user-friendly"" (p. 287) than their previous Innovative system. The authors also explain that the new blended acquisitions and cataloging workflow caused ""discomfort and confusion among staff "" (p. 288). As with Erlandson and Kuskie (2015), the article notes the rudimentary report feature for collection analysis and statistics.
The literature on Alma implementations yields very little detail about ERM within the system. In his discussion of Purdue's experience as development partners for Alma, Bracke (2012) notes that they were able to have input into the design of Alma's ERM functions and that many of the ERM functions that had been problematic in their previous systems (Voyager/SFX) were improved. Details on specific ERM functions are not provided, however, and the conclusions were drawn prior to Purdue's production launch of Alma. Branch (2014) provides an acquisitions department's perspective of Virginia Commonwealth's Alma migration and notes that Alma has streamlined library operations, integrated print and electronic resource processes, created efficiencies, and eliminated communications silos. She also discusses how the acquisitions department's premigration workflow analysis revealed many inefficiencies such as use of paper forms, gaps in workflows, and lack of system integration. No detailed analysis of workflow post-Alma migration however, is described.
Although Persing and Moon (2014) do not explicitly discuss electronic resources in their conference proceeding on print serial workflows in Alma, they do state that Alma can streamline workflows using task lists and work orders and also point out that Alma integrates resource formats into a single database.
Two reports of Sierra implementations focus primarily on use of the system's ability to use applicationprogram interfaces (APIs) to integrate external services and do not address ERM functions. Atkinson (2012) discusses how the Orange County Library System took advantage of Sierra's API functionality to connect with external systems, and Padgett and Hooper (2015) explain how Sierra's Database Navigator Application (SierraDNA) can be leveraged to create custom APIs to improve library services.
Workflow gaps in traditional ERM systems
Although there are no studies that identify whether LSPs fully support ERM workflows, the gaps within the traditional ILS/ERMS environment are well documented.
A seminal survey conducted by Collins and Grogg (2011) of librarians and vendors identifies areas for improvement in ERM. Over a third of librarians surveyed indicate that workflow and communications management are critically important, yet poorly managed within their ERM system, if managed at all. Usage data and statistics management also fall short because silos within the ERM system do not allow for the integration of cost and usage to be captured at both the individual title and package levels and tracked over time. Feather (2007), in her analysis of Ohio State University Libraries' communication network, demonstrates that complexity not only exists with the management of the electronic resources themselves, but with the ways in which library staff communicate to each other about acquiring, accessing, and maintaining these resources. She notes the flurry of e-mail exchanges when problems arise and the staff 's tendency to ""deluge those who manage these resources with communications"" (p. 204). Her analysis reveals that traditional ILSs are unable to capture and file important internal communications (e-mail, text files, and so on) or incorporate and internalize library workflows. Rathmel, Mobley, Pennington, and Chandler's (2015) survey of electronic resource troubleshooting asked 226 participants from a wide variety of libraries to identify the tools they use to track e-resource problems. Most of respondents (96%) identify e-mail as the tool they use most often when recording, tracking, and archiving problems. Of the survey respondents using an ERMS, only 23% are using it to track electronic resource problems to some or to a great extent, while a small number of respondents who ""were about to implement an ERM hoped that the ERM would help with tracking incidents"" (p. 96). The authors conclude that the next-generation library service platforms are still too new to be effectively assessed for their data, tracking, and information-management capabilities.
Use of multiple systems and tools for ERM
The complexity and ever-changing nature of ERM has made it necessary for libraries to invest in multiple software systems as well as use manual workarounds to support ERM workflows. This proliferation of ERM tools is evidenced in the literature. As recently as 2014, a review of electronic resource software systems shows that multiple systems-including the ILS, ERMS, and knowledge bases-are required to accomplish the full range of ER tasks (Anderson, 2014).
Two surveys find widespread use of supplemental manual tools for ERM. England's (2013) survey finds that librarians store a wide range of data in different places: Administrative data supporting collection management, e-resource management, and licensing is stored in a combination of spreadsheets, e-mails, shared drives, and even paper files, depending on the type of resource and the librarian's preferences. A similar survey of academic librarians by Branscome (2014) finds that although electronic resources are predominantly managed within commercial library software systems, librarians also use more than 30 additional tools. Blake and Collins (2010) describe the complexities of ERM as akin to ""a sequence from a Kafka novel"" (p. 242). Their in-depth interviews with 10 academic librarians who manage electronic resources found that all 10 interviewees use multiple systems, including knowledge bases, ILS, link resolvers, A-Z lists, MARC records services, subscription agents, and ERMS. They also note that these commercial tools and services are not enough-librarians sometimes need to perform redundant or manual processes, such as loading records into multiple systems or manually updating records title by title.
Research method
For this study, an online survey was created using the Qualtrics survey tool and e-mailed directly to 445 library staff in academic libraries that had adopted one of the target library systems (Alma, Sierra, or WMS). Libraries were selected by searching the Library Technology Guides website (http://librarytechnology.org) for U.S. academic libraries that have licensed one of the target systems. E-mails were sent to 110 Sierra libraries, 150 WMS libraries, and 185 Alma libraries. Highly specialized institutions (such as seminaries, technical institutes, or culinary schools) were omitted, as were libraries with electronic resource budgets of less than $10,000 annually. Library staff e-mails were obtained from public-facing library websites, and job titles were used to identify staff involved in ERM tasks (e.g., ""Electronic Resources Librarian""). E-mails were sent to professional and paraprofessional library staff. The survey was e-mailed on March 10, 2016, and remained open for 4 weeks, closing on April 7, 2016.
In addition to the direct mailings, the survey was posted to library listservs during the same time period. Listservs included ERM-related lists (serialst@listserv.nasig.org, alcts-eres@lists.ala.org, http://www.eril-l.org) as well as technology and academic library lists (collib-l@lists.ala.org, lita-l@lists.ala.org). Vendor/user group lists were also included for both Ex Libris and Innovative Interfaces, the vendors for Alma and Sierra, respectively.
Survey responses were entirely anonymous, and no attempt was made to identify either the library or the respondent. The only information solicited from the respondents was the library system they used. Because of this anonymity, as well as the broad audience of some library listservs, respondents may represent both academic and nonacademic libraries.
A test survey was presented at the ACRL New England Chapter Electronic Resources Management Interest Group (ACRL NEC ERMIG) meeting on November 18, 2015. The participation of ERM librarians at the ERMIG meeting resulted in major revisions to the survey instrument, including making the survey shorter and more focused on specific ERM workflow areas.
The survey consisted of an IRB consent form, two preliminary questions to identify the participant's library system, and 18 task-related questions designed to identify whether ERM tasks were performed using the target LSP systems. The study was approved by the Boston College Institutional Review Board, and all participants were required to read a consent form at the beginning of the survey and press a button indicating their consent. Participants were then asked to identify their library system by selecting one of the target LSPs from a dropdown menu. The dropdown menu also included an option for ""other, "" and participants who selected ""other"" were asked to provide their system name in a write-in field. Each subsequent question consisted of a brief description of an ERM task (e.g., ""Record beginning and end dates of subscription"") followed by four possible radio-button responses:
a. We perform this task in our library system. b. We perform this task outside our library system. c. We perform this task both in and outside our library system. d. I don't know or not applicable.
Participants could select only one response per task. The complete survey instrument is provided in Appendix A.
ERM tasks were constructed using the TERMS (Techniques for Electronic Management) framework (Emery & Stone, n.d.) as well as the workflows of both Boston College and Tufts University. TERMS was used to provide a standard ERM workflow not specific to any institution or software system. The survey focused on the following three areas of the ERM workflow:
r Section 1: Assessing a resource for purchase (TERMS 1)-7 questions r Section 2: Acquiring and implementing a resource (TERMS 2 &3)-8 questions r Section 3: Assessing a resource for renewal (TERMS 4)-3 questions Survey tasks were developed without any prior evaluation of actual functionalities in Alma, Sierra, or WMS. Instead, the survey was developed from the standpoint that there are certain core ERM functions, as outlined by TERMS, that an ERM system should be able to support. Because of this focus on nonsystem-specific ERM tasks, and deliberate lack of prior knowledge of existing system functions, it is possible that some survey tasks are simply impossible to perform within an LSP. There is no way of knowing how a participant would have responded to this situation: he or she may have chosen the ""not applicable"" response or indicated the function was performed ""outside our library system"" or possibly skipped the question.
As tasks for different types of e-resources (databases, journal packages, single titles, and so on) can vary significantly, the survey asked participants to consider only one type of resource-a journal package. An example of a commonly licensed journal package (Oxford Journals Online) was provided for clarification.
Survey responses were analyzed using both Qualtrics and Microsoft Excel. Results were filtered within the Qualtrics survey tool to include only the target systems. Any response from participants indicating use of a library system other than Alma, Sierra, or WMS was removed prior to analysis. Results were then exported to Microsoft Excel, and totals and percentages were calculated for each of the four previously described responses. Results were generated for each task, each library system, each section area, and all library systems combined.
Results
The number of completed survey responses received for Alma, Sierra, and WMS library systems totaled 299. Alma libraries made up 43% of responses, while Sierra and WMS made up 40% and 16%, respectively. Additionally, 55 responses were discarded because they did not indicate one of the three target systems.
Assessing a journal package for purchase
The first section of the survey consisted of seven questions about ERM tasks performed when assessing a journal package for purchase. The task-specific questions were preceded by the prompt: ""When evaluating a journal package (e.g., Oxford Journals Online) for purchase, where do you perform the following tasks?"" Six of the questions asked participants where they recorded information about the journal package, including access type, number of users, cost, library fund, vendor contact information, and trial dates. One question asked where participants performed their evaluation of the journal package title list and coverage dates.
Responses from Alma users showed that many perform prepurchase assessment tasks within the system. Question 1.5, ""Record the library fund used to pay for the resource, "" had the highest percentage (54%) of respondents indicating they perform this task within Alma. The questions about recording vendor-contact information (Q1.6) and recording the type of access (Q1.1) also had high percentages performing the tasks within Alma: 41% and 39%, respectively. Recording trial start and end dates (Q1.7) had the highest percentage performing this task outside of Alma, at 49%. The task of evaluating the title lists and coverage dates (Q1.3) received the highest percentage of users (37%) needing to use Alma as well as an outside tool.
For Sierra, ""recording the library fund for purchase"" (Q1.5) also received the highest percentage of users performing the task within the system (48%). Unlike with Alma users, most Sierra users (67%) evaluate title lists entirely outside the system. Only 8% of users indicate they can evaluate title lists within Sierra. Tasks often needing both Sierra and another tool are recording the type of access (Q1.1) and recording the cost of the resource (Q1.4), with 27% and 28% of users indicating they perform these tasks both in and out of Sierra.
Prepurchase journal package assessment tasks are less frequently performed within WMS than in either Alma or Sierra. None of these tasks received more than 28% of users indicating they are performed within WMS. As with Alma, the task most performed outside WMS is recording trial start and end dates (Q1.7), with 64% of users indicating this is done outside the system. Recording the cost of the resource (Q1.4), as well as type of access (Q1.1) are also tasks many users perform outside the system -47% of respondents indicate ""outside the library system"" for these tasks. As with Alma, the task with the most users using multiple tools is evaluating title lists (Q1.3), with 25% of WMS users choosing this option.
Alma had the highest percentage of assessment tasks performed within the library system (34%), while WMS had the lowest percentage (20%). WMS had the highest percentage of assessment tasks performed outside the system (48%). WMS also had a relatively high percentage (17%) of ""don't know or not applicable"" responses, which could indicate that some of these tasks may not be applicable to the WMS system (see Table 1).
Acquiring and implementing a journal package
The second section of the survey consisted of eight questions in the area of acquiring and implementing a journal package. As in Section 1, task-specific questions were preceded by a prompt: ""When licensing and implementing a journal package, where do you perform the following tasks?"" This section included two questions identical to those asked in the first section: ""record the type of access; e.g., perpetual, rolling, one-time purchase, open access"" (Q2.6) and ""record the number of users-e.g., single user, multiple users, site license"" (Q2.7). These tasks were included in both sections because they may be performed either during the prepurchase assessment stage or during the implementation stage, depending on a given library's workflow. Questions unique to this section cover licensing, recording information such as subscription dates, title lists, administrative data, and activating journal package titles.
As with the prepurchase assessment tasks, many Alma users indicate they perform implementation tasks within the system. Journal title activation (Q2.8) had the highest percentage of any question in the survey: 91% of Alma users perform this task within the system and 0% activate journal titles outside the system. Other tasks that generated high in-system percentages for Alma were recording subscription dates (66%) and recording type of access (54%). At 40%, the task most performed by the Alma users outside the system is recording administrative data (Q2.3).
Sierra had the lowest in-system percentages for this section. In stark contrast to Alma, Sierra users generally do not activate journal titles within the system: only 9% indicated they perform this task in Sierra, while 47% perform the task outside Sierra. At 46%, the implementation task most performed within Sierra is recording subscription dates (Q2.4), and the task performed most often outside the system (62%) is storing or linking to the journal package license (Q2.2).
Like Alma, the implementation task most performed within WMS is activating journal titles at 69% (Q2.8). Also like Alma, recording administrative data is the task most performed outside the system (63%). Percentages for the tasks performed both within and outside WMS are all within a fairly close range, from 19% to 28%.
Alma had the highest percentage of implementation tasks performed in the library system (49%), while Sierra had the lowest percentage (27%). WMS had the highest percentage of implementation tasks performed outside the library system (44%; see Table 2). 
Assessing a journal package for renewal
The third and final section of the survey consisted of three questions about tasks performed when assessing a journal package for renewal. The prompt read, ""When determining whether to renew a journal package, where do you perform the following tasks?"" The three assessment tasks follow: calculating cost per use of journal titles, comparing usage statistics with other packages, and viewing usage statistics. These three assessment tasks are overwhelmingly performed outside the system for all three LSPs, and percentages are comparable across systems. Cost per use (Q3.1) assessment is performed in Alma by 5% of users, in Sierra by 4%, and in WSP by 6%. Results for comparing usage statistics with other packages, titles, or both (Q3.2) is almost identical for Alma and Sierra: 6% of Alma users and 5% of Sierra users perform this task within their respective systems. WMS results were higher, with 13% of users indicating they can accomplish this task in the system. At 16%, WMS also had the highest percentage of users indicating they view usage statistics within the system (Q3.3), with Alma and Sierra trailing at 6% and 4%, respectively.
The low in-system functionality for these renewal assessment tasks may be an indication that usage statistics functionality is either unavailable in the systems or is available but too problematic to be useful. Indeed, in 2015 Erlandson and Kuskie noted that usage statistics were as yet unavailable in WMS. Despite this, WMS had the highest percentage of renewal assessment tasks performed inside the library system (12%), while Sierra had the lowest percentage (4%). Respondents indicated that 85% of Sierra libraries perform these tasks outside the system. Alma had a relatively high percentage of ""don't know or not applicable"" responses (17%), which may also indicate a lack of usage statistic functionality within Alma (see Table 3).
Summary of all survey responses
There is little correlation between the three systems as to which ERM tasks are most frequently performed within the library systems. Only one task ranks within the top five most frequently performed tasks across all three systems: ""record package details, including full list of titles and coverage dates. "" Other similarities are limited to two out of the three systems. For example, ""activate all journal titles"" is the task most frequently performed within both Alma and WMS, but it ranks 12 out of 18 for Sierra. The task most frequently performed within Sierra is ""record the library fund that will be used to pay for the resource. "" This same task is ranked fourth for Alma users and twelfth for WMS users (see Table 4).
The tasks most frequently performed outside the LSPs are much more consistent. As previously noted, the tasks associated with assessing a journal package for renewal rank within the top five tasks performed outside the library system across all three LSPs. In addition, a number of renewal assessment tasks are performed outside the library system by Sierra and WMS users more than 50% of the time (see Table 5). 
%
Record beginning and end dates of subscription % Record the library fund that will be used to pay for the resource % Activate all journal titles-i.e., ensure titles display to users in the discovery interface(s) % . Combined responses from ""We perform this task outside our library system"" and ""We perform this task both in and outside our library system. ""
Conclusion and future study
This study explored whether libraries can manage electronic resources completely within library services platforms or if additional tools are still required. The results clearly indicate that many Alma, Sierra, and WMS libraries are still performing core ERM tasks outside their systems. The results also provide insight as to which ERM workflow areas or tasks are fairly well supported by these systems and which are not. While Alma and Sierra had comparable percentages related to in-system tasks associated with assessing a journal package, Alma appeared to outperform the other systems in the area of acquiring and implementing a journal package, scoring the highest percentages in seven of the eight questions. All three systems performed equally poorly in the area of assessing a journal package for renewal. These results indicate that usage statistics and cost-per-use data are not yet useful within LSPs, and that this is the workflow area with the greatest need for improvement.
The gaps remaining within LSP systems become especially evident when results from the ""we perform this task outside our library system"" are combined with those from ""we perform this task both in and outside our library system. "" Both responses indicate that some additional tool or system is being used to perform the task. Combined results indicate that many libraries still rely on outside systems either fully or in conjunction with their LSP when performing ERM tasks (see Table 6).
Future study
As this survey only assessed a subsection of ERM workflows, it provides only a partial look at system gaps. The survey questions were narrowly focused on journal package tasks and only on tasks that take place during prepurchase evaluation, implementation, and assessment for renewal. Further study is needed to determine what gaps exist for other types of resources (e.g., electronic books, databases, open-access resources, and so on) as well as other areas of the ERM workflow.
Another possible area for further exploration is examining the gaps identified in this study in greater detail. This study discovered which tasks are or are not being performed in LSP systems but did not attempt to learn why. Additional research could include in-depth interviews with the electronic resource librarians who use these three systems to address questions such as the following: Which specific additional tools are being used and why? Are tasks performed externally due to a complete lack of functionality within the systems, or are functions available but in need of improvement?
This study also did not compare the gaps in LSPs with gaps that had previously existed in the traditional ILS/ERMS environments. There is need for a comparative study that explicitly measures any improvements gained by moving to these new systems.
This research suggests that ERM remains a complex process that is, as yet, too daunting to encompass within any one software system. It hints that electronic resources workflows may still involve convoluted manual workarounds and a patchwork of tools and that LSPs need further development. It is possible that LSPs, as single, monolithic systems, are not the answer, and that truly streamlined, customized, and integrated workflows will not be possible until the next iteration of library systems. Until either LSPs address the gaps in ERM workflows, or new systems are developed to replace LSPs, however, we will not know whether ERM can be truly automated or whether it will remain a complex, clumsy, and arduous endeavor. 
Section 2: Acquiring and implementing a journal
When licensing and implementing a journal package, where do you perform the following tasks?
We perform this task in our library system
We perform this task outside our library system We perform this task both in and outside our library system ",6166,6890
10.1155/2022/2856574,,Wang 2022,True,Juan Wang,Personalized Information Service System of Smart Library Based on Multimedia Network Technology,"
Scientific and objective book quality evaluation and research on the value of books in the smart library are conducive to improving the reading needs of readers. However, it is difficult to obtain the important coefficient of the subjective and objective weight of the personalized information service index of the smart library at present, which has the problem of poor system performance.erefore, a personalized information service system of the smart library based on multimedia network technology is designed. In multimedia network technology, the data collector module, data retriever module, and data memory module of the system hardware are designed. e Ethernet interface is connected through three buses to ensure the efficient operation of the system; according to the hardware, the software flow is introduced, which is divided into UI layer, logical business layer, and data access layer. On this basis, the application program is designed, all operation instructions follow the association rules, and the single-chip microcomputer is connected with the voice chip through the SPI serial port, so as to complete the design of the personalized information service system of the smart library. e experimental results show that the average absolute deviation of the designed system is small, the comprehensive performance is strong, and the update delay is kept within 0.6 s, which can improve the work efficiency.
",2022-09-07,Modern intelligence,,"Introduction
With the continuous development of society in recent years, the smart library, as the ""concentration place"" of books, is also the best place for readers to acquire knowledge and information in depth [1,2]. Books make an important contribution to society and people's acquisition of cultural knowledge and are also an important product of human civilization and development [3,4]. Titles were there was a substantial growth in recent years, at the same time there were many behind the rapid growth of performance driven by economic interests of hidden trouble, the main performance for the quality of books has plummeted, and similar content in the books published, and follow the repeat publishing phenomenon everywhere so that the books are of variable quality [5]. Due to the continuous development of information and communication technology and the wide application of the Internet, the personalized information service system management of books in smart libraries has become one of the main themes of the current library management reform and development [6,7]. In this case, it is necessary to effectively design the personalized information service system in the smart library and establish a complete and scientific book quality evaluation index system in the smart library to identify the book quality in the smart library, so that more excellent books can stand out [8,9].
Because the design method of the personalized information service system of the smart library has far-reaching development significance, it has also become a hot topic studied by experts and scholars, and has attracted extensive attention. Reference [10] proposes the role of media resource center in bringing novelty and creativity to Nigerian school library services.
is article reveals the novelty of the functions of the center, especially in training qualified personnel to serve as librarians, media experts, reading teachers, and other school media personnel in Nigeria's preschool, primary, and postprimary education institutions, as well as organizing programmes related to children's reading and library use in Nigeria. Reference [11] proposes to evaluate the impact of traditional and digital marketing practices on university library services and resources. Determine how digital and traditional marketing methods can raise awareness among users and make better use of library services and resources. Understand the role of library staff in university library marketing practice and technology in digital and traditional ways. Based on quantitative research methods, cross-sectional survey research methods were used for data collection.
Although the above methods have made some progress, the application of multimedia network technology is not particularly sufficient, so the design of the intelligent library personalized information service system is based on multimedia network technology. Multimedia network technology refers to the text-based data communication and the text-based communication technology, including file transfer, e-mail, remote login, network news, and e-commerce. e technology is a comprehensive, interdisciplinary technology. It integrates the computer technology, network technology, communication technology, and a variety of information science technology achievements and has become the world's fastest developing and the most dynamic high-tech. In multimedia network technology, the data collector module, the data retrieval module, and the data storage module of the system hardware are designed, and the Ethernet interface is connected by three buses to ensure the efficient operation of the system. According to the hardware analysis software process, it is divided into UI layer, logical business layer, and data access layer. On this basis, the application program is designed, all operation instructions follow association rules, and the MCU is connected with the voice chip through the SPI serial port, thus the personalized information service system of the smart library is designed.
e application in the personalized information service system of the intelligent library has remarkable effects, and the designed system has high performance.
Personalized Information Service System of Smart Library under Multimedia Network Technology
In multimedia network technology, the data collector module, data retrieval module, and data memory module of the system hardware are mainly designed, and the Ethernet interface is connected through three buses to ensure the efficient operation of the system; according to the hardware, the software process is introduced, which is divided into UI layer, logic business layer, and data access layer; on this basis, the application program is designed, all operation instructions follow the association rules, and the single-chip microcomputer and the voice chip are connected through the SPI serial port, thus completing the design of the personalized information service system of the smart library.
Hardware Design of the Personalized Information Service
System of Smart Library. e basic components of multimedia network technology are: CD-ROM with optical drive, which is an important symbol of multimedia network technology system; It has the functions of A/D analog-todigital conversion and D/A digital analog conversion, which can convert the analog signal and digital signal of voice to each other, so that the multimedia network technology has high-quality digital voice function; display with high definition [12,13]. Under the multimedia network technology, the establishment of the personalized information service system of the smart library is the basis to ensure better service of resources. It can truly provide resources according to users' needs and enable users to obtain the richest resources, if allowed [14,15]. e hardware function of the designed intelligent library personalized information service system is to use the client to feed back the data resources that customers need to retrieve and use the data collector module to collect library resources, and the data integrator will integrate and process the collected resources.
e data searcher module will check and search the integrated data resources and finally feedback to the user by the data memory module [16]. e hardware structure of the personalized information service system of the smart library is shown in Figure 1.
e system hardware in Figure 1 applies multimedia network technology to share the resources of Smart Library.
e system hardware adds a large number of embedded products to support various cloud classroom systems, improve teaching quality, and ensure students' learning efficiency [17].
Data Collector Module.
e personalized information service system of the smart library designed in this paper can realize high-quality positioning collection, and compress the collected audio and video to meet the low-power requirements of the system [18,19]. e internal chip of the collector is the GD32F103RCT6 chip, which was launched by Shenzhen Zhuocun Electronic Technology Co., Ltd. and has the characteristics of multifunctional multimedia application, which is convenient for collection, compression, and transportation [20,21]. e GD32F103RCT6 chip consumes very low power. Generally, the chip can maintain normal acquisition work above 5 V voltage. e acquisition speed under low voltage can also reach 10000 MIPS, and the acquisition speed under high voltage is higher [22,23]. e acquisition capability fully meets the acquisition requirements of large-scale library digital resources, and the acquisition accuracy is much higher than that of other chips, with an accuracy of 10 bits or even more [24]. e power supply mode of the data collector module is dual power supply of internal and external interfaces, and the internal interface is connected to the external interface to ensure the continuous input of voltage [25].
e collector is automatically connected to the wireless network. After signal coding and synthesis, it is transmitted to the wireless network data terminal, stored in the terminal, and recorded on the hard disk. e collector structure is shown in Figure 2.
When collecting video signals, the collector in Figure 2 selects SCLK as the clock to record all signals, and the input video is one video. When collecting GPS signals, data transmission signals, RX data reception signals, and asynchronous signal interfaces are used as power interfaces, and the maximum baud rate supported is 2.56 Mvyte/s. When collecting, it is necessary to connect and receive personalized information service system signals. e positioning accuracy of GPS signals is very strong, and the error rate is less than 4 m, which can locate high-speed mobile signals [26,27]. e signal collector is connected by PCI and HPI, and the bus interface is an Ethernet interface. e transceiver can send and receive 10 M∼100 M personalized information service data. In order to realize the simultaneous transmission of service data network, a wireless module is added in the system.
Data Retriever Module.
e data searcher module is the core part of the hardware of the personalized information service system of the whole Smart Library. e searcher adopts the high-precision BAM6 chip developed by TRM Company in Norway. TRM fully analyzed the shortcomings and advantages of the previous two generations of BAM5 and BAM4 when developing the BAM6 chip.
erefore, this chip is significantly better than the previous two generations of chips in working time and retrieval efficiency [28]. e working delay time of the BAM6 chip is very short, only 10 minutes μs. e rapid mode is used to control the retrieval speed, so the speed is very high. ere are many retrieval interfaces inside. When the searcher runs, the interfaces will be connected together and work at the same time, which greatly reduces the working time, strengthens the retrieval ability, and retrieves more library digital resources [29]. e BAM6 chip requires the working voltage to be above 20 V and the data packet length of the chip is 45 B. It works under the multimedia network technology and the data retrieval rate is 80 Mb/s. However, in the low-voltage state, the chip will enter the automatic sleep mode and cannot start working.
Data Memory Module.
In order to improve the storage efficiency of the hardware memory of the personalized information service system of the smart library, a flash memory with large storage range and low manufacturing cost is selected. A single-chip microcomputer is added to the memory to increase the storage capacity and reduce the floor area. e memory structure is shown in Figure 3.
ere are three buses outside the memory, each bus is connected with an Ethernet interface, and different interfaces are connected with different signals [30]. e three wire buses are as follows:
Bus 1: connect signals in I/O mode, input and output in two-way way, and realize two-way exchange of data.
e remark mode is I/O. Bus 2: connect the signal in OUT mode, output the signal in one-way, and control the signal to enter. Bus 3: connect the signal in BUSY mode, and the input mode is busy signal input.
Software Design of the Personalized Information Service
System of Smart Library. In order to realize the software design of the intelligent library personalized information service system and complete the data receiving and processing, it communicates with the system terminal host computer through the wireless sensor network RS232 serial communication protocol of multimedia network technology. e gateway of the intelligent library personalized information service system uses socket process communication combined with a multithreading mechanism to complete the communication with the intelligent library personalized information service system terminal host computer [31]. e software structure of the personalized information service system of the smart library is shown in Figure 4.
As shown in Figure 4, the specific contents of the system software are as follows:
(1) e UI layer of the intelligent library personalized information service system based on multimedia network technology, that is, the user interface of the intelligent library personalized information service system, is mainly used to realize the organic interaction between the intelligent library personalized information service system and readers, including the design of book borrowing, return, query interaction, reader information management, user interface attribute configuration, and so on. e UI layer is connected with the logical business layer (UI layer service providing layer). e UI layer of the intelligent library personalized information service system based on multimedia network technology designed in this paper includes two parts: RFID tag reading and writing component and user page component using the ISO/IEC14443 protocol. e RFID tag reading and writing component using the ISO/IEC14443 protocol is essentially the interface part between the PC and the RFID RF terminal to realize the data reading and writing function of the RFID RF terminal. e design of UI layer interface components mainly includes the book borrowing management module, reader information management module, book ID information management module, and system parameter attribute setting [32]. (2) e logical business layer of the intelligent library personalized information service system based on multimedia network technology: the main function is all logical operation and processing of readers and managers. (3) Data access layer of the intelligent library personalized information service system based on multimedia network technology: it provides services for all logical operation and processing processes of readers and managers, and is mainly responsible for data access of the intelligent library personalized information service system database.
Application Design.
e personalized information service system of the smart library connects the single-chip microcomputer with the voice chip through an SPI serial port, and all operation instructions must follow the association rules [33,34]. All serial data transmission must be kept at a low level, and a high level should be used between two instructions. In order to ensure system security, a user authentication stage should be added in the process of application programming, and the system reminder function can be realized only through authorization.
In the implementation process, the layer-by-layer search iterative method is used to control the reminder function by calculating the confidence of association rules. e specific calculation contents are as follows:
In formula (1), A a represents the total amount of all data in the personalized information service data item set, B b represents the total amount of all data in the item set contained in the smart library database, and C c represents the total amount of support corresponding to each subserver.
In the application structure, the storage and playback of reminder instructions are controlled by pressing the key to realize the reminder function. erefore, the elimination of key jitter and key response are introduced [35,36]. e key jitter time is determined by the equipment performance. e software delay is used to detect whether the key is really pressed, which effectively avoids the key jitter time.
In order to reduce the error, set F to represent the end value of the counter, so as to obtain the key cycle as follows:
(
In formula (2), C s represents the initial value of counting and T z represents the clock cycle of a single-chip microcomputer. According to the key cycle shown in formula (2), the reminder time can be accurately mastered to avoid the delay problem of traditional system reminder.
When the system is started, the relevant reminder information is displayed on the display screen, and the prestored information is selected through the relevant keys of the keyboard. Under the multimedia network technology, the reminder instruction can be issued through the singlechip microcomputer controller of the hardware module to complete the design of the application program of the personalized information service system of the smart library.
Experimental Study
In order to test the actual working effect of the personalized information service system of the smart library based on multimedia network technology, an experiment is designed.
e experimental environment is Intel(R) Pentium(R) 4, the CPU is 2.40 GHz, the memory is 1024 MB, and the operating system is Microsoft Windows XP SP3. Other experimental parameters are shown in Table 1.
According to the contents of Table 1, this paper uses the system in this paper, the system in reference [10] and the system in reference [11], respectively, to carry out the comparative experiment of intelligent library personalized information service system design.
ree different methods are used to compare the accuracy of the personalized information service of the smart library. e average value of absolute deviation P JC is used as the evaluation index of the accuracy of the personalized information service system. e average value of absolute deviation, i.e. the average deviation, refers to the average value of the absolute deviation of each measured value. e smaller the value, the better the effect of the personalized information service of the smart library. e calculation method is shown in
In formula (3), |S| represents the book quality evaluation data set, U i represents the actual scoring value of readers on item i Book personalized information service scheme in the book quality evaluation data set, and W i represents the predicted scoring value of readers on item i scheme given by the book personalized information service system. Compare the average absolute deviation (%) of three different methods for personalized information service of the smart library, and the results are shown in Figure 5. rough the analysis of Figure 5, it can be seen that the average absolute deviation of the personalized information service of the smart library using the system in this paper is lower than that of reference [10] system and reference [11] system. is is mainly because in the process of designing the personalized information service system of the smart library using the system in this paper, high-quality positioning acquisition can be realized, and the collected audio and video can be compressed. Get the important coefficient of subjective and objective weight of each evaluation index of the personalized information service in the smart library, so that the average absolute error of the personalized information service in the smart library using this system is low.
In order to further verify the effectiveness of the designed system, it is necessary to comprehensively analyze the effects of different performance of the system, and compare and analyze the storage performance, operation difficulty coefficient, expansion performance, security performance, and integration performance, respectively. e comparison results are shown in Table 2.
It can be seen from Table 2 that the storage performance of the system in this paper is good, the operation difficulty coefficient is small, the expansion performance and security performance are good, and it has the ability of data sharing and integration at the same time. While the storage performance of reference [10] system is general, the operation difficulty coefficient is large, the expansion performance is good, but the security performance is poor, it does not have data sharing, and the integration ability is also poor. Reference [11] system has general storage performance, large operation difficulty coefficient, and poor expansion performance, but it has good security, data sharing, and general integration ability. Based on the traditional system, the designed system improves the storage performance of the system to make the storage performance of the designed system better. Continue to explore the effects of the personalized information service system of the smart library based on multimedia network technology. e working frequency of the system is detected every 60 s, and there is no waiting time when readers query relevant book information. e time difference between the actual database update frequency and the corresponding book information update frequency of the designed system is defined as the update delay (s). e comparison results are shown in Figure 6.
According to Figure 6, compared with reference [10] system and reference [11] system, the book information refresh frequency of the personalized information service system of the smart library based on multimedia network technology designed in this paper can meet the practical requirements of the smart library. e system update delay is kept within 0.6 s, which can effectively realize multiple links such as on-board book inventory, return to the shelf, and query circulation, and greatly improve the work efficiency of the system.
To sum up, the designed system has good performance and realizes the intelligent inventory function and personalized service of personalized information service of the University Smart Library. With the help of the application of the personalized information service system of the University Smart Library, the work progress of the library and the work efficiency of librarians are greatly improved, which can better serve readers.
Discussion and Analysis
In terms of multimedia network technology, aiming at the development of personalized information service of the smart library, the countermeasures are as follows:
Improving the Management Mechanism of Personalized
Information Service. Due to the lack of a macro-control mechanism in China's libraries, in order to coordinate and develop libraries and realize the common knowledge and sharing of information resources, China's higher education document guarantee system has been established. Strengthen the cooperation between the system and its member museums, improve the service level of higher education, and give full play to the maximum social and economic benefits. Strengthening the service consciousness and management means alone cannot meet the requirements of improving the service quality of the library. e cooperation between departments, the adjustment of structure, and the unification of the evaluation system are indispensable. Master the information needs of users, track and evaluate the service quality, pay attention to the feedback of user satisfaction, adopt incentive mechanism to stimulate the initiative of service personnel, and provide users with comprehensive and high-quality personalized information services. To develop the library personalized information service, the improvement of library personalized information service mechanism plays a very important role in the development of the smart library.
Strengthening the Construction of Information Service Resources

Strengthening the Content Construction of Information
Resources. Establish a professional navigation database and a characteristic database. In order to cope with the rapid expansion and disorder of network information resources, the smart library should collect, sort, and classify the Reference [10] system
Reference [11] system network information resources according to the specialty according to the characteristics of teachers and students, for the purpose of meeting users' learning and scientific research, highlight the professionalism of subject navigation, and establish professional navigation links. For the construction of information resources, the smart library is based on the careful investigation of databases at home and abroad. According to the professional setting of the university, the focus of scientific research and the information needs of users, combined with the library's collection resources, the smart library establishes a characteristic database according to a certain discipline, specialty, or local characteristics.
Strengthening the Organization and Integration of Information Resources.
Resource integration and service integration are the main contents of information resource integration. Resource integration refers to the integrated information system formed by classifying and sorting the existing information resources. Service integration needs to build a personal information database, which includes the basic information of users and feedback information of users on services. Compared with traditional information services, in the organization of information resources, the new requirements of personalized information services are: the content should be targeted, clear, easy to understand, open, and flexible; in the navigation system, the classification should be detailed and reasonable; the user interface should be friendly; the evaluation ability and information navigation should be strong; and the cross-platform seamless connection of information resource content should be realized. e library needs to sort out and classify the discrete electronic resources and establish a professional navigation database with a friendly interface, powerful functions, and comprehensive according to the needs of users.
Strengthening the Co-Construction and Sharing of Information Resources.
In the school, each department will establish its own library reference room according to its own situation, which overlaps with the resource construction of the library, resulting in a low utilization rate and a waste of material and human resources. erefore, strengthening the cooperation between reference room and library and jointly building and sharing information resources in the school is the main measure of information co-construction and sharing. Smart libraries can give full play to their respective advantages of document resources, which not only avoids the repeated construction of resources but also enriches the acquisition of users' information resources.
Changing the Service Concept and Improving the Quality of Service
Personnel. e establishment of a humanistic care service concept of ""people-oriented, user-first"" is very beneficial to the active and effective development of personalized information services in the library. ""People-oriented and service-oriented"" refers to putting the needs of users in the first place, taking the completion of this goal as the fundamental purpose of the work, all for the sake of users and convenience, and providing users with maximized and optimized services. To meet the service needs of users, service personnel should have strong information analysis ability and language ability to answer various questions for users; we should broaden the scope of knowledge and strengthen learning, especially the relevant knowledge of library and information; master skilled information skills; be able to analyze, integrate, and summarize information; and master the sources and retrieval methods of various types of information. Only with these can service personnel improve the service level and provide users with obvious personalized information services. ""Subject Librarian"" is a kind of special personalized information service talent arising from the problems of low technical level, single knowledge, and poor information analysis ability of library service personnel. Subject librarian refers to the person that the library sets up to establish contact with a certain department or discipline specialty as the counterpart unit, build a bridge between the department, discipline specialty, and the library, communicate with each other, and actively collect and provide Reference [10] system
Reference [11] system document information services for users. is kind of service personnel needs to have a certain level of foreign language, be familiar with library business, be skilled in computer operation, have high educational background and professional title, profound cultural heritage and strong language ability, and be able to provide powerful help for teaching and scientific research.
Strengthening the Construction of Personalized
Information Service Systems. To develop a personalized information service system, the library should have both rich system resources and rich system functions, and be able to provide comprehensive services. e designed system interface should be concise, intuitive, and clear-cut, and users should be able to make personalized customizations; it should be able to realize automatic integration with other resource systems, which not only saves users' time but also reduces users' use burden; and can protect user privacy. e measures to improve the personalized information service include the following: establishing a professional navigation system, building an effective information space, systematically organizing relevant information resources, providing users with a good retrieval interface, information customization, and information push. e system can provide users with selection services. Users can also select and manage information to realize their interactive functions. e problems encountered in use can be solved in time to improve the service quality.
To sum up, under the multimedia network technology, the smart library will encounter many problems in the process of building personalized information services. e smart library needs to improve its personalized service management mechanism, strengthen the construction of information resources, strengthen the construction of personalized information service system, and strengthen the research and training of user needs. While changing the service concept, library service personnel should also pay attention to the improvement of their own quality, improve the service quality, and make the library's personalized information service develop continuously.
Conclusions and Prospects

Conclusions
(1) e average absolute deviation of the intelligent library personalized information service system based on multimedia network technology is lower than that of the reference system, which has a good effect. It can realize high-quality location collection and get the important coefficient of subjective and objective weight of each evaluation index of personalized information service a in smart library. (2) e system in this paper has good storage performance, low operation difficulty coefficient, good expansion performance, and security performance, as well as data sharing and integration ability.
(3) e book information refresh frequency of the personalized information service system designed in this paper can meet the practical requirements of the smart library, and the system update delay is kept within 0.6 s, which effectively improves the working efficiency of the system. With the application of the personalized information service system of the university intelligent library, the progress of library work and the work efficiency of librarians have been greatly improved, and it can serve readers better.
Prospects.
e personalized information service system of the smart library realizes the unattended of the library, uses the accurate positioning of multimedia network technology and the realization of self-help borrowing and returning books, reduces the labor intensity of the staff, and enables them to devote more energy to scientific research. However, in order to serve college teachers and students more perfectly, there are still a lot of problems to be solved in order to explore a more intelligent, personalized information service mode.
e further development of the personalized information service system of the smart library can use the mobile device library platform for subsequent docking on the basis of this information service system and realize a more efficient and intelligent library information service system through the construction of the mobile platform. For example, it can realize that the mobile device can receive an update of all library information, the system can automatically urge the return of books, etc. Interlibrary communication and information sharing can be achieved by relying on the mobile platform to improve the quality of interlibrary communication and information sharing. Relying on the advantages of multimedia network technology, we can popularize the installation and popularization scope of book borrowing and returning terminals, open library resources, break the constraints of time and space, and realize the possibility of systematic remote management and real-time circulation of books through the push of terminal points, so as to improve the circulation times in the book cycle.
e development of technology and the popularization of networks promote the development of library cause. In multimedia ne",5688,6220
10.1108/lht-11-2017-0254,,,False,Yi Xie,An IoT-based risk warning system for smart libraries,"
Purpose -When integrating smart elements offered by emergent technologies, libraries are facing the challenges of technological renovation and maintaining their operation using emerging technology. Given the importance of smart library, new technologies are needed in building new libraries or renovation of existing libraries. The purpose of this paper is to propose a risk warning system for library construction or renovation in the aspect of risk management. Design/methodology/approach -The proposed Internet of Things (IoT)-based system consists of sensors that automatically monitor the status of materials, equipment and construction activities in real time. AI techniques including case-based reasoning and fuzzy sets are applied. Findings -The proposed system can easily track material flow and visualize construction processes. The experiment shows that the proposed system can effectively detect, monitor and manage risks in construction projects including library construction. Originality/value -Compared with existing risk warning systems, the proposed IoT-based system requires less data for making dynamic predictions. The proposed system can be applied to new builds and renovation of libraries.
",2019-11-18,Library Hi Tech,Emerald,"Introduction
Emergent technologies bring libraries innovative challenges, concerns and opportunities for development (Bayani et al., 2018). In digital era, libraries' role changes from a provider of information to a facilitator of learning (Holmgren, 2010). Accordingly, traditional libraries are converting into learning commons, aiming for providing users seamless learning with holistic spaces that combine research, technology and other services (Accardi et al., 2010). By incorporating Internet of Things (IoT), particularly radio frequency identification device (RFID), and the introduction of the concept of building automation (Pang, Xie, Zhu and Luvisotto, 2018;Pang, Zhu, Xie and Luvisotto, 2018), libraries are becoming smart (Bayani et al., 2018). Based on wireless sensors, RFID tags and wireless sensor networks (WSN) (Li, Zhao, Wang, Zhang and Li, 2014), smart libraries convert their process into an intelligent system and perform book monitoring, registering, inventory control, self-checking in/ out and detecting automatically as well as in an efficient manner (Bayani et al., 2018). In addition, IoT can help libraries manage lighting, security systems, fire-fighting systems (Bhardwaj et al., 2011), real-time messaging service (Bayani et al., 2018), air monitoring (Peng et al., 2015) and ventilation (Wang and Wang, 2013).
Many libraries choose to renovate their traditional space into learning commons, making their new learning commons smart whereas remaining open, providing patrons with access to the collections, offering working environment for library employees, as well as enabling construction personnel to work in the buildings (Kruger and Barstow, 2009). As such, security is a major concern in library buildings under renovation given the uncertain and unexpected risks. The security challenges in renovation of libraries are similar to those in other construction projects, which are high-risk due to many factors, such as unpredictable geological and hydrological conditions, complex construction equipment and methods and third-party impacts on the surrounding environment (Choi et al., 2004). So far, the complexity of the risk management has been well discussed in the existing literature. Many technologiesbased systems have been proposed to monitor and control risks on construction sites (Alaeddini and Dogan, 2011;An and Song, 2012;Jiang and Ma, 2017;Sousa and Einstein, 2012;Zhou and Zhang, 2011). However, most of them are limited to specific risk management and cannot detect risks under complex circumstances. Real-time information sharing and dynamic scheduling are not supported either. Accordingly, this paper proposes a risk warning system (Yang, Li, Ji and Xu, 2001;Yang, Li, Xie and Xu, 2001) based on IoT technologies with fuzzy algorithms. Case-based reasoning (CBR) ( Jiang et al., 2014;Yang, Li, Ji and Xu, 2001;Yang, Li, Xie and Xu, 2001) are applied for data processing and analytics. Experiments have been conducted to test the proposed system. The results show that the proposed system can effectively detect, monitor, and manage risks in construction projects. Given the similarity of risk issues in new builds and renovation of libraries and other construction projects, the proposed system is expected to be applied to libraries as well.
The remainder of the paper is structured as follows: Section 2 provides background about IoT, smart libraries, security and risk issues in construction projects. Section 3 presents the risk warning system. Finally, Section 4 includes a summary and conclude this paper.
Background

Internet of Things
As the representative of new technology, IoT attracts a lot of interest from researchers (Assarzadeh and Aberoumand, 2018;Bi et al., 2014Bi et al., , 2017Bi et al., , 2018;;Cai et al., 2014;Cheng et al., 2018;Fang et al., 2014;Gürdür and Asplund, 2018;Lai et al., 2017;Li et al., 2018;Lu, 2018;Mao et al., 2016;Srinivasa et al., 2018;Xiao et al., 2014;Xu et al., 2014;Yang and Xu, 2018). IoT connects various objects via the IoT (Kim, 2017;Li, Oikonomou, Tryfonas, Chen, and Xu, 2014;Li, Zhao, Wang, Zhang and Li, 2014;Liu et al., 2017;Wang, Bi and Xu, 2014;Xu et al., 2014Xu et al., , 2018)). By combining these connected objects with automated systems, it is possible to gather and analyze real-time information among people and people, people and things, and things and things (Xue and Xu, 2010;Liu, 2012).
IoT involves architecture, sensor/identification, coding, transmission, data processing, network and discovery (Ning and Wang, 2011;Wang et al., 2015). The main technologies adopted in IoT include RFID, near-field communication (NFC), low-rate wireless personal area networks (LR-WPANs), Bluetooth, Zigbee, wireless fidelity (Wi-Fi), worldwide interoperability for microwave access (WiMAX), mobile communications, WSN (Ahmadi et al., 2018). Given the capabilities of these new technologies, IoT can be applied to collect, analyze and share safety information without affecting on-site activities of construction. According to Ding et al. (2013), IoT technologies, such as RFID, global positioning systemGPS, and ultra-wideband could be used for identifying unsafe status of structures and environments as well as unsafe behavior of human workers in real time.
IoT creates smart infrastructures by connecting unique identifiable devices with RFID in the existing configuration (Bayani et al., 2018). Therefore, it has been applied in smart cities, online-smart business, smart power consummation, smart ecology and environment, security, monitoring and emergencies, transportation, industrial automation processes, educational systems and smart libraries (Bayani and Vilchez, 2017).
Smart libraries
Even though collections of libraries move online at the digital age, libraries still need innovative ways to use their physical space to serve users' needs as a relevant, comfortable, and user-focused space (Norton et al., 2013). As traditional space is renovated into learning commons, many individual study spaces with convenient access to computers as well as group study spaces with convenient collaboration stations and large-screen monitors are built. In addition, strong and stable wireless access, modern furniture and comfortable seating are provided in libraries (Norton et al., 2013). Usually, libraries remain open during their renovation for continue offering services to patrons.
IoT improves many contexts of the society, including traditional library system (Bayani et al., 2018;Liang and Chen, 2018;Wang et al., 2018). Although traditional libraries apply RFID barcodes in their administrative processes, they are not smart because data collected for virtual objects and physical objects are not connected and they cannot be used for generating detailed map related to the real situations and decisions (Bayani et al., 2018).With the help of IoT technologies, libraries become smart and leverage networks of intelligent sensors and actuators connected to the internet, providing a vast range of high-level services to users and librarians (Antevski et al., 2016). According to Hossain et al. (2018), the massive literature data can be loaded into library management system through RFID technology. Librarians can acquire book information data by using RFID handheld reader for scanning RFID tags of books, whereas users can conveniently and easily find their desired books through intelligent navigation retrieval terminals. In addition, NFC tags can be used for tracking books and users (Mrunal et al., 2014;Nisha et al., 2017), alerting book renewal and return (Srinivasan and Vanithamani, 2013).
Other than locating books on bookshelves, IoT can be adopted in management of library building facilities, such as setting lighting, security systems and fire-fighting systems (Bhardwaj et al., 2011), performing air monitoring (Peng et al., 2015) as well as ventilation (Wang and Wang, 2013). In a word, work quality and efficiency in libraries will be greatly improved with IoT technologies.
Risk management in construction projects
Safety is a major concern in construction industry (Hossain et al., 2018). Many construction projects require huge investment, have long time frame, and require special technologies. Construction sites are typical large-scale systems that might cause enormous potential risks for public safety (Shi et al., 2012). With the fast development of urbanization, many construction projects have been rapidly planned and implemented around the world in recent years (Li, 2013(Li, , 2018)). However, partly due to the inefficient management of information in construction projects, accidents in construction are rising. This problem has attracted a lot of attention from researchers and practitioners (De Jong et al., 2010). Many factors, such as uncertainty of project environment, low effectiveness of risk monitoring, lack of safety management control and the lack of geological survey are associated with safety accidents.
To address the challenges of safety in construction projects, researchers have introduced various techniques and tools to prevent and control safety accidents, such as safety check list, comprehensive fuzzy evaluation method (Feng and Xu, 1999;Xu, 1990) and decision trees (Duan and Xu, 2012). Some researchers try to analyze the causes and consequences of risks based on the Man-Machine-Material-Method-Environment (5M) model. For instance, Li and Lu argue that personnel's behavior and skills have a great impact on construction risks. They find that a well-trained work force can dramatically reduce construction risks.
In order to address the challenges of risk management, researchers also introduced various models and tools to detect, evaluate and manage risks in construction (Wang and Wang, 2009;Yang et al., 2010;Peng and Mou, 2010;Song et al., 2011;Chen and Xie, 2012). For example, Zhou and Zhang (2011) propose a fuzzy evaluation method which can be used to evaluate multiple construction risks. Based on failure mode effects and criticality analysis, An and Song (2012) establish a working framework including the construction safety risk assessment and emergency measures to strengthen the resource planning of site safety management. Alaeddini and Dogan (2011) establish a quality risk assessment model combined with fuzzy logic method to evaluate the occupational health and safety of civil engineering construction personnel. Sousa and Einstein (2012) propose a construction risk assessment method based on geological prediction model and construction decision model. Jiang and Ma (2017) discuss the comprehensive similarity in metro construction projects and propose a reasoning model, which can improve the intelligent level of the risk accidents in metro construction.
However, these approaches do not fit well for dynamic risk management in construction projects (Alaeddini and Dogan, 2011). Recognizing the weakness of these approaches, researchers began to cast their eyes on new technologies. So far, many studies have found that construction safety management can benefit a lot from new technologies such as IoT (Peruzzini and Stjepandić, 2018), RFID (Wang, Bi and Xu, 2014;Wang, Xu, Bi and Xu, 2014) and GPS (Qian and Lin, 2016). Realizing the advantages of IoT, scholars have adopted IoT technology to monitor safety, position risks and manage personnel and equipment (Luo, 2013). For example, Yang (2014) integrates IoT, cloud computing and network communications to monitor real-time security situation of underground project. Yang et al. (2017) propose an IoT-based system which can monitor construction site and surrounding areas.
According to Fox (2017), 25 new libraries were built across the USA between July 1, 2016 and June 30, 2017. Total cost of these new libraries is about $194m with an average cost of $8m. In specific, the total spending on equipment in these new libraries is $20m with an average of $0.84m. During the same period, 42 libraries completed their renovation or addition projects in the USA. The renovation costs about $215m in total with an average of $5.25m. Regarding the spending on equipment, the total spending is about $23m and the average is $0.57m. Despite the huge spending on new builds and renovation of libraries, real-time risk warning systems were overlooked. To this end, this paper proposes an IoT-based risk warning system for new builds and/or renovation of libraries.
Fuzzy sets
Introduced by Zadeh (1965), fuzzy sets theory (FST) was adopted for dealing with uncertainty due to imprecision and vagueness. Researchers choose FST to generate powerful problem-solving techniques with wide applicability, especially in the field of decision making (Chen and Chen, 2007). FST offers the frame of analysis that could deal with imprecision in inputs (Mentes and Helvacioglu, 2011). Recently, fuzzy analytics has been applied to solve many real world problems including engineering problems (Boukezzoula et al., 2007;Chi et al., 2016;Feng et al., 2006;Kumar et al., 2016Kumar et al., , 2017;;Li et al., 2012;Manna et al., 2017;Xing et al., 2013).
Model

System framework
This risk warning system is designed to be an interconnecting network to detect, control and manage accidents in construction. Figure 1 shows the architecture of the system. This system is composed of four layers: the perception layer, the transport layer, the processing layer and the application layer. The perception layer includes a number of sensors, RFIDs, cameras, sign monitors and video recorders which can collect object position data, events data and monitoring data from construction site. The data collected by various sensors include structured data, semistructured data and unstructured data. In order to facilitate information transmission, the data will be transformed and standardized before sending to transport layer. The transport layer provides logical communication between IoT layer and data layer. ZigBee, TCP/IP and IPX/SPX protocols are used in this layer to transmit standardized data to data layer. The processing layer consists of three components: data processing component, library component and data evaluation component. Data processing component manipulate items of data to produce meaningful risk information. Library component includes a number of libraries like case library and factor library to store empirical data of cases and event status. Data evaluation component compares and matches event data with data stored in libraries to generate risk knowledge for further analysis. The application layer is the highest layer which consists of early warning application, control application and linkage application.
When potential risks are detected, early warning application will assess the severity of consequences, and broadcast the risk alarm to whole system if necessary. After receiving the alarm, control application will implement the emergency plan such as mandatory evacuation and system backup to protect personnel and equipment. The linkage application will send the alarm information to relevant departments simultaneously.
Data processing workflow
The data processing of the system consists of four phases. Figure 2 shows how incoming data are processed in the workflow. In the first phase, data collected by sensors are standardized and sent to the processing layer. In order to identify key characteristics of the
Case library

Elements of library

Distributed sensor Distributed sensor
Distributed sensor
Current event condition data
Empirical event condition data
Matching experience event

Matching empirical event decision data

Current event operation decision data
The actual decision data of the current event
Elements and their adjustment weights
Elements and their fixed weights
Similarity calculation
Attribute reduction fixed weight update Experts participation ...
Figure 2. The workflow of the
IoT-based risk warning system data, system computes the similarity between current event data and empirical event data using expert evaluation weight and system weight. Then the key characteristics will be used to match event cases stored in the case library and the factor library. Once a similar event case is found, the system will be able to predict current event decision data using information stored in this event case. In the last phase of data processing, staff inputs the actual decision data into the case library to generate new knowledge, the elements and their weight stored in the element library are also updated.
Risk evaluation algorithm
CBR (Elfirdoussi and Jarir, 2019;Li and Xu, 1992;Pokojski et al., 2018;Xu, 1994Xu, , 1995Xu, , 1996) is adopted to improve the accuracy of risk detection. The main idea of CBR is to use past experiences to identify and solve new problems. In this study, the comparison between current events and previous risk events allows us to detect potential safety risk with less volume of data. The main procedures of this algorithm are described below.
3.3.1 Building event matrix. In the system, there are m cases of event T stored in the system, the set of cases is C ¼ {c 1 , c 2 , …, c m }. There are n elements stored in the system. The set of elements is F ¼ {f 1 , f 2 , …, f n }. After standardizing data from the sensors, event matrix is formed as:
where v ij is the jth element of the ith case.
Next, set the current event to t, and its type is T. The set of factor values is represented as
3.3.2 Computing similarity between current event and historical event. The approximate degree of the f j factor value of current event t and historical case c i is defined as:
The fixed weight set of n elements is W s ¼ {w s1 , w s2 , …, w sn }, and P n i¼1 w si ¼ 1. If the weight of adjustment is given by an expert, the set is called W r ¼ {w r1 , w r2 , …, w rn }, and P n i¼1 w ri ¼ 1. Therefore, the weights of n elements is set to W ¼ {w 1 , w 2 , …, w n }, among them:
Based on Equation (2), the comprehensive approach of the current event t and the experience case c i is as follows:
When the sim(t, c i ) is greater than threshold μ, it can be considered that the two cases are similar, the current event t data can be inferring based on empirical case c i . If multiple experience cases are similar to current events, then the empirical cases with the highest degree of integration are selected for reasoning.
924
LHT 37,4
For certain types of events, {f k } is a set of event elements. If the approximate degree of the f k factor value of current event t and historical case c i is great than a threshold μ k , it is considered that two events are similar.
The decision data of the experience case c i is d i and the decision data of the current event t can be modified according to the decision data of the experience case c i :
3.3.3 Attribute reduction and fixed weight update. Attribute reduction and fixed weight will be updated after each event decision, the actual conditions data and the decision data are added to the knowledge base. This process includes the reduction of the set of elements and the determination of the fixed weights. First, the information system S is established:
is the factor set which includes factors affecting the metro construction risk, and decision-making elements such as risk level or security level; V is the set of factor values; F is an information function, which specifies in the elements of each object value. It specifies the element value of each object in U.
The pos R (X) is determined by the decision-making elements R, it must belong to the element of X.
C, the condition factor f i is the essential factor for the safety risk early warning model. For each kind of risk, the factors involved in the conditional data are gradually refined with the accumulation of empirical knowledge. The fixed weight of each factor is as follows:
where card(x) is the number of elements in the x.
Experiment
Assume that a construction project is being carried out in a city. The proposed risk warning system has pre-stored the empirical data of various events, including conditional data and decision data. Suppose each event data transmitted by the sensor contains seven elements ( f1, f2, …, f7) and one decision data type d 1 , after all event data are standardized, the matrix of event cases is created, as shown in Table I.
Standardized data for event cases
Next, we reduce some redundant attributes of event cases, decision-making elements of each event are listed as follows: 1;2;3;4;5;6;9;10 f g ; ¼ 3; 4; 5; 6; 7; 8; 9; 10 f g ;
Based on decision-making elements of each event, the decision of f 4 and f 7 for event d 1 can be reduced, only factor sets {f 1 , f 2 , f 3 , f 5 f 6 } are kept. Using Equation ( 5), we get fixed weight of each factor {0.6, 0.8, 0.8, 0.7, 0.8}. The normalized fixed weight is listed as: and standardized conditional data of the current event t is:
V t ¼ 2; 3; 1; 2; 3 f g :
Using Equation (1), we can get the approximate degree of all event factor values and experience cases: sim v t1 ; v 1 ð Þ¼ 1; 1; 0:5; 0:5; 0:5; 0:5; 0:5; 0:5; 1; 0:5 ð Þ ; sim v t2 ; v 2 ð Þ¼ 0:5; 0:5; 0; 0; 0:5; 0; 0; 0:5; 0:5; 0:5 ð Þ ;
sim v t3 ; v 3 ð Þ¼ 1; 0:5; 0:5; 0:5; 1; 0:5; 0:5; 0:5; 0:5
sim v t5 ; v 5 ð Þ¼ 1; 1; 0:5; 0:5; 0:5; 0:5; 0; 0; 0; 1 ð Þ :
After aggregating the approximation of the current event t and the empirical case C, we get: sim t; C ð Þ ¼ 0:869; 0:736; 0:368; 0:438; 0:703; 0:368; 0:332; 0:464; 0:524; 0:809 ð Þ :
Suppose the absolute key conditions are f 4 , and the threshold is 1; the proximity threshold μ between case c 1 and c 10 is 0.8. In this case, we can conclude that case c 1 and c 10 are similar to the current event t. And case c 1 has the most comprehensive approximation to the current event t. Finally, using Equation (4), we will get the decision data of current event t which is 2.265.
926
LHT 37,4
Summary and conclusion
With IoT tags and RFID, smart libraries integrate database and cloud computing, and further continuously monitor books in real time, track labeled objects geographically (Bayani et al., 2018). Like construction projects, new builds and renovation of libraries faces a lot of security and risk issues. For instance, the personnel in libraries that are under renovation includes library employees, users and construction workers. How to let libraries operate properly as well as protect the safety of everybody on-site are big challenges. Moreover, the equipment and materials are spread across the renovation site. It is vital to get real-time status of these equipment and materials. The proposed IoT-based risk warning system is a dynamic one which can collect, analyze and share safety information across construction sites. Its key characteristics ensure that it can be easily extended to risk management for library renovation. Based on the specific requirements of library renovation, the risk warning system of library renovation could be consisted of four layers: the perception layer, the transport layer, the processing layer and the application layer. Among them, the perception layer collects object data from library renovation sites. The transport layer provides logical communication between the perception layer and the processing layer. It transmits information obtained from the perception layer to the processing layer (Xu and Duan, 2019;Yang et al., 2018;Zheng et al., 2014). By adopting CBR, the processing layer compares and matches event data with historical data and further evaluates potential risks of events. The application layer is the platform for risk broadcasting, emergency planning and information sharing and collaboration among stakeholders.
In summary, the proposed IoT-based system makes it possible for library management to monitor equipment, materials and personnel in new builds and renovation of libraries in real time. By using RFID tags on books and office supplies, the system can easily track material flow and visualize logistics processes. This will improve the efficiency of material flow and reduce logistics costs in libraries. In addition, compared with other cross-region projects, library renovation projects are relatively small. The proposed system fits the requirements of library renovation because the CBR approach enables the proposed system to detect and predict construction risks with small number of data.
Security risks in construction projects are extremely high due to the uncertainty of project environment, low effectiveness of risk monitoring and unpredictable geological conditions. To detect and prevent accidents in construction projects, the status of materials, equipment and the activities of workers must be monitored in real time. Due to their limited capabilities, existing risk monitoring systems and models cannot detect risks under complex circumstances, such as cross-region construction and complicated geological environments. To address these challenges, this study proposes an IoT-based risk warning system. This system consists of large number of IoT devices, which can automatically collect and share real-time information. In particular, the adoption of CBR approach allows the proposed system to make dynamic and accurate risk prediction with small number of data. The experiment shows that the proposed system can effectively detect, monitor and manage risks in construction projects. Future studies should test the proposed system in new builds and renovation of libraries. Results generated by library renovation projects and constructions should be further compared to validate the proposed system.",5661,6355
10.6017/ital.v35i3.9255,,Yeh and Walter 2016,True,Shea-Tinn Yeh,Critical Success Factors for Integrated Library System Implementation in Academic Libraries: A Qualitative Study,"
Integrated library systems (ILSs) support the entire business operations of an academic library from acquiring and processing library resources to making them available to user communities and preserving them for future use. As libraries' needs evolve, there is a pressing demand for libraries to migrate from one generation of ILS to the next. This complex migration process often requires significant financial and personnel investment, but its success is by no means guaranteed. We draw on enterprise resource planning and critical success factors (CSFs) literature to identify the most salient CSFs for ILS migration success through a qualitative study with four cases. We found that careful selection process, top management involvement, vendor support, project team competence, staff user involvement, interdepartmental communication, data analysis and conversion, project management and project tracking, staff user education and training, and managing staff user emotions are the most salient CSFs that determine the success of a migration project.
",2015,Library Hi Tech,,"INTRODUCTION
The first generation of integrated library systems (ILSs) were developed specifically for library operations focused on the selection, acquisition, cataloging, and circulation of print collections. As libraries' nonprint materials steadily grow, the print-centric ILSs became less and less efficient in supporting libraries' daily operations. Recent years have seen an emergence of a new generation of ILSs, commonly called Library Services Platforms (LSPs), that takes into account the management of both print and electronic collections. LSPs take advantage of cloud computing and network advancements to provide economies of scale and to allow a library to better share data with other libraries. Furthermore, LSPs unify the entire suite of library operations to provide efficient workflow at the back end and advanced online discovery tools at the front end for the library. 1 Given the claimed benefits of the emerging LSP and the fact that vendors are phasing out support for their legacy ILSs, we project that more libraries will be migrating to LSPs as the systems mature and libraries' needs evolve. redundancy, data retrieval and reporting efficiency, easy module extension, and Internet commerce capability.
Just like an ERP system for a business, a complete library management solution comprises a suite of integrated applications that manage a broad range of library processes including circulation, acquisition, cataloging, electronic resources management, and system administration. LSPs, the current generation of library management systems, are designed to manage both physical and digital collections. LSPs follow the service-oriented architecture (SOA) and can be deployed through multitenant Software as a Service (SaaS) distribution model. 15 In addition to supporting all library functions, LSPs integrate with other university systems, such as student registry and finance, and provide front-end for library patrons in a cloud environment that leverages a global network of systems for discovery of a wide array of resources. 16 Since an LSP is essentially an enterprise system for library functions, CSFs of ERP implementation success could guide LSP implementation.
CSFs are conditions that must be met for an implementation to be successful. 17 More than ninety CSFs have been identified for ERP implementation success. 18,19 Those CSFs have been classified according to various schemes, but we found the strategic versus tactical classification most relevant to the library context. 20 Strategic factors address the big picture involving the breakdown of goals into do-able items. Tactical factors, on the other hand, are the methods to accomplish the doable items that lead to achieving the goals. 21 By examining the entire list of CSFs from both the strategic and the tactical perspectives, we identify top CSFs for library-management-solution implementation and migration success, defined as on-time and on-budget delivery as well as smooth implementation process, 22,23 through a qualitative study.
METHOD
We conducted semi-structured interviews with open-ended questions to identify the most salient CSFs for implementation success. Since we needed to reduce more than ninety CSFs in the literature to a list of most salient CSFs in the library context and to potentially identify new CSFs, a qualitative-interview approach was more suitable than a quantitative-survey approach. A twostep process was used to arrive at the final list. First, we evaluated all CSFs in the literature and identified a subset of CSFs that might be most relevant for library-systems implementation. 24 Second, this CSFs subset was used to develop an interview guide for semistructured interviews conducted later to further reduce this subset. Open-ended questions were also used during the interviews to elicit additional CSFs. An institutional review board (IRB) application was submitted and approved. The result of this two-step process is a list of ten CSFs discussed in the results section, with nine CSFs coming from our initial list and one CSF emerging from the interviews.
The criterion for recruiting study libraries is that the library has implemented a new LSP within the last three years. This is because the LSP is the current generation of ILS, and it is only within the last few years that various LSP vendors began to promote and implement the LSPs. A recruitment email was sent to libraries listed as adopters on various vendors' press release sites. Participating recipients referred the interview request to appropriate migration team members whom we later contacted to schedule interviews. This resulted in up to five people from each participating library being interviewed in person or via Skype. Their positions are listed in table 1. Interviews were recorded, transcribed, and cleaned. Emails to the same interviewees were used for follow-up questions as needed. After interviews with each library, qualitative data analysis was performed to identify CSFs that emerged from the interviews. Interviews continued until no new CSFs emerged in the last interview. In total, staff from four libraries were interviewed between October 2014 and March 2015 about their implementation process and experience from staff user perspective. The design and implementation of discovery public interface experience was not part of this inquiry. 
RESULTS
The following CSFs emerged from interviews: careful selection process, top management involvement, vendor support, project team competence, staff user involvement, interdepartmental communication, data analysis and conversion, project management and project tracking, staff user education and training, managing staff user emotions. We discuss each CSF next.
Careful Selection Process
Most ILSs are commercial, off-the-shelf software systems that can vary dramatically in functionality from system to system. 25 For example, some packages are more suitable for large institutions while others are more suitable for smaller ones. To mitigate risks in productivity or transaction loss and to minimize system and implementation costs, a library needs to determine the best ""fitness-of-use"" system. Such a determination is the outcome of a careful selection process. Although there is no commonly accepted technique, method, or tool for this process, all selection processes share common key steps suggested in the literature. 26 They are the following as applied to library-systems selection: define stakeholder requirements, search for products, create a short list of most promising candidates based on a set of ""must-have"" requirements, evaluate the candidates on the short list, and analyze the evaluation data to make a selection. In addition, if the server option was chosen instead of the cloud option, selected hardware needs to satisfy system requirements for the final configuration.
Careful selection process emerged as a CSF that affected implementation outcome for all four libraries. All cases were migrating to an LSP system. Some systems can be offered as locally installed systems, which require appropriate in-house and hardware capabilities. Case 1 did not consider its IT capability when deciding on a turnkey system. As a result, the library experienced difficulties in setting up the infrastructure in-house during the implementation. Each of the other three cases considered the candidate system's compatibility with the legacy system, the match between library needs and system functionalities, system maturity, migration costs, data storage needs, and vendor support before and during the implementation as well as continued vendor support throughout the life of the new system. Even though each of the three libraries arrived at its system choice differently, on reflection, interviewees expressed relief and satisfaction in their decisions to choose their respective systems.
""We were in the position where our servers were out of date and warranty, needed to be replaced. The servers were too small. We had sizing issues and we couldn't update to the most recent version of Aleph . . . Alma being a cloud based solution will eliminate our need to be 'in the server business.'"" (Case 2).
""We went through a very extensive formal process to select this system."" (Case 3)
Top Management Involvement
Successful implementation requires strong leadership by executives who understand, support, and champion the project. 27 When this involvement is trickled down through organizational hierarchy, it leads to an organizational commitment, which is required for implementation success for complex projects. 28,29 Since library-system implementation is a complex project that (if done correctly) will transform the entire library and reposition it for better efficiency, strong leadership is critical as well.
In all four cases, top management were involved in the final decisions of their respective system choices. In cases 1 and 2, top management also took charge in securing funding for the migration projects. Interviewees stressed that top management support was very important in their respective project implementations.
""The top level management took the recommendations from the systems librarians at the time, with the blessing of the council determined whether they want to proceed with the product Alma, and had funding conversations with the financial people."" (Case 2)
""We have faculty library committee, faculty governance oversight. We showed them webinars of the products we considered before we signed them, so we have faculty representation on board. We held open forum and were inclusive in our invitations."" (Case 4)
Vendor Support
With a new technology, it is critical to acquire external technical expertise, often from the vendor, to facilitate successful implementation. 30 Effective vendor support includes adequate and highquality technical support during and after implementation, sufficient training provided for both the project team and staff users, and positive relationships between all parties in the project. 31 Additionally, there should be adequate knowledge transfer between the vendor consultants and the clients, which can be achieved by defining roles, achieving shared understanding, and enhancing relationships through competent communication. 32,33 In the case of library-system implementations, vendor support is particularly important because of the complexity of each new generation of the system and the library personnel's knowledge gap in understanding the nuts and bolts of the new system.
Effective vendor support was identified in each case as a critical success factor determining the implementation outcome even though the form of vendor support varied from case to case. In case 1, the vendor sent different consultants with various expertise as project managers on the basis of the project phase. In case 2, the vendor sent one consultant who served as the main project manager. In case 3, the vendor provided a project manager and a team of technicians. In case 4, consultants were shared across multiple consortium libraries that were implementing the system at the same time. No matter how vendor support was provided, it was essential for implementation success as indicated by interviewees.
""The vendor has been very supportive and provides a group of experts throughout the process, some are knowledgeable in server business while others are skilled project managers."" (Case 1)
Project Team Competence
Since library-system migration affects all functional areas of a library, members of the implementation team need to be cross-functional. Furthermore, members with both business knowledge and technology knowhow are especially crucial for implementation success. 34 Competence of vendor consultants assigned to the project also influences implementation success, as discussed earlier. Additionally, it is important to have an in-house project leader who champions the project and who has the essential skills and authority to set goals that legitimize change. 35 Having a competent project team was essential for implementation success for each of our cases.
In each case, the vendor provided the project manager and the library provided a co-manager who was a champion figure. Other team members came from various functional areas such as acquisition, circulation, cataloging, electronic resources management, and system administration. For example, in case 1, the technology librarian participated as a co-project manager. The projectmanagement team comprised module experts within the library and from functional areas. In addition, the university's technology services department lent technical support during early stages of implementation when servers need to be set up. The interviewees all stressed the importance of project-team competence.
""Without the infrastructure knowledge from the university's technology team and their time and full support to negotiate with the vendor, the migration project would not have been possible."" (Case 1)
""The university's IT made sure that we are in compliance with campus policies and expectations for securities."" (Case 2)
Staff User Involvement
It is important that the project team involve staff users early on, otherwise the implementation process may be bumpy. When end users are involved in decisions relating to system selection and implementation, they are more invested in and concerned with the success of the system, which in turn leads to greater system use and user satisfaction. 36,37 As such, it is one of the most cited critical success factors in ERP implementation. 38 Because personal relevance to the system is just as important for library-system implementation, effective staff user involvement with implementation is positively related to implementation success.
Staff user involvement has emerged as a main success factor in all our cases and contributed to the implementation project outcome. In case 1, staff users were not consulted as to whether an LSP was necessary for the library, although they were informed of the reasons for implementation. Additionally, staff users were not involved when the project timetable was negotiated. This lack of early staff user involvement led to considerable stress down the road, which made the implementation process bumpy. The other three cases involved staff users early on; as a result, staff users experienced much less stress and frustration down the road. Specifically, in case 2, the staff users were educated about the need for migration through staff meetings, town hall meetings, supervisory meetings, council meetings, and forums. Many product-demo sessions were conducted for the staff so they would have the knowledge to participate before the final decision was made. There were daily internal newsletters conveying implementation news throughout implementation months. In case 3, the entire library was involved with the selection of a new system. While the key staff (such as circulation manager, acquisition manager, and reference manager) had more input than others, everyone offered input about the project. As such, the buyin with the new system was strong from all stakeholders. In case 4, staff users were involved early on through open forums and webinars. The following quotes are examples of interviewee sentiment concerning staff user involvement:
""Everybody is involved in choosing the system; partially because Evergreen had been so problematic. We wanted to make sure that everyone is on board."" (Case 3) ""Migration is the most time consuming aspect of the library staff work during the time of the project, without their buy-ins, it is difficult to have a successful project."" (Case 4)
Interdepartmental Communication
The importance of effective communications across functional and departmental boundaries is well known in information-systems-implementation literature. 39 With consultants coming from the vendor, project team members coming from different functional areas, and staff users with different perceptions and understandings of the implementation project, the importance of effective communications between all involved cannot be overstated. Communications should start early, be consistent and continuous throughout various stages of the implementation process, and include a system overview, rationale for implementation, briefings for process changes, and contact-points establishment. 40 Expectations and goals should be communicated to all stakeholders and to all levels of the organization. 41 Effectiveness of interdepartmental communication affected the implementation outcome in all our cases.
In case 1, the library's project manager was designated to communicate with the vendor when issues arose, such as hardware and software configurations, system backup and use, and task assignments. The formal project plan was established using the web-based Basecamp so that team members in different roles with different responsibilities could communicate and work together online. Regular meetings were held and emails were exchanged between project team members. However, there is a lack of effective interdepartmental communication with staff who were not on the project team. This resulted in the absence of necessary system testing that would have detected some data-integrity issues. Such issues later caused the system to be offline for days, which brought much frustration and stress to everyone. In the other three cases, all actors were well informed through news releases, meetings, presentations, and webinars. Concerns were communicated to the project team and addressed timely. As a result, the level of frustration was very low for those three cases.
Data Analysis and Conversion
A fundamental requirement for the effectiveness of an ERP system is the accuracy of its data, 42 and the same is true for a library system. Data types in a legacy ILS are often of an outdated format and can differ from formats supported by a new library system. Conversion from one format to another can be an overwhelming process, especially when there is no existing expertise in the library. Since migrating legacy data to the new system is essential, effective data analysis for conversion is a critical success factor for implementation success.
The smoothness of each of the four implementation cases was related to the project team's data analysis and conversion efforts. In case 1, the library did not spend any effort to analyze, convert, or clean the data. As a result, the system experienced data-integrity issues after it went live. The other three libraries either devoted time to clean and convert the data or had a third party do the data cleaning. As a result, no system issues arose from data-integrity problems. Interviewees from case 2 told us, ""We elected to freeze the data 30 days sooner in terms of bibliographic data, so that we can do an authority control project with a third party vendor.""
Project Management and Project Tracking
According to ERP implementation literature, effective project-management practices are critical for implementation success. Such practices include defining clear objectives, establishing a formal implementation plan, designing a realistic work plan, and establishing resource requirements. 43 The formal implementation plan needs to identify modules to be implemented, tasks to be undertaken, and all technical and nontechnical issues to be considered. 44 Project progress must be carefully monitored through meetings and reports. 45,46 Effective project management and tracking has affected implementation outcome in all our cases. A popular project management and tracking software is Basecamp, a web-based project management and collaboration tool initially released in 2004. 47 It offers discussion boards, to-do lists, file sharing, milestone management, event tracking, and messaging system that help project teams stay organized and connected despite their different locations. All cases used Basecamp for project management and tracking, which contributed to on-time and on-budget project completion for all cases.
Staff User Education and Training
A new system often frustrates users who do not receive adequate training in its functionalities and use. 48 When feeling frustrated and stressed, users may avoid using the system. Proper and adequate training will sooth users and eliminate their reluctance to use the new system, which in turn helps realize productivity gains. 49,50 Training processes should consider factors such as training curriculum, user commitment, trainers' personnel skills and competence, as well as training schedule, budget, evaluation, and methods. 51 Effective staff user training has emerged as a critical success factor from all our cases. In case 1, staff users had access to a vendor-supplied preview portal, which simulated system functionalities. Staff users were so familiar with the new system by the time the system went live that they were eager to engage with it. In cases 2, 3 and 4, staff users were trained through demo products, online video trainings, Q&A, and on-site training sessions conducted by the vendor.
These training materials and sessions served to ease staff user's feeling of uncertainty and anxiety, as the following quotes show:
""The online training videos were provided to all staff in the library and followed up with Q&A sessions which members of the committee will host in their respective areas. . . . Then Ex Libris did a week long onsite training workshop serve for the final deep configuration issues. . . . We know that there are staff users who want to be ahead of the game, yet there are always people who don't want to learn until the day before they go live."" (Case 2)
""We have a training package with several onsite visits, each one is for a few days. The trainer focused on one aspect of the system. It was more than watching the videos online. Because of the small staff here, almost everyone attended at least one training."" (Case 3)
""The trainers varied with their expertise, we developed fondness for some more than others. The training is functional in nature. The vendor's priority was about trainer availability and to keep the project on time. We became familiar with trainers' expertise; we were able to request the right trainer with the job."" (Case 4)
Managing Staff User Emotions
Although education and training eases user anxiety, it does not completely eliminate it. Emotions felt by users early in the implementation of a new system have important effects on the use of the system later on. 52 How to manage staff user anxiety and negative emotions when they appear has emerged as a critical success factor in all our cases, as shown in the following quotes:
""There were so many things going on in the library during the migration go-live week.
The unknown of the migration success made staff users uncomfortable. Should the migration date be decided in consideration of other initiatives, the frustration experienced would have been a lot less and might not have been ignored during the going-live week."" (Case 1)
""The frustration was just change; it was the fact that we have to learn something new. . . .
Primarily the frustration was handled by the lead."" (Case 2)
""There was a challenge, especially early on, in getting people to engage with the manuals and the literature in documentation. It is as if everyone is being asked to learn a new language. . . . The key relationship between the onsite coordinator and the project manager on the vendor side is important. When those two exchange information and handle frustration diplomatically, this bridge between the two organizations can smooth over a lot of rough feathers on either or both sides."" (Case 4)
This final CSF did not come directly from the ninety-plus CSFs that we started with, although it aligned closely with ""Change Management"" category. 53 This CSF emerged mostly from the interview process.
Summary of Results
The results of the case studies for each critical factor are summarized in table 2. Implementation project outcome is summarized in table 3. An implementation is considered successful if it was completed on-time and on-budget and if the implementation process was smooth as reflected in the number and degree of unexpected problems along the way. 
Critical Success

DISCUSSION AND CONCLUSIONS
The implementation of a new ILS is a large-scale undertaking that affects every aspect of a library's operations as well as every staff user's workflow process. As such, it is imperative for library administrators to understand what factors contribute to a successful implementation. Our qualitative study shows that there are two categories of CSFs: strategic and tactical. From the strategic perspective, top management involvement, vendor support, staff user involvement, interdepartmental communication, and staff user emotion management are critical. From the tactical perspective, project team competence, project management and project tracking, data analysis and conversion, and staff user education and training to break down the technical barrier greatly affect implementation outcome. In addition, selection of the final system from a variety of choices and options requires a careful consideration of both strategic and tactical issues. Each factor identified is important in its own right during the implementation process. Combined, they complement each other to guide an implementation to success. Among the list of CSFs identified, the role of staff user emotion management was not identified during the theoretical phase of the study; it only emerged as an important CSF during interviews. Top management involvement, vendor support, project team competence, project management and tracking, and staff user education and training are CSFs that were somewhat intuitive, and they were implemented by all cases. However, a library may select an end system without careful considerations. It may also be unaware of the importance of involving users early on, the importance of opening clear lines of interdepartmental communications, or the importance of performing data analysis and conversion before the implementation. Staff user emotion management, especially, is at the risk of being an afterthought of an implementation.
By identifying the most salient CSFs, this study offers practical contributions to academic library leaders and administrators in understanding how critical success factors play a role in ensuring a smooth and successful ILS implementation. Although CSFs have been extensively studied in the discipline of information-systems management, this is the first study to apply CSFs in the library context. Since library management has its unique challenges compared to businesses, identifying CSFs for library-system-implementation success is important not only for the current migration to LSPs but also for future migrations to future generations of ILSs as the needs of libraries continue to evolve.
As with any empirical research, there are limitations to this study. The number of academic libraries interviewed is small despite no new information being discovered after the fourth interview. The vendors represented in this study are only two of the many in the market providing LSPs to libraries. With these aforementioned limitations, the results of this study may not be generalizable to libraries implementing an LSP with vendors other than Innovative Interfaces and Ex Libris. Additionally, the results may not be generalizable to nonacademic libraries.
This research can be extended to validate the proposed CSFs quantitatively by performing a survey research in academic libraries. Studying interactions between identified factors will offer an even greater contribution. This research can be experimented in other types of libraries to generalize inferences. In addition, case libraries 3 and 4 both expressed that LSP changes the public interface that is used by external users, and they wished to have more opportunities for outreach prior to the implementation. Although the design and implementation of the public interface was not considered within the scope of this research, this comment is insightful because it may imply that future studies should consider a project champion to be a critical success factor. The project champion must have people-related skills and position to introduce changes in achieving buy-in from staff users. 54,55 ",5192,5665
10.1108/LHT-06-2015-0065,,,False,Min Zhang,Which platform should I choose? Factors influencing consumers’ channel transfer intention from web-based to mobile library service,"
Purpose -With more and more individuals relying on mobile devices to obtain information, many libraries launch mobile application to satisfy mobile users' information need. The purpose of this paper is to figure out factors influencing consumers' channel transfer intention of library service from web-based platform to mobile app. Design/methodology/approach -A structural equation model is proposed based on categorization theory. In addition, situational contexts are taken into account to make research model more suitable for the real condition. Data collected from 319 samples are used for hypotheses examining. Findings -The relationships between source and target (perceived differentiation including function differentiation and resource differentiation) positively affect perceived situation efficiency, which in turn shapes intention to use mobile library application. Perceived mobile library quality positively influences perceived differentiation, perceived situation efficiency and mobile library adoption intention. In summary, perceived situation efficiency is the main factor. Practical implications -Both quality and situational factors should be taken seriously, and mobile device producers and mobile app developers should cooperate on improving the quality of mobile app. Meanwhile, it is critical to examine the relationship between web based and mobile library service in the initial or early stage of mobile library development. Originality/value -By focussing on the impacts of the relationship between web and mobile library service and evaluation of mobile library on the adoption intention, this paper not only provides a theoretical understanding of mobile library adoption behavior but also offers practical insights to library managers and app developers for promoting such a process.
",2016-03-21,Library Hi Tech,Emerald,"Introduction
Mobile devices, especially smart phones, have been widely used in recent years due to their capabilities of supporting many applications. The sales of smart phones to end users reached 336 million units during the first quarter of 2015, exceeding that of PC, which are 115.7 million units (Canalys, 2015;Gartner, 2015). Ericsson also shows that 70 percent of people will use smart phones and 80 percent of all mobile data traffic will come from smart phones by 2020 Ericsson (2015). The ever-increasing propagation of smart phones and mobile network has induced great use of mobile access. With the rise in smart phone use, People are taking advantage of being connected to data wherever they are. Mobile application (app) is the main access to mobile network. For example, about 1.6 million apps are there in the two largest stores Apple App Store and Google Play (Canalys, 2013). Moreover, the number of app users also growing fast, which may reach 4.4 billion by the end of 2017 according to Portio Research (2013). China is also a very potential country for the development of smart phones. In China, the shipments of smart phones are about 40.3 million in the April 2015, (MIIT, 2015) and the cumulative total downloads of mobile app was about 28.6 billion in the third quarter of 2014, demonstrating that the mobile app has come of age EnfoDesk (2014).
The rapid growth and use of mobile network, coupled with the development of mobile commerce, changes users' information search behavior thoroughly. Demands for mobile library services are dramatically increasing due to large user base of mobile services. Therefore, libraries explore mobile devices as a way to connect with users, creating a library application (""app"") or mobile website that allows users to access library hours, view their library account or even search databases. For Instance, over 76 percent of Chinese top universities which are sponsored with ""Project 985"" have provided mobile library apps and want to transfer their customers from PC terminal to mobile terminal to optimize users' perceived experience. But the success in web services cannot promise the success in mobile services because there are several problems which will be generated in the channel transition process.
""Channel"" is a significant concept is marketing science. With the development and popularity of web-based application, more and more people choose online business model instead of offline business model. With the convenience of mobile-based application, more and more people shift from web-based method to mobile-based method. To clarity this kind of phenomenon, scientists use the concept of ""channel"" to simplify it. In this research, we borrow the concept ""channel"" from marketing science to library science to make our research more common. In addition, ""channel transfer intention"" and ""channel usage behavior"" is also applied. If the consumer wants to shift from web-based library service to mobile-based library service, we classify it into ""channel transfer intention."" What's more, we classify consumers' behavior as ""channel usage behavior"" is they continue to use a certain kind of channel.
In fact, we notice that different from some other mobile service such as mobile purchase, mobile entertainment, mobile social interaction or mobile engagement, not all of the customers will transfer from web-based platform to mobile app when they want to use library service. And then, it would be interesting to examine the influencing factors which stimulating or blocking channel transfer intention, which is the purpose of the present study. The structure of this paper is organized as follows. The next section gives a literature review of related studies. The theoretical background of this study is described in Section 3, coupled with research model and associated hypotheses. The research methodology and analysis results are presented in Sections 4 and 5, respectively. Section 6 contains some key findings plus theoretical and practical implications. The last section gives a brief summary of the whole study.
Literature review
With the proliferation of mobile devices and mobile connectivity, the popularity of the mobile app is increasing rapidly. Meanwhile, an important stream of research has focussed particularly on the understanding of various determinants of mobile app adoption. In terms of theoretical background, technology acceptance model and unified theory of acceptance and usage of technology (UTAUT), as the classical adoption models, were widely applied to study the usage intention determinants of different end-customer mobile services (Cheng and Huang, 2013;Oliveira et al., 2014). In addition, task-technology fit (TTF) which took utility of technology into consideration was also generally used in studies of technology adoption Shih and Chen (2013). With respect to influential factors of mobile service adoption behavior, both extrinsic and intrinsic motivations were examined. According to the theory of motivation, extrinsic motivation refers to satisfaction independent of the actual activity itself, and intrinsic motivation refers that the activity is valued for its own sake and appears to be self-sustained (Calder and Staw, 1975). For extrinsic motivation, perceived usefulness and social influence were widely discussed and validated (Hanafizadeh et al., 2014;Morosan, 2014;Tai and Ku, 2013). And for intrinsic motivation, perceived ease of use, perceived risk and perceived trust were widely discussed and validated (Shih and Chen, 2013;Morosan, 2014;Tai and Ku, 2013). Moreover, several studies examined the control effect of demographic variables, implying that gender, age, educational level and culture were important factors to predict mobile service adoption (Chan and Chong, 2013;Chung, 2014). Various kinds of mobile services were investigated in the extant researches, such as mobile banking (Sohail and Al-Jabri, 2014), mobile ticketing (Brakewood et al., 2014), mobile payment (Zhou, 2014) and so on.
Although there are a lot of studies on mobile service adoption in the previous literatures, the number of that related to mobile library services is limited. Previous relevant studies on mobile library service mostly focussed on generalized definition, such as mobile websites, SMS reference and so on (Li, 2013;Mavuso, 2012). The studies on mobile library apps concentrated either on the design (Bishop, 2012;Bishop and Bartlett, 2013;Pianos, 2012) or on the students' adoption of mobile library app (Chang, 2013). With respect to the determinants and predictors of intention to use mobile library app, Chang (2013) found that UTAUT model examined the users' behavioral intention well, and the moderating effect of TTF was also significant. In all, it is worth analyzing the factors to predict the intention to use mobile library app.
In the process of mobile adoption, the perception of web-based services is an important factor because mobile-based service can be considered as an extension of web-based service. And in the process of service extension, a customer's experience with the web-based services may affect his or her perceptions about the mobile-based counterpart. Therefore, it is necessary to study the adoption of mobile platform in web-mobile extension context. The literatures mentioned above mainly focussed on a pure mobile context. With the opening up of extended mobile channels, some scholars paid attention to mobile service adoption in multi-channel context. Based on trust transfer theory, Wang et al. (2013) revealed that trust in web eWOM services would positively influence trust in mobile eWOM services, which was in turn positively related to mobile eWOM services adoption. Based on categorization theory, Yang et al. (2014) showed that web service quality and two relation-relevant factors namely perceived integration and perceived consistency were the predictors of users' perception of mobile services). However, in the field of library service, few studies examined the factors to predict the intention to use mobile library app, especially in the multi-channel context. Furthermore, the characteristics of mobile library app are different from mobile commerce. First, generally speaking, the most important purpose of mobile library app is not to make profits, but to satisfy users' need and provide them with better service. Hence, it is necessary to investigate users' demand for mobile service. Second, the users of academic library service including students, professors and staff are relatively constant. They not only use mobile service but also web-based service. Therefore, the emergence of mobile service must affect the web-based service, either complementary or competition. To avoid incidental channel conflict, it is also necessary to handle the relationship between web-based service and mobile service. At last, users acquire library service principally for the sake of learning, but learning needs a relatively long time to digest acquired information. In this view, users may prefer web-based platform to mobile app, which is different from mobile commerce. Therefore, contrary to mobile commerce, perceived differentiation between web-based platform and mobile app should be paid more attention in the field of library service.
The literatures reviewed suggest that it is necessary to investigate determinants of mobile library app adoption in multi-channel context. The aim of this study is to provide a comprehensive insight into mobile library app adoption in universities. In addition, we take the ubiquitous features of mobile device and mobile network into consideration which were rarely discussed in previous studies, and propose a new variable named perceived situation efficiency.
Theoretical background and research model 3.1 Categorization theory
In order to handle the amount and variety of information in life, categorization is adopted to recognize, differentiate and understand a new object on the basis of its similarities or dissimilarities with the corresponding old one. In other words, people naturally classify the objects and events of their world by matching the cognitive summary of their most common features with the category prototypes (Nye and Forsyth, 1991). That is the foundation of categorization theory. Categorization theory was based on cognitive economy, and was widely applied in social management, such as leadership appraisals and social identity (Chatman and Spataro, 2005;Palich et al., 1992;Zdaniuk and Bobocel, 2013), which contributed to the emergence of leadership categorization theory and self-categorization theory.
Based on the Computers Are Social Actors paradigm, researches related to humancomputer interaction (HCI) demonstrated that HCI followed the social rules and expectations of human-human interactions (Nass et al., 1995;Kim et al., 2013). Individuals mindlessly apply social rules to interactions with computers that exhibit anthropomorphic cues or social categories even when they recognize that they are interacting with non-human agents or machines (Kim, 2014). Thus, categorization theory is also applicable to the process of HCI, such as users' interaction with mobile app. According to categorization theory, users' impression and attitude of mobile services are influenced by their knowledge about web-based services and integrated knowledge structures in their memory. However, related study suggested that perception of extended products or services were influenced not only by the perception of associated existing products or services but also by the attitude toward the specific attributes of the extension (Yang et al., 2014;Boush and Loken, 1991). Therefore, the category-based evaluation on extended products or services relies on the discrepancy level between the new object and the category stored in users' memory (Ozanne et al., 1992).
The current study chooses categorization theory as the theoretical framework for three reasons. First, as the natural cognitive reaction to stimulus, categorization plays an important role in the process of our organizing and structuring the world, recognizing and understanding the new objects encountered in everyone's daily life. Second, in the web-mobile service extension context, mobile library app shares similar user base and concept feelings. Thus, user would naturally classify them into the same category based on their similarities which is just in accordance with the basic assumption of the categorization theory. Third, compared to web-based platform, ubiquitous of mobile service enables users to access information at the point-of-need, regardless of where they are and what they are doing. The uniqueness of mobile platform also meets the necessary conditions for categorization theory.
3.2 Research model and hypotheses 3.2.1 Evaluation of target: perceived quality and perceived efficiency. With the advent of web2.0, everyone can create and consume information at the same time. The ubiquity of information has made attention scarce and valuable which gives rise to an attention economy Huberman (2013). According to attention economy, web-based service and mobile app could not attract users' attention simultaneously. Thus, web-based service and mobile app must have their own advantages. Compared to computer, mobile device is limited in storage, battery life and process speeds, but superior in localization, immediacy and customization. Studies on information search behavior found that users tried alternative internal strategies (such as characteristics of the new objects) rather than searched for more information at high discrepancy level (Ozanne et al., 1992). Therefore, the characteristics of mobile app would be evaluated in the process of decision making.
Perceived quality is the global assessment of a user's judgment on the superiority or excellence of a service offering (Hwang and Kim, 2007;Song et al., 2010). Furthermore, quality play an important role in the impact on adoption of information systems in the framework of Information System Success Model, including system quality and information quality (Delone and Mclean, 2003). Therefore, it is reasonable to adopt perceived mobile library quality as the antecedent variable of adoption intention. In addition, as a measurement of instrumental performance of mobile library app, perceived mobile library quality is viewed as extrinsic motivation. Moreover, the theory of motivation pointed out that extrinsic motivation was related to users' behavior (Weiner, 2010). Since mobile device is the carrier of mobile app, and mobile app is psychologically perceived as distinct source of library service, mobile library quality is measured by both hardware (mobile device) quality and software (mobile app) quality. Therefore, we hypothesize that: H1. Perceived mobile library quality will positively influence intention to use mobile library application.
According to the Theory of Planned Behavior, intention to perform behavior is influenced not merely by attitudes toward the behavior and social norms, but also by perceived behavioral control (Ajzen, 1991). Situational characteristic was one of the components of perceived behavioral control, defined as the extent to which an individual was given control over different physical environments during a given behavior performing ( (Ajzen, 2005;Orvis et al., 2009). The situation of mobile app usage is almost unlimited because of the ubiquitous features of mobile device and connectivity. Thus, situational characteristic is one of unique features of mobile library app. Studies about situations focussed on location-related conditions, which were divided into the dimension of physical environments, media access and social dynamics (Karnowski and Jandura, 2014;Zhang and Zhang, 2012). By using mobile library app, users could acquire library service anywhere and anytime so that the influence by the changes of physical environments, media access and social dynamics are reduced. Such an advantage could improve the usage efficiency of library service which is just the results that users want. Thus, perceived situation efficiency is introduced to represent situational factor in the present study. Efficiency is an economic concept to measure the maximum output from effective inputs (Picard, 1989). Perceived situation efficiency is defined as relative cost of acquiring information from mobile library app in a certain situational context (Li, 2014). Since both situational context and perceived efficiency could affect user' behavior, combined that perceived efficiency was positively related to preferred media adoption (Karnowski and Jandura, 2014;Li, 2014), we hypothesize that:
H2. Perceived situation efficiency will positively influence intention to use mobile library application.
Based on the definition of efficiency, it is related to time spend and monetary cost. High quality of mobile library means high speed of access and great format of information, which will contribute to easiness to access quality and organized information that users need. Therefore, high quality of mobile library could save users' time spent in some extent. In the development of IS success model, S. Paek also pointed out that quality would positively influence perceived efficiency (Paek et al., 1994). Therefore, we hypothesize that:
H3. Perceived mobile library quality will positively influence perceived situation efficiency.
3.2.2
Relationships between source and target: perceived differentiation. Categorization theory provides a framework for investigation into how users' perception of a new channel is influenced by the old channel in multi-channel context. But the influence happened only when the new target was classified into existing known category (Palich et al., 1992;Zdaniuk and Bobocel, 2013). In the process of categorization, the most important issue is the discrepancy level between source and target weighted by the saliency of their common and distinctive features. Previous studies mainly focussed on the similarities between source and target. For instance, in the brand extension literature, perceived fit was adopted to measure the degree of similarity between parent brand and extended products in terms of their attributes, usage situations and target market, and was empirically examined to be positively related to the evaluation of extended products or services (Hem et al., 2014;Kim and Yoonb, 2013;Park et al., 1991;Zhang et al., 2013); in the attribution of leadership, if a person shares the similar characteristics with observers' leadership prototype, he or she would be more favorably regarded as leader (Epitropaki et al., 2013;Olivola et al., 2014); in the web-mobile service transition, perceived service-platform fit positively affects intention adoption of mobile service (Lee and Chang, 2013;Yang et al., 2014;Sun et al., 2014). However, it is more necessary to pay attention to discrepancy between web based and mobile platforms because the users of library service are relatively constant. At the same time, the purpose of construction of mobile library app is to meet users' need better rather than expand market shares. In the present study, we use perceived differentiation to measure the discrepancy between web based and mobile library service. And followed the definition of perceived fit, which was divided into functional fit and image fit (Carter and Curry, 2013), perceived differentiation is conceptualized as a two-dimensional construct which contains function differentiation and resource differentiation in this study because function and resource are the main services provided by library. Function differentiation refers to the degree to which mobile library app and web-based library platform satisfy users' different needs, such as database and library catalogue searching, books returning prompt and so on. Resource differentiation is defined as the degree to which mobile library app and web-based library platform provide different information based on the same retrieval. Perceived similarity was empirically examined to be positively related to the perceptions of target (Lee and Chang, 2013). Following the same parallel, we hypothesize that: H4. Perceived differentiation will positively influence perceived situation efficiency.
To investigate the first impression of mobile library app, perceived mobile library quality focusses on the characteristics of mobile device and mobile app, which are very different from web-based platform. In other words, system quality is the main point according to the definition in Information System Success Model (Delone and McLean, 2003). Moreover, the differentiation between mobile library app and web-based platform is caused by the hardware difference between mobile device and computer. Thus, perceived mobile library quality is helpful to improve users' perceived differentiation between mobile library app and web-based platform. Therefore, we hypothesize that: H5. Perceived mobile library quality will positively influence perceived differentiation.
Structural framework.
From the constructs and their relationships discussed above, we can draw the overall research framework, as shown in Figure 1. In particular, it illustrates that perceived situation efficiency and perceived mobile library quality positively influence intention to use mobile library. Moreover, perceived situation efficiency are affected by both perceived differentiation (including function and resource differentiation) and perceived mobile library quality. At the same time, perceived mobile library quality positively affects perceived differentiation. All of the constructs are measured by multi-item scales adapted from prior studies with minor changes in wording in order to fit the specific research context (as shown in the Appendix). A seven-Likert scale, from strongly disagree to strongly agree, is adopted to measure the four constructs. Specially, five items on perceived differentiation were adapted from Carter and Curry (2013) to access the differentiation in function and resource between web based and mobile library service. Perceived mobile library quality was evaluated on two aspects including hardware quality and software quality. Three items of intention to use mobile library were adopted from Davis (1989) to assess users usage intention of mobile library apps. Perceived situation efficiency was the combination of perceived efficiency and situational context, whose items were adapted from Li (2014) and Karnowski and Jandura (2014). At last, in order to ensure the logical consistency and ease of understanding of the questionnaire, we conduct a pilot test to refine the questionnaire wording before formal data collection. On the whole, the questionnaire was unambiguous and easy to complete.
Questionnaire design and data collection method
The current study uses smart phones as the target mobile platform because smart phones are more portable than other mobile devices, and they are carried almost at all times (Sun et al., 2014). Meanwhile, Chinese academic libraries are selected as a case to examine the proposed research model. And then most participants of this study are college students.
The questionnaire is divided into three major sections. The first section is an introduction of mobile library app in order to make every participant has a preliminary understanding of the research target. The second section is demographic information (gender, grade level) and general mobile usage habits (circumstance and function). The last section includes the measurement of constructs mentioned above.
Empirical data for this study were collected from both an online survey and paperversion questionnaire. Several methods were used to recruit participants. First, we sent invitation messages containing the online questionnaire URL to friends and students by instant messaging tools. Second, we searched for the potential participants in some related online communities, and sent them short messages. Third, the paper-version was distributed to students by professor after class. At last, a total of 337 surveys were returned. After eliminating 18 invalid samples, 319 valid surveys were collected.
Data analysis method
In the process of data analysis, we used a two-step method to test the model, beginning with the measurement model to test the reliability and validity of the instrument and then analyzing the structural model for hypotheses testing (Anderson and Gerbing, 1988). The measurement model describes the supposed relations of the observed variables to the underlying constructs, and the structural model presents the assumed causal relations of the estimated constructed.
The partial least squares (PLS) are selected as data analysis method in this study based on the following three points. First, the whole model of this research is lack of solid assumptions from certain theories, which makes PLS appropriate for this study, as it emphasizes prediction rather than theory examination; second, both formative indicators and reflective indicators are included in the model of the study, so that it is proper to adopt PLS (Hair et al., 2012). Third, PLS works well with small sample size, which is about ten times the largest number of structural paths directed at a particular construct in the model (Gefen et al., 2000). The sample size in this study meets the necessary conditions for using PLS. SmartPLS 2.0 is used in the present study. In the process of model construction, latent variables are pictured as ellipses while observed variables are depicted as rectangles, and a single arrow pointed to the dependent variable from the independent variable as hypothesized.
Results

Demographic profile of respondents
Among the valid survey respondents, 145 (45.5 percent) were male and 174 (54.5 percent) were female. For education distribution, a large majority of them (97.5 percent) were undergraduate or above. In terms of their experience with the mobile network and mobile library app, around 98.4 percent of the respondents had used mobile device to surf the internet and about 41.1 percent of the respondents had used mobile library app. Moreover, 144 (45.1 percent) of the respondents used mobile device to communicate, and most of them (83.1 percent) took their leisure time to surf the mobile internet. Table I displays the demographic information in details.
As shown in Table I, surfing internet through mobile devices in the leisure time is very common among college students. Hence, college students is used to spend much of their leisure time surfing internet through mobile devices. However, the main usage of mobile app is communication, followed by reading and learning, implying that mobile app is more like tools for communication than learning. In addition, among the participants, nearly half of them downloaded and used mobile library app.
Measurement model
SEM is consisted of measurement model and structural model. The former is to assess the relationship between a latent variable and its associated observed variables by a series of indices. In the present study, we mainly examine reliability and construct validity which is composed of convergent validity and discriminant validity.
5.2.1 Reliability analysis. Reliability refers to the internal consistency of items constituting constructs. In other words, it means the degree of closeness among all items of a construct. In this study, Cronbach's α was adopted to test reliability of constructs, and the results are presented in Table II. The Cronbach's α of all constructs is above 0.7, indicating that the research constructs have good internal reliability (Chin, 1998).
Convergent validity.
Convergent validity indicates the degree to which the measures of a construct that are theoretically related, are also related in reality. It should come up to three criteria using the measurement model: first, the loadings of measurement items on the variables through CFA are greater than 0.70; second, the associated average variance extracted (AVE) of measures is greater than 0.50; and finally, the associated composite reliability (CR) of measures is greater than 0.70 (Fornell and Larcker, 1981).
The data from Table II shows that the loadings of measurements range from 0.83 to 0.97, which are all over 0.70. In addition, both the associated AVE and CR exceed the recommended thresholds. Therefore, we can come to the conclusion that every measure scale has a high convergent validity. LHT 34,1 5.2.3 Discriminant validity. Discriminant validity indicates the extent to which a given latent variable is different from other latent variables. If the square root of AVE is greater than the correlation between that construct and all other constructs, the discriminant validity of the measurements is satisfactory (Fornell and Larcker, 1981). Table III displays the correlation matrix of the constructs and their square root of AVE value which is presented on the diagonal. The results imply that every construct has higher correlation with itself, indicating satisfactory discriminant validity.
Structural model
The structural model is to examine the linear relation between exogenous latent variable and endogenous latent variable. The path significance levels were estimated by using a bootstrap with 500 resamples. The R 2 criteria were used to assess the predictive capacity of the structural model. Examination of R 2 shows that the model explains perceived situation efficiency and intention to use mobile library. The results of the analysis are depicted in Figure 2, which displays the overall explanatory power and the estimated path coefficients. The results illustrate that the exogenous variables explain 7 percent of the variance in perceived differentiation, 48 percent of the variance in perceived situation efficiency and 40 percent of variance in intention to use mobile library. All of the structural paths are statistically significant. The results also demonstrate that the impact of perceived mobile library quality on perceived situation efficiency is greater than the impact of perceived differentiation, but both of them are significant with path coefficients of 0.658 and 0.102, respectively (H3 and H4 are supported). Moreover, perceived mobile library quality is significantly related to perceived differentiation, with a path coefficient of 0.272 (H5 is supported). Perceived situation efficiency and perceived mobile library quality are statistically significant in explaining adoption intention of mobile library with path coefficients of 0.133 and 0.530, respectively (H1 and H2 are supported). The results also show that perceived situation efficiency is more important in explaining adoption intention. In addition, perceived differentiation and perceived mobile library quality are multi-dimensional constructs, and the path coefficients mean factor loadings.
Discussion

Interpretation of results
This study investigates the determinants to predict mobile library app adoption in the context of web-mobile extension. The current study provides several important findings. First, perceived situation efficiency and perceived mobile library quality positively affect intention to use mobile library, demonstrating that both quality and situation efficiency are necessary to satisfy library users' needs in mobile era. Specially, the estimated coefficients and significance levels of perceived situation efficiency are relatively stronger than those of perceived mobile library quality, which highlights the importance of situation efficiency for determining user intention to use mobile library.
In addition, the current study also finds that perceived mobile library quality has a strong impact on perceived situation efficiency. This finding further strengthens the important role of situational factors in shaping users' behavior toward mobile library service. In mobile era, the situational context of users is always changing, so it becomes very important to satisfy their efficiency needs in different situational contexts.
Second, the present study also finds that users' perceived differentiation between web based and mobile library service in terms of function differentiation and resource differentiation positively influences their perception of situation efficiency. The present study points out that the theory of categorization is appropriate to be used in a dual-channel environment. The results suggest that the difference of function and resource of mobile library service make it more efficient than web-based library service in terms of different situation contexts. Distinguished features should be designed in the mobile library ap",6191,6889
